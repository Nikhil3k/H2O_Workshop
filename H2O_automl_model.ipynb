{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML: Automatic Machine Learning\n",
    "\n",
    "AutoML: Automatic Machine Learning  \n",
    "\n",
    "H2Oâ€™s AutoML is used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. Stacked Ensembles will be automatically trained on collections of individual models to produce highly predictive ensemble models which, in most cases, will be the top performing models in the AutoML Leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorials\n",
    "\n",
    "* Intro to AutoML + Hands-on Lab - Erin LeDell, Machine Learning Scientist... [https://youtu.be/42Oo8TOl85I](https://youtu.be/42Oo8TOl85I)  \n",
    "* Scalable Automatic Machine Learning in H2O [https://youtu.be/j6rqrEYQNdo](https://youtu.be/j6rqrEYQNdo)      \n",
    "\n",
    "* Scalable Automatic Machine Learning in H2O [https://youtu.be/j6rqrEYQNdo](https://youtu.be/j6rqrEYQNdo)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing H2O and h2o python\n",
    "\n",
    "See [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/downloading.html)  \n",
    "\n",
    "Click the Download H2O button on the [http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html](http://h2o-release.s3.amazonaws.com/h2o/latest_stable.html) page. This downloads a zip file that contains everything you need to get started.\n",
    "\n",
    "\n",
    "```bash\n",
    "cd ~/Downloads\n",
    "unzip h2o-3.20.0.1.zip\n",
    "cd h2o-3.20.0.1\n",
    "java -jar h2o.jar\n",
    "```\n",
    "\n",
    "Point your browser to http://localhost:54321.\n",
    "\n",
    "**Install in Python**  \n",
    "\n",
    "Install dependencies (prepending with sudo if needed):\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "pip install tabulate\n",
    "pip install scikit-learn\n",
    "pip install colorama\n",
    "pip install future\n",
    "```\n",
    "\n",
    "Remove any existing H2O module for Python.\n",
    "\n",
    "```bash\n",
    "pip uninstall h2o\n",
    "```\n",
    "\n",
    "Use pip to install this version of the H2O Python module.  \n",
    "\n",
    "```bash\n",
    "pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o\n",
    "```\n",
    "\n",
    "Note: When installing H2O from pip in OS X El Capitan, users must include the --user flag. For example:\n",
    "\n",
    "\n",
    "```bash\n",
    "pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o --user\n",
    "```\n",
    "\n",
    "Initialize H2O in Python and run a demo to see H2O at work.\n",
    "\n",
    "```python\n",
    "python\n",
    "import h2o\n",
    "h2o.init()\n",
    "h2o.demo(\"glm\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data\n",
    "\n",
    "H2O model file file will be saved in one of two formats.\n",
    "\n",
    "\n",
    "There are two ways to save the leader model -- binary format and MOJO format.  If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use.\n",
    "\n",
    "See [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/save-and-load-model.html)    \n",
    "\n",
    "```python\n",
    "\n",
    "# save the model\n",
    "model_path = h2o.save_model(model=model, path=\"/tmp/mymodel\", force=True)\n",
    "\n",
    "# or\n",
    "\n",
    "h2o.save_model(aml.leader, path = \"./models\")\n",
    "\n",
    "# or\n",
    "\n",
    "aml.leader.download_mojo(path = \"./models\")\n",
    "\n",
    "# load the model\n",
    "saved_model = h2o.load_model(model_path)\n",
    "\n",
    "```\n",
    "\n",
    "**Saving data from runs**   \n",
    "\n",
    "\n",
    "Stats about the models can be saved as text or csv or put directly in a database.\n",
    "\n",
    "Much of the data is gathered by converting H2O objects to pandas data frame.  So anything that a pandas data frame can be saved as is supported.\n",
    "[https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)    \n",
    "\n",
    "\n",
    "```python\n",
    "data_pd = object.as_data_frame(use_pandas=True)\n",
    "```  \n",
    "\n",
    "Otherwise data is returned as python dictionaries or lists.  \n",
    "\n",
    "```python\n",
    "[('addr_state', 258199.28125, 1.0, 0.19965953057652525), ('int_rate', 203347.0625, 0.7875585924002257, 0.15724357886013807), ('dti', 116477.5703125, 0.45111500600856147, 0.09006941033569575), ('revol_util', 110586.1484375, 0.42829766179877776, 0.08551371010176734), ('annual_inc', 96993.90625, 0.3756552139898724, 0.07500314368384206), ('loan_amnt', 95294.5, 0.36907345186500207, 0.0736890321476241), ('total_acc', 90064.8046875, 0.3488189597255124, 0.06964502975498767), ('longest_credit_length', 84291.921875, 0.3264607146345416, 0.06518099303560954), ('purpose', 77462.203125, 0.30000936776426446, 0.05989972953637317), ('emp_length', 63839.28125, 0.24724809821677224, 0.04936543922589935), ('term', 34895.7265625, 0.1351503629040408, 0.02698405801466782), ('home_ownership', 26499.876953125, 0.10263342649457897, 0.02049174175536795), ('delinq_2yrs', 20556.2578125, 0.0796139234508423, 0.015895678583550586), ('verification_status', 14689.3369140625, 0.056891470971368166, 0.01135892438795138)]\n",
    "```  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting H2O server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import h2o package and specific estimator \n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h2o.init with python seems very sensitive the the H2O version.  If the H2O cluster version is 3.20.0.1 and the python h2o library is 3.19.0 it will fail so we set strict_version_check=False\n",
    "\n",
    "If the H2O cluster isn't found h2o.init will start one.\n",
    "\n",
    "Note that the current script starts each H2O instance on a different port.  It's not clear why but should we do this we should choose from only the higher ports.\n",
    "\n",
    "A port number is a 16-bit unsigned integer, thus ranging from 0 to 65535.  There is no reason to choose a port less than 10000.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_121\"; OpenJDK Runtime Environment (Zulu 8.20.0.5-macosx) (build 1.8.0_121-b15); OpenJDK 64-Bit Server VM (Zulu 8.20.0.5-macosx) (build 25.121-b15, mixed mode)\n",
      "  Starting server from /Users/bear/anaconda/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmpxg3dfx65\n",
      "  JVM stdout: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmpxg3dfx65/h2o_bear_started_from_python.out\n",
      "  JVM stderr: /var/folders/lh/42j8mfjx069d1bkc2wlf2pw40000gn/T/tmpxg3dfx65/h2o_bear_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>02 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.0.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 month and 16 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_bear_w4adug</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.556 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.5 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         02 secs\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.0.2\n",
       "H2O cluster version age:    1 month and 16 days\n",
       "H2O cluster name:           H2O_from_python_bear_w4adug\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.556 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.5 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(strict_version_check=False) # start h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h2o.automl Parameters\n",
    "\n",
    "[http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)  \n",
    "\n",
    "NB:  Eventually one wants to expose all the parameters to the expert user.   \n",
    "\n",
    "**Required Data Parameters**\n",
    "\n",
    "y: This argument is the name (or index) of the response column.  \n",
    "\n",
    "training_frame: Specifies the training set.  \n",
    "\n",
    "The user gives the name of the depenent variable and training file name.   \n",
    "\n",
    "\n",
    "**Required Stopping Parameters**  \n",
    "\n",
    "One of the following stopping strategies (time or number-of-model based) must be specified. When both options are set, then the AutoML run will stop as soon as it hits one of either of these limits.\n",
    "\n",
    "max_runtime_secs: This argument controls how long the AutoML run will execute for. This defaults to 3600 seconds (1 hour).  \n",
    "\n",
    "\n",
    "max_models: Specify the maximum number of models to build in an AutoML run, excluding the Stacked Ensemble models. Defaults to NULL/None.\n",
    "\n",
    "\n",
    "### Optional Parameters\n",
    "\n",
    "**Optional Data Parameters**  \n",
    "\n",
    "x: A list/vector of predictor column names or indexes. This argument only needs to be specified if the user wants to exclude columns from the set of predictors. If all columns (other than the response) should be used in prediction, then this does not need to be set.  \n",
    "\n",
    "validation_frame: This argument is used to specify the validation frame used for early stopping of individual models and early stopping of the grid searches (unless max_models or max_runtime_secs overrides metric-based early stopping).  \n",
    "\n",
    "leaderboard_frame: This argument allows the user to specify a particular data frame use to score & rank models on the leaderboard. This frame will not be used for anything besides leaderboard scoring. If a leaderboard frame is not specified by the user, then the leaderboard will use cross-validation metrics instead (or if cross-validation is turned off by setting nfolds = 0, then a leaderboard frame will be generated automatically from the validation frame (if provided) or the training frame).  \n",
    "\n",
    "fold_column: Specifies a column with cross-validation fold index assignment per observation. This is used to override the default, randomized, 5-fold cross-validation scheme for individual models in the AutoML run.  \n",
    "\n",
    "weights_column: Specifies a column with observation weights. Giving some observation a weight of zero is equivalent to excluding it from the dataset; giving an observation a relative weight of 2 is equivalent to repeating that row twice. Negative weights are not allowed.  \n",
    "\n",
    "ignored_columns: (Optional, Python only) Specify the column or columns (as a list/vector) to be excluded from the model. This is the converse of the x argument.  \n",
    "\n",
    "**Optional Miscellaneous Parameters**  \n",
    "\n",
    "nfolds: Number of folds for k-fold cross-validation of the models in the AutoML run. Defaults to 5. Use 0 to disable cross-validation; this will also disable Stacked Ensembles (thus decreasing the overall best model performance).\n",
    "\n",
    "balance_classes: Specify whether to oversample the minority classes to balance the class distribution. This option is not enabled by default and can increase the data frame size. This option is only applicable for classification. Majority classes can be undersampled to satisfy the max_after_balance_size parameter.\n",
    "\n",
    "class_sampling_factors: Specify the per-class (in lexicographical order) over/under-sampling ratios. By default, these ratios are automatically computed during training to obtain the class balance.\n",
    "\n",
    "max_after_balance_size: Specify the maximum relative size of the training data after balancing class counts (balance_classes must be enabled). Defaults to 5.0. (The value can be less than 1.0).\n",
    "\n",
    "stopping_metric: Specifies the metric to use for early stopping of the grid searches and individual models. Defaults to \"AUTO\". The available options are:\n",
    "\n",
    "AUTO: This defaults to logloss for classification, deviance for regression\n",
    "deviance (mean residual deviance)\n",
    "logloss\n",
    "MSE\n",
    "RMSE\n",
    "MAE\n",
    "RMSLE\n",
    "AUC\n",
    "lift_top_group\n",
    "misclassification\n",
    "mean_per_class_error  \n",
    "\n",
    "stopping_tolerance: This option specifies the relative tolerance for the metric-based stopping criterion to stop a grid search and the training of individual models within the AutoML run. This value defaults to 0.001 if the dataset is at least 1 million rows; otherwise it defaults to a bigger value determined by the size of the dataset and the non-NA-rate. In that case, the value is computed as 1/sqrt(nrows * non-NA-rate).\n",
    "\n",
    "stopping_rounds: This argument is used to stop model training when the stopping metric (e.g. AUC) doesnâ€™t improve for this specified number of training rounds, based on a simple moving average. In the context of AutoML, this controls early stopping both within the random grid searches as well as the individual models. Defaults to 3 and must be an non-negative integer. To disable early stopping altogether, set this to 0.\n",
    "\n",
    "sort_metric: Specifies the metric used to sort the Leaderboard by at the end of an AutoML run. Available options include:\n",
    "\n",
    "AUTO: This defaults to AUC for binary classification, mean_per_class_error for multinomial classification, and deviance for regression.\n",
    "deviance (mean residual deviance)\n",
    "logloss\n",
    "MSE\n",
    "RMSE\n",
    "MAE\n",
    "RMSLE\n",
    "AUC\n",
    "mean_per_class_error  \n",
    "\n",
    "seed: Integer. Set a seed for reproducibility. AutoML can only guarantee reproducibility if max_models is used because max_runtime_secs is resource limited, meaning that if the available compute resources are not the same between runs, AutoML may be able to train more models on one run vs another. Defaults to NULL/None.\n",
    "\n",
    "project_name: Character string to identify an AutoML project. Defaults to NULL/None, which means a project name will be auto-generated based on the training frame ID. More models can be trained and added to an existing AutoML project by specifying the same project name in muliple calls to the AutoML function (as long as the same training frame is used in subsequent runs).\n",
    "\n",
    "exclude_algos: List/vector of character strings naming the algorithms to skip during the model-building phase. An example use is exclude_algos = [\"GLM\", \"DeepLearning\", \"DRF\"] in Python or exclude_algos = c(\"GLM\", \"DeepLearning\", \"DRF\") in R. Defaults to None/NULL, which means that all appropriate H2O algorithms will be used, if the search stopping criteria allow. The algorithm names are:\n",
    "\n",
    "GLM\n",
    "DeepLearning\n",
    "GBM\n",
    "DRF (This includes both the Random Forest and Extremely Randomized Trees (XRT) models. Refer to the Extremely Randomized Trees section in the DRF chapter and the histogram_type parameter description for more information.)\n",
    "StackedEnsemble\n",
    "keep_cross_validation_predictions: Specify whether to keep the predictions of the cross-validation predictions. If set to FALSE, then running the same AutoML object for repeated runs will cause an exception because CV predictions are are required to build additional Stacked Ensemble models in AutoML. This option defaults to TRUE.\n",
    "\n",
    "\n",
    "keep_cross_validation_models: Specify whether to keep the cross-validated models. Deleting cross-validation models will save memory in the H2O cluster. This option defaults to TRUE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assume the following are passed by the user from the web interface\n",
    "\n",
    "'''\n",
    "Need a user id and project id?\n",
    "\n",
    "'''\n",
    "target='bad_loan' \n",
    "data_file='loan.csv'\n",
    "run_time=333\n",
    "run_id='SOME_ID_20180617_221529' # Just some arbitrary ID\n",
    "server_path='/Users/bear/Documents/INFO_7390/H2O'\n",
    "classification=True\n",
    "scale=False\n",
    "max_models=None\n",
    "balance_y=False # balance_classes=balance_y\n",
    "balance_threshold=0.2\n",
    "project =\"automl_test\"  # project_name = project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bear/Documents/INFO_7390/H2O/loan.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use local data file or download from some type of bucket\n",
    "import os\n",
    "\n",
    "data_path=os.path.join(server_path,data_file)\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
     ]
    }
   ],
   "source": [
    "# Use local data file or download from some type of bucket\n",
    "if not os.path.isfile(data_path):\n",
    "  data_path = 'https://raw.githubusercontent.com/h2oai/app-consumer-loan/master/data/loan.csv'\n",
    "\n",
    "# Load data into H2O\n",
    "df = h2o.import_file(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:163987\n",
      "Cols:15\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>loan_amnt         </th><th>term     </th><th>int_rate          </th><th>emp_length        </th><th>home_ownership  </th><th>annual_inc        </th><th>purpose           </th><th>addr_state  </th><th>dti               </th><th>delinq_2yrs       </th><th>revol_util        </th><th>total_acc         </th><th>bad_loan          </th><th>longest_credit_length  </th><th>verification_status  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>int               </td><td>enum     </td><td>real              </td><td>int               </td><td>enum            </td><td>real              </td><td>enum              </td><td>enum        </td><td>real              </td><td>int               </td><td>real              </td><td>int               </td><td>int               </td><td>int                    </td><td>enum                 </td></tr>\n",
       "<tr><td>mins   </td><td>500.0             </td><td>         </td><td>5.42              </td><td>0.0               </td><td>                </td><td>1896.0            </td><td>                  </td><td>            </td><td>0.0               </td><td>0.0               </td><td>0.0               </td><td>1.0               </td><td>0.0               </td><td>0.0                    </td><td>                     </td></tr>\n",
       "<tr><td>mean   </td><td>13074.169141456336</td><td>         </td><td>13.715904065566173</td><td>5.68435293299533  </td><td>                </td><td>71915.67051974901 </td><td>                  </td><td>            </td><td>15.881530121290117</td><td>0.2273570060625282</td><td>54.07917280242258 </td><td>24.579733834274638</td><td>0.1830388994249544</td><td>14.854273655448353     </td><td>                     </td></tr>\n",
       "<tr><td>maxs   </td><td>35000.0           </td><td>         </td><td>26.06             </td><td>10.0              </td><td>                </td><td>7141778.0         </td><td>                  </td><td>            </td><td>39.99             </td><td>29.0              </td><td>150.70000000000002</td><td>118.0             </td><td>1.0               </td><td>65.0                   </td><td>                     </td></tr>\n",
       "<tr><td>sigma  </td><td>7993.556188734649 </td><td>         </td><td>4.391939870545795 </td><td>3.6106637311002365</td><td>                </td><td>59070.915654918244</td><td>                  </td><td>            </td><td>7.587668224192549 </td><td>0.6941679229284182</td><td>25.285366766770505</td><td>11.685190365910659</td><td>0.3866995896078875</td><td>6.947732922546696      </td><td>                     </td></tr>\n",
       "<tr><td>zeros  </td><td>0                 </td><td>         </td><td>0                 </td><td>14248             </td><td>                </td><td>0                 </td><td>                  </td><td>            </td><td>270               </td><td>139459            </td><td>1562              </td><td>0                 </td><td>133971            </td><td>11                     </td><td>                     </td></tr>\n",
       "<tr><td>missing</td><td>0                 </td><td>0        </td><td>0                 </td><td>5804              </td><td>0               </td><td>4                 </td><td>0                 </td><td>0           </td><td>0                 </td><td>29                </td><td>193               </td><td>29                </td><td>0                 </td><td>29                     </td><td>0                    </td></tr>\n",
       "<tr><td>0      </td><td>5000.0            </td><td>36 months</td><td>10.65             </td><td>10.0              </td><td>RENT            </td><td>24000.0           </td><td>credit_card       </td><td>AZ          </td><td>27.65             </td><td>0.0               </td><td>83.7              </td><td>9.0               </td><td>0.0               </td><td>26.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>1      </td><td>2500.0            </td><td>60 months</td><td>15.27             </td><td>0.0               </td><td>RENT            </td><td>30000.0           </td><td>car               </td><td>GA          </td><td>1.0               </td><td>0.0               </td><td>9.4               </td><td>4.0               </td><td>1.0               </td><td>12.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>2      </td><td>2400.0            </td><td>36 months</td><td>15.96             </td><td>10.0              </td><td>RENT            </td><td>12252.0           </td><td>small_business    </td><td>IL          </td><td>8.72              </td><td>0.0               </td><td>98.5              </td><td>10.0              </td><td>0.0               </td><td>10.0                   </td><td>not verified         </td></tr>\n",
       "<tr><td>3      </td><td>10000.0           </td><td>36 months</td><td>13.49             </td><td>10.0              </td><td>RENT            </td><td>49200.0           </td><td>other             </td><td>CA          </td><td>20.0              </td><td>0.0               </td><td>21.0              </td><td>37.0              </td><td>0.0               </td><td>15.0                   </td><td>verified             </td></tr>\n",
       "<tr><td>4      </td><td>5000.0            </td><td>36 months</td><td>7.9               </td><td>3.0               </td><td>RENT            </td><td>36000.0           </td><td>wedding           </td><td>AZ          </td><td>11.2              </td><td>0.0               </td><td>28.3              </td><td>12.0              </td><td>0.0               </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>5      </td><td>3000.0            </td><td>36 months</td><td>18.64             </td><td>9.0               </td><td>RENT            </td><td>48000.0           </td><td>car               </td><td>CA          </td><td>5.3500000000000005</td><td>0.0               </td><td>87.5              </td><td>4.0               </td><td>0.0               </td><td>4.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>6      </td><td>5600.0            </td><td>60 months</td><td>21.28             </td><td>4.0               </td><td>OWN             </td><td>40000.0           </td><td>small_business    </td><td>CA          </td><td>5.55              </td><td>0.0               </td><td>32.6              </td><td>13.0              </td><td>1.0               </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>7      </td><td>5375.0            </td><td>60 months</td><td>12.69             </td><td>0.0               </td><td>RENT            </td><td>15000.0           </td><td>other             </td><td>TX          </td><td>18.08             </td><td>0.0               </td><td>36.5              </td><td>3.0               </td><td>1.0               </td><td>7.0                    </td><td>verified             </td></tr>\n",
       "<tr><td>8      </td><td>6500.0            </td><td>60 months</td><td>14.65             </td><td>5.0               </td><td>OWN             </td><td>72000.0           </td><td>debt_consolidation</td><td>AZ          </td><td>16.12             </td><td>0.0               </td><td>20.6              </td><td>23.0              </td><td>0.0               </td><td>13.0                   </td><td>not verified         </td></tr>\n",
       "<tr><td>9      </td><td>12000.0           </td><td>36 months</td><td>12.69             </td><td>10.0              </td><td>OWN             </td><td>75000.0           </td><td>debt_consolidation</td><td>CA          </td><td>10.78             </td><td>0.0               </td><td>67.10000000000001 </td><td>34.0              </td><td>0.0               </td><td>22.0                   </td><td>verified             </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad_loan\n",
      "['loan_amnt', 'term', 'int_rate', 'emp_length', 'home_ownership', 'annual_inc', 'purpose', 'addr_state', 'dti', 'delinq_2yrs', 'revol_util', 'total_acc', 'longest_credit_length', 'verification_status']\n"
     ]
    }
   ],
   "source": [
    "# assign target and inputs for logistic regression\n",
    "y = target\n",
    "X = [name for name in df.columns if name != y]\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loan_amnt', 'emp_length', 'delinq_2yrs', 'total_acc', 'longest_credit_length']\n",
      "['term', 'home_ownership', 'purpose', 'addr_state', 'verification_status']\n",
      "['int_rate', 'annual_inc', 'dti', 'revol_util']\n"
     ]
    }
   ],
   "source": [
    "# determine column types\n",
    "ints, reals, enums = [], [], []\n",
    "for key, val in df.types.items():\n",
    "    if key in X:\n",
    "        if val == 'enum':\n",
    "            enums.append(key)\n",
    "        elif val == 'int':\n",
    "            ints.append(key)            \n",
    "        else: \n",
    "            reals.append(key)\n",
    "\n",
    "print(ints)\n",
    "print(enums)\n",
    "print(reals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# impute missing values\n",
    "_ = df[reals].impute(method='mean')\n",
    "_ = df[ints].impute(method='median')\n",
    "\n",
    "if scale:\n",
    "    df[reals] = df[reals].scale()\n",
    "    df[ints] = df[ints].scale()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set target to factor for classification by default or if user specifies classification\n",
    "if classification:\n",
    "    df[y] = df[y].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0', '1']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[y].levels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### balance_classes check \n",
    "\n",
    "If one class in two class classification is less than 20% of the total then one should set balance_classes=True\n",
    "\n",
    "That is,\n",
    "\n",
    "balance_classes=balance_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if classification:\n",
    "    class_percentage = y_balance=df[y].mean()[0]/(df[y].max()-df[y].min())\n",
    "    if class_percentage < balance_threshold:\n",
    "        balance_y=True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(run_time)\n",
    "type(run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validate rather than take a test training split\n",
    "\n",
    "Cross-validation rather than taking a test training split reduces the variance of the estimates of goodness of fit statistics.  In rare cases one should take a test training split but this should be left to the expert users.\n",
    "\n",
    "This also means the pro user can just upload the data and not worry about taking a test training split.  \n",
    "\n",
    "We can pass the original, full dataset, `df` (without passing a `leaderboard_frame`).  This is a more efficient use of our data since we can use 100% of the data for training, rather than 80% or so.  This time our leaderboard will use cross-validated metrics. It also gives better estimates of goodness of fit statistics.\n",
    "\n",
    "*Note: Using an explicit `leaderboard_frame` for scoring may be useful in some cases, which is why the option is available.*  \n",
    "\n",
    "But it's not preferable in most cases.  Leave it as an expert option.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
     ]
    }
   ],
   "source": [
    "# automl\n",
    "# runs for run_time seconds then builds a stacked ensemble\n",
    "aml = H2OAutoML(max_runtime_secs=run_time,project_name = project,balance_classes=balance_y) # init automl, run for 300 seconds\n",
    "aml.train(x=X,  \n",
    "           y=y,\n",
    "           training_frame=df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard\n",
    "\n",
    "Next, we will view the AutoML Leaderboard.  Since we did not specify a `leaderboard_frame` in the `H2OAutoML.train()` method for scoring and ranking the models, the AutoML leaderboard uses cross-validation metrics to rank the models.  \n",
    "\n",
    "A default performance metric for each machine learning task (binary classification, multiclass classification, regression) is specified internally and the leaderboard will be sorted by that metric.  In the case of binary classification, the default ranking metric is Area Under the ROC Curve (AUC).  In the future, the user will be able to specify any of the H2O metrics so that different metrics can be used to generate rankings on the leaderboard.\n",
    "\n",
    "The leader model is stored at `aml.leader` and the leaderboard is stored at `aml.leaderboard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>model_id                                           </th><th style=\"text-align: right;\">     auc</th><th style=\"text-align: right;\">  logloss</th><th style=\"text-align: right;\">  mean_per_class_error</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>StackedEnsemble_AllModels_AutoML_20190108_000309   </td><td style=\"text-align: right;\">0.702053</td><td style=\"text-align: right;\"> 0.440191</td><td style=\"text-align: right;\">              0.352014</td><td style=\"text-align: right;\">0.372258</td><td style=\"text-align: right;\">0.138576</td></tr>\n",
       "<tr><td>StackedEnsemble_BestOfFamily_AutoML_20190108_000309</td><td style=\"text-align: right;\">0.702053</td><td style=\"text-align: right;\"> 0.440191</td><td style=\"text-align: right;\">              0.352014</td><td style=\"text-align: right;\">0.372258</td><td style=\"text-align: right;\">0.138576</td></tr>\n",
       "<tr><td>GLM_grid_1_AutoML_20190108_000309_model_1          </td><td style=\"text-align: right;\">0.698362</td><td style=\"text-align: right;\"> 0.439271</td><td style=\"text-align: right;\">              0.354907</td><td style=\"text-align: right;\">0.371955</td><td style=\"text-align: right;\">0.138351</td></tr>\n",
       "<tr><td>XRT_1_AutoML_20190108_000309                       </td><td style=\"text-align: right;\">0.688334</td><td style=\"text-align: right;\"> 0.472261</td><td style=\"text-align: right;\">              0.364171</td><td style=\"text-align: right;\">0.382791</td><td style=\"text-align: right;\">0.146529</td></tr>\n",
       "<tr><td>DRF_1_AutoML_20190108_000309                       </td><td style=\"text-align: right;\">0.686498</td><td style=\"text-align: right;\"> 0.478133</td><td style=\"text-align: right;\">              0.364403</td><td style=\"text-align: right;\">0.383717</td><td style=\"text-align: right;\">0.147239</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view leaderboard\n",
    "lb = aml.leaderboard\n",
    "lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will view a snapshot of the top models.  Here we should see the two Stacked Ensembles at or near the top of the leaderboard.  Stacked Ensembles can almost always outperform a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_AutoML_20190108_000309\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0748160248427708\n",
      "RMSE: 0.27352518136868276\n",
      "LogLoss: 0.2770108718654861\n",
      "Null degrees of freedom: 147784\n",
      "Residual degrees of freedom: 147781\n",
      "Null deviance: 140732.20131171282\n",
      "Residual deviance: 81876.10339728172\n",
      "AIC: 81884.10339728172\n",
      "AUC: 0.9705354292408278\n",
      "pr_auc: 0.8960099911776978\n",
      "Gini: 0.9410708584816556\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3054804393299643: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>114475.0</td>\n",
       "<td>6243.0</td>\n",
       "<td>0.0517</td>\n",
       "<td> (6243.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>4811.0</td>\n",
       "<td>22256.0</td>\n",
       "<td>0.1777</td>\n",
       "<td> (4811.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>119286.0</td>\n",
       "<td>28499.0</td>\n",
       "<td>0.0748</td>\n",
       "<td> (11054.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1      Error    Rate\n",
       "-----  ------  -----  -------  ------------------\n",
       "0      114475  6243   0.0517   (6243.0/120718.0)\n",
       "1      4811    22256  0.1777   (4811.0/27067.0)\n",
       "Total  119286  28499  0.0748   (11054.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.8010654</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2324010</td>\n",
       "<td>0.8527882</td>\n",
       "<td>256.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4116258</td>\n",
       "<td>0.8349729</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.3440950</td>\n",
       "<td>0.9274622</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1052357</td>\n",
       "<td>1.0</td>\n",
       "<td>361.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.7554259</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2517271</td>\n",
       "<td>0.9051343</td>\n",
       "<td>245.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2395344</td>\n",
       "<td>0.9063511</td>\n",
       "<td>252.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.30548      0.801065  216\n",
       "max f2                       0.232401     0.852788  256\n",
       "max f0point5                 0.411626     0.834973  167\n",
       "max accuracy                 0.344095     0.927462  198\n",
       "max precision                0.963831     1         0\n",
       "max recall                   0.105236     1         361\n",
       "max specificity              0.963831     1         0\n",
       "max absolute_mcc             0.30548      0.755426  216\n",
       "max min_per_class_accuracy   0.251727     0.905134  245\n",
       "max mean_per_class_accuracy  0.239534     0.906351  252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.32 %, avg score: 21.52 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100010</td>\n",
       "<td>0.8231913</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.0546052</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200020</td>\n",
       "<td>0.7645343</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7922026</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8293156</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1092105</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300030</td>\n",
       "<td>0.7172474</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7405818</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7997376</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1638157</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400041</td>\n",
       "<td>0.6758864</td>\n",
       "<td>5.4304164</td>\n",
       "<td>5.4525814</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.6959487</td>\n",
       "<td>0.9986468</td>\n",
       "<td>0.7737904</td>\n",
       "<td>0.0543097</td>\n",
       "<td>0.2181254</td>\n",
       "<td>443.0416418</td>\n",
       "<td>445.2581383</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500051</td>\n",
       "<td>0.6383709</td>\n",
       "<td>5.3417566</td>\n",
       "<td>5.4304164</td>\n",
       "<td>0.9783491</td>\n",
       "<td>0.6571369</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.7504597</td>\n",
       "<td>0.0534230</td>\n",
       "<td>0.2715484</td>\n",
       "<td>434.1756558</td>\n",
       "<td>443.0416418</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000034</td>\n",
       "<td>0.4826144</td>\n",
       "<td>4.9922256</td>\n",
       "<td>5.2113359</td>\n",
       "<td>0.9143321</td>\n",
       "<td>0.5558423</td>\n",
       "<td>0.9544624</td>\n",
       "<td>0.6531576</td>\n",
       "<td>0.2496028</td>\n",
       "<td>0.5211512</td>\n",
       "<td>399.2225650</td>\n",
       "<td>421.1335859</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500017</td>\n",
       "<td>0.3715061</td>\n",
       "<td>3.7611647</td>\n",
       "<td>4.7279673</td>\n",
       "<td>0.6888618</td>\n",
       "<td>0.4241561</td>\n",
       "<td>0.8659329</td>\n",
       "<td>0.5768272</td>\n",
       "<td>0.1880519</td>\n",
       "<td>0.7092031</td>\n",
       "<td>276.1164677</td>\n",
       "<td>372.7967271</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2957882</td>\n",
       "<td>2.5648335</td>\n",
       "<td>4.1872021</td>\n",
       "<td>0.4697523</td>\n",
       "<td>0.3308477</td>\n",
       "<td>0.7668911</td>\n",
       "<td>0.5153344</td>\n",
       "<td>0.1282373</td>\n",
       "<td>0.8374404</td>\n",
       "<td>156.4833515</td>\n",
       "<td>318.7202128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000034</td>\n",
       "<td>0.2099951</td>\n",
       "<td>1.1785170</td>\n",
       "<td>3.1842845</td>\n",
       "<td>0.2158468</td>\n",
       "<td>0.2476441</td>\n",
       "<td>0.5832055</td>\n",
       "<td>0.4261023</td>\n",
       "<td>0.1178557</td>\n",
       "<td>0.9552961</td>\n",
       "<td>17.8517042</td>\n",
       "<td>218.4284479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1682202</td>\n",
       "<td>0.3351057</td>\n",
       "<td>2.4720139</td>\n",
       "<td>0.0613750</td>\n",
       "<td>0.1869477</td>\n",
       "<td>0.4527523</td>\n",
       "<td>0.3663157</td>\n",
       "<td>0.0335094</td>\n",
       "<td>0.9888056</td>\n",
       "<td>-66.4894267</td>\n",
       "<td>147.2013891</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000034</td>\n",
       "<td>0.1432933</td>\n",
       "<td>0.0897742</td>\n",
       "<td>1.9955531</td>\n",
       "<td>0.0164422</td>\n",
       "<td>0.1548801</td>\n",
       "<td>0.3654879</td>\n",
       "<td>0.3240274</td>\n",
       "<td>0.0089777</td>\n",
       "<td>0.9977833</td>\n",
       "<td>-91.0225818</td>\n",
       "<td>99.5553054</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1259732</td>\n",
       "<td>0.0195817</td>\n",
       "<td>1.6662356</td>\n",
       "<td>0.0035864</td>\n",
       "<td>0.1341923</td>\n",
       "<td>0.3051731</td>\n",
       "<td>0.2923893</td>\n",
       "<td>0.0019581</td>\n",
       "<td>0.9997414</td>\n",
       "<td>-98.0418298</td>\n",
       "<td>66.6235637</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999966</td>\n",
       "<td>0.1118935</td>\n",
       "<td>0.0022168</td>\n",
       "<td>1.4285256</td>\n",
       "<td>0.0004060</td>\n",
       "<td>0.1187515</td>\n",
       "<td>0.2616362</td>\n",
       "<td>0.2675846</td>\n",
       "<td>0.0002217</td>\n",
       "<td>0.9999631</td>\n",
       "<td>-99.7783204</td>\n",
       "<td>42.8525554</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0991367</td>\n",
       "<td>0.0003694</td>\n",
       "<td>1.25</td>\n",
       "<td>0.0000677</td>\n",
       "<td>0.1054206</td>\n",
       "<td>0.2289390</td>\n",
       "<td>0.2473134</td>\n",
       "<td>0.0000369</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.9630559</td>\n",
       "<td>25.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999966</td>\n",
       "<td>0.0873202</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111153</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0932206</td>\n",
       "<td>0.2035021</td>\n",
       "<td>0.2301925</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1115288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0637468</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0802774</td>\n",
       "<td>0.1831512</td>\n",
       "<td>0.2152005</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift         cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -----------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010001                    0.823191           5.45997      5.45997            1                0.866429   1                           0.866429            0.0546052       0.0546052                  445.997   445.997\n",
       "    2        0.020002                    0.764534           5.45997      5.45997            1                0.792203   1                           0.829316            0.0546052       0.10921                    445.997   445.997\n",
       "    3        0.030003                    0.717247           5.45997      5.45997            1                0.740582   1                           0.799738            0.0546052       0.163816                   445.997   445.997\n",
       "    4        0.0400041                   0.675886           5.43042      5.45258            0.994587         0.695949   0.998647                    0.77379             0.0543097       0.218125                   443.042   445.258\n",
       "    5        0.0500051                   0.638371           5.34176      5.43042            0.978349         0.657137   0.994587                    0.75046             0.053423        0.271548                   434.176   443.042\n",
       "    6        0.100003                    0.482614           4.99223      5.21134            0.914332         0.555842   0.954462                    0.653158            0.249603        0.521151                   399.223   421.134\n",
       "    7        0.150002                    0.371506           3.76116      4.72797            0.688862         0.424156   0.865933                    0.576827            0.188052        0.709203                   276.116   372.797\n",
       "    8        0.2                         0.295788           2.56483      4.1872             0.469752         0.330848   0.766891                    0.515334            0.128237        0.83744                    156.483   318.72\n",
       "    9        0.300003                    0.209995           1.17852      3.18428            0.215847         0.247644   0.583206                    0.426102            0.117856        0.955296                   17.8517   218.428\n",
       "    10       0.4                         0.16822            0.335106     2.47201            0.061375         0.186948   0.452752                    0.366316            0.0335094       0.988806                   -66.4894  147.201\n",
       "    11       0.500003                    0.143293           0.0897742    1.99555            0.0164422        0.15488    0.365488                    0.324027            0.00897772      0.997783                   -91.0226  99.5553\n",
       "    12       0.6                         0.125973           0.0195817    1.66624            0.00358641       0.134192   0.305173                    0.292389            0.0019581       0.999741                   -98.0418  66.6236\n",
       "    13       0.699997                    0.111893           0.0022168    1.42853            0.000406009      0.118752   0.261636                    0.267585            0.000221672     0.999963                   -99.7783  42.8526\n",
       "    14       0.8                         0.0991367          0.000369441  1.25               6.76636e-05      0.105421   0.228939                    0.247313            3.69454e-05     1                          -99.9631  25\n",
       "    15       0.899997                    0.0873202          0            1.11112            0                0.0932206  0.203502                    0.230193            0               1                          -100      11.1115\n",
       "    16       1                           0.0637468          0            1                  0                0.0802774  0.183151                    0.215201            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1374305415577651\n",
      "RMSE: 0.370716254779535\n",
      "LogLoss: 0.4374035749971233\n",
      "Null degrees of freedom: 16201\n",
      "Residual degrees of freedom: 16198\n",
      "Null deviance: 15373.717640908228\n",
      "Residual deviance: 14173.625444206784\n",
      "AIC: 14181.625444206784\n",
      "AUC: 0.7038412027583177\n",
      "pr_auc: 0.34120380509032733\n",
      "Gini: 0.4076824055166355\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.19535344211739553: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>9850.0</td>\n",
       "<td>3403.0</td>\n",
       "<td>0.2568</td>\n",
       "<td> (3403.0/13253.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1367.0</td>\n",
       "<td>1582.0</td>\n",
       "<td>0.4635</td>\n",
       "<td> (1367.0/2949.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11217.0</td>\n",
       "<td>4985.0</td>\n",
       "<td>0.2944</td>\n",
       "<td> (4770.0/16202.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      9850   3403  0.2568   (3403.0/13253.0)\n",
       "1      1367   1582  0.4635   (1367.0/2949.0)\n",
       "Total  11217  4985  0.2944   (4770.0/16202.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1953534</td>\n",
       "<td>0.3987900</td>\n",
       "<td>248.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1166556</td>\n",
       "<td>0.5648862</td>\n",
       "<td>337.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2709433</td>\n",
       "<td>0.3762722</td>\n",
       "<td>185.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6365348</td>\n",
       "<td>0.8189730</td>\n",
       "<td>28.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7519927</td>\n",
       "<td>0.6470588</td>\n",
       "<td>7.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0720133</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8355440</td>\n",
       "<td>0.9999245</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2288610</td>\n",
       "<td>0.2366986</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1667771</td>\n",
       "<td>0.6414397</td>\n",
       "<td>277.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1542798</td>\n",
       "<td>0.6472159</td>\n",
       "<td>291.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.195353     0.39879   248\n",
       "max f2                       0.116656     0.564886  337\n",
       "max f0point5                 0.270943     0.376272  185\n",
       "max accuracy                 0.636535     0.818973  28\n",
       "max precision                0.751993     0.647059  7\n",
       "max recall                   0.0720133    1         397\n",
       "max specificity              0.835544     0.999925  0\n",
       "max absolute_mcc             0.228861     0.236699  218\n",
       "max min_per_class_accuracy   0.166777     0.64144   277\n",
       "max mean_per_class_accuracy  0.15428      0.647216  291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.20 %, avg score: 18.47 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100605</td>\n",
       "<td>0.6070463</td>\n",
       "<td>2.9324155</td>\n",
       "<td>2.9324155</td>\n",
       "<td>0.5337423</td>\n",
       "<td>0.6747785</td>\n",
       "<td>0.5337423</td>\n",
       "<td>0.6747785</td>\n",
       "<td>0.0295015</td>\n",
       "<td>0.0295015</td>\n",
       "<td>193.2415480</td>\n",
       "<td>193.2415480</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200593</td>\n",
       "<td>0.5374457</td>\n",
       "<td>2.6792049</td>\n",
       "<td>2.8061998</td>\n",
       "<td>0.4876543</td>\n",
       "<td>0.5705091</td>\n",
       "<td>0.5107692</td>\n",
       "<td>0.6228042</td>\n",
       "<td>0.0267887</td>\n",
       "<td>0.0562903</td>\n",
       "<td>167.9204920</td>\n",
       "<td>180.6199755</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300580</td>\n",
       "<td>0.4880498</td>\n",
       "<td>2.4418070</td>\n",
       "<td>2.6849849</td>\n",
       "<td>0.4444444</td>\n",
       "<td>0.5106372</td>\n",
       "<td>0.4887064</td>\n",
       "<td>0.5854920</td>\n",
       "<td>0.0244151</td>\n",
       "<td>0.0807053</td>\n",
       "<td>144.1807016</td>\n",
       "<td>168.4984922</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400568</td>\n",
       "<td>0.4585920</td>\n",
       "<td>2.5096350</td>\n",
       "<td>2.6412150</td>\n",
       "<td>0.4567901</td>\n",
       "<td>0.4730993</td>\n",
       "<td>0.4807396</td>\n",
       "<td>0.5574371</td>\n",
       "<td>0.0250933</td>\n",
       "<td>0.1057986</td>\n",
       "<td>150.9634988</td>\n",
       "<td>164.1214984</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500555</td>\n",
       "<td>0.4315051</td>\n",
       "<td>2.1026672</td>\n",
       "<td>2.5336382</td>\n",
       "<td>0.3827160</td>\n",
       "<td>0.4444311</td>\n",
       "<td>0.4611591</td>\n",
       "<td>0.5348638</td>\n",
       "<td>0.0210241</td>\n",
       "<td>0.1268227</td>\n",
       "<td>110.2667152</td>\n",
       "<td>153.3638229</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000494</td>\n",
       "<td>0.3337569</td>\n",
       "<td>2.0280564</td>\n",
       "<td>2.2810033</td>\n",
       "<td>0.3691358</td>\n",
       "<td>0.3752998</td>\n",
       "<td>0.4151758</td>\n",
       "<td>0.4551310</td>\n",
       "<td>0.1013903</td>\n",
       "<td>0.2282130</td>\n",
       "<td>102.8056382</td>\n",
       "<td>128.1003253</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500432</td>\n",
       "<td>0.2806429</td>\n",
       "<td>1.8313553</td>\n",
       "<td>2.1311822</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.3060461</td>\n",
       "<td>0.3879062</td>\n",
       "<td>0.4054565</td>\n",
       "<td>0.0915565</td>\n",
       "<td>0.3197694</td>\n",
       "<td>83.1355262</td>\n",
       "<td>113.1182244</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000370</td>\n",
       "<td>0.2443961</td>\n",
       "<td>1.5939574</td>\n",
       "<td>1.9969175</td>\n",
       "<td>0.2901235</td>\n",
       "<td>0.2612294</td>\n",
       "<td>0.3634681</td>\n",
       "<td>0.3694109</td>\n",
       "<td>0.0796880</td>\n",
       "<td>0.3994574</td>\n",
       "<td>59.3957357</td>\n",
       "<td>99.6917462</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000247</td>\n",
       "<td>0.1976308</td>\n",
       "<td>1.2582089</td>\n",
       "<td>1.7507319</td>\n",
       "<td>0.2290123</td>\n",
       "<td>0.2187809</td>\n",
       "<td>0.3186587</td>\n",
       "<td>0.3192112</td>\n",
       "<td>0.1258054</td>\n",
       "<td>0.5252628</td>\n",
       "<td>25.8208893</td>\n",
       "<td>75.0731928</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000123</td>\n",
       "<td>0.1687228</td>\n",
       "<td>1.0988132</td>\n",
       "<td>1.5877774</td>\n",
       "<td>0.2</td>\n",
       "<td>0.1820912</td>\n",
       "<td>0.2889986</td>\n",
       "<td>0.2849365</td>\n",
       "<td>0.1098678</td>\n",
       "<td>0.6351306</td>\n",
       "<td>9.8813157</td>\n",
       "<td>58.7777382</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1468073</td>\n",
       "<td>1.0174196</td>\n",
       "<td>1.4737199</td>\n",
       "<td>0.1851852</td>\n",
       "<td>0.1575376</td>\n",
       "<td>0.2682385</td>\n",
       "<td>0.2594599</td>\n",
       "<td>0.1017294</td>\n",
       "<td>0.7368600</td>\n",
       "<td>1.7419590</td>\n",
       "<td>47.3719905</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999877</td>\n",
       "<td>0.1302072</td>\n",
       "<td>0.7935873</td>\n",
       "<td>1.3603761</td>\n",
       "<td>0.1444444</td>\n",
       "<td>0.1384109</td>\n",
       "<td>0.2476083</td>\n",
       "<td>0.2392871</td>\n",
       "<td>0.0793489</td>\n",
       "<td>0.8162089</td>\n",
       "<td>-20.6412720</td>\n",
       "<td>36.0376128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999753</td>\n",
       "<td>0.1160390</td>\n",
       "<td>0.6952367</td>\n",
       "<td>1.2653646</td>\n",
       "<td>0.1265432</td>\n",
       "<td>0.1229040</td>\n",
       "<td>0.2303148</td>\n",
       "<td>0.2226624</td>\n",
       "<td>0.0695151</td>\n",
       "<td>0.8857240</td>\n",
       "<td>-30.4763280</td>\n",
       "<td>26.5364591</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999630</td>\n",
       "<td>0.1030191</td>\n",
       "<td>0.5460152</td>\n",
       "<td>1.1754529</td>\n",
       "<td>0.0993827</td>\n",
       "<td>0.1093927</td>\n",
       "<td>0.2139495</td>\n",
       "<td>0.2085048</td>\n",
       "<td>0.0545948</td>\n",
       "<td>0.9403188</td>\n",
       "<td>-45.3984820</td>\n",
       "<td>17.5452853</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999506</td>\n",
       "<td>0.0899922</td>\n",
       "<td>0.3866194</td>\n",
       "<td>1.0878107</td>\n",
       "<td>0.0703704</td>\n",
       "<td>0.0965274</td>\n",
       "<td>0.1979974</td>\n",
       "<td>0.1960637</td>\n",
       "<td>0.0386572</td>\n",
       "<td>0.9789759</td>\n",
       "<td>-61.3380556</td>\n",
       "<td>8.7810707</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0661632</td>\n",
       "<td>0.2101370</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0382480</td>\n",
       "<td>0.0822573</td>\n",
       "<td>0.1820146</td>\n",
       "<td>0.1846775</td>\n",
       "<td>0.0210241</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.9862999</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100605                   0.607046           2.93242   2.93242            0.533742         0.674778   0.533742                    0.674778            0.0295015       0.0295015                  193.242   193.242\n",
       "    2        0.0200593                   0.537446           2.6792    2.8062             0.487654         0.570509   0.510769                    0.622804            0.0267887       0.0562903                  167.92    180.62\n",
       "    3        0.030058                    0.48805            2.44181   2.68498            0.444444         0.510637   0.488706                    0.585492            0.0244151       0.0807053                  144.181   168.498\n",
       "    4        0.0400568                   0.458592           2.50963   2.64121            0.45679          0.473099   0.48074                     0.557437            0.0250933       0.105799                   150.963   164.121\n",
       "    5        0.0500555                   0.431505           2.10267   2.53364            0.382716         0.444431   0.461159                    0.534864            0.0210241       0.126823                   110.267   153.364\n",
       "    6        0.100049                    0.333757           2.02806   2.281              0.369136         0.3753     0.415176                    0.455131            0.10139         0.228213                   102.806   128.1\n",
       "    7        0.150043                    0.280643           1.83136   2.13118            0.333333         0.306046   0.387906                    0.405457            0.0915565       0.319769                   83.1355   113.118\n",
       "    8        0.200037                    0.244396           1.59396   1.99692            0.290123         0.261229   0.363468                    0.369411            0.079688        0.399457                   59.3957   99.6917\n",
       "    9        0.300025                    0.197631           1.25821   1.75073            0.229012         0.218781   0.318659                    0.319211            0.125805        0.525263                   25.8209   75.0732\n",
       "    10       0.400012                    0.168723           1.09881   1.58778            0.2              0.182091   0.288999                    0.284937            0.109868        0.635131                   9.88132   58.7777\n",
       "    11       0.5                         0.146807           1.01742   1.47372            0.185185         0.157538   0.268238                    0.25946             0.101729        0.73686                    1.74196   47.372\n",
       "    12       0.599988                    0.130207           0.793587  1.36038            0.144444         0.138411   0.247608                    0.239287            0.0793489       0.816209                   -20.6413  36.0376\n",
       "    13       0.699975                    0.116039           0.695237  1.26536            0.126543         0.122904   0.230315                    0.222662            0.0695151       0.885724                   -30.4763  26.5365\n",
       "    14       0.799963                    0.103019           0.546015  1.17545            0.0993827        0.109393   0.21395                     0.208505            0.0545948       0.940319                   -45.3985  17.5453\n",
       "    15       0.899951                    0.0899922          0.386619  1.08781            0.0703704        0.0965274  0.197997                    0.196064            0.0386572       0.978976                   -61.3381  8.78107\n",
       "    16       1                           0.0661632          0.210137  1                  0.038248         0.0822573  0.182015                    0.184677            0.0210241       1                          -78.9863  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.13857609376371857\n",
      "RMSE: 0.3722581010048251\n",
      "LogLoss: 0.4401913910510862\n",
      "Null degrees of freedom: 147784\n",
      "Residual degrees of freedom: 147781\n",
      "Null deviance: 140733.9728124682\n",
      "Residual deviance: 130107.36945296955\n",
      "AIC: 130115.36945296955\n",
      "AUC: 0.7020527753495225\n",
      "pr_auc: 0.3375985314167658\n",
      "Gini: 0.40410555069904497\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.17298272394626243: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>81649.0</td>\n",
       "<td>39069.0</td>\n",
       "<td>0.3236</td>\n",
       "<td> (39069.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>10296.0</td>\n",
       "<td>16771.0</td>\n",
       "<td>0.3804</td>\n",
       "<td> (10296.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>91945.0</td>\n",
       "<td>55840.0</td>\n",
       "<td>0.334</td>\n",
       "<td> (49365.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      81649  39069  0.3236   (39069.0/120718.0)\n",
       "1      10296  16771  0.3804   (10296.0/27067.0)\n",
       "Total  91945  55840  0.334    (49365.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1729827</td>\n",
       "<td>0.4045738</td>\n",
       "<td>275.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1166841</td>\n",
       "<td>0.5633569</td>\n",
       "<td>341.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2578390</td>\n",
       "<td>0.3662781</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6424048</td>\n",
       "<td>0.8173698</td>\n",
       "<td>31.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8611637</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0700469</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8611637</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1825932</td>\n",
       "<td>0.2367313</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1657609</td>\n",
       "<td>0.6461004</td>\n",
       "<td>282.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1604917</td>\n",
       "<td>0.6483125</td>\n",
       "<td>288.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.172983     0.404574  275\n",
       "max f2                       0.116684     0.563357  341\n",
       "max f0point5                 0.257839     0.366278  202\n",
       "max accuracy                 0.642405     0.81737   31\n",
       "max precision                0.861164     1         0\n",
       "max recall                   0.0700469    1         397\n",
       "max specificity              0.861164     1         0\n",
       "max absolute_mcc             0.182593     0.236731  265\n",
       "max min_per_class_accuracy   0.165761     0.6461    282\n",
       "max mean_per_class_accuracy  0.160492     0.648312  288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.32 %, avg score: 18.32 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100010</td>\n",
       "<td>0.5992652</td>\n",
       "<td>2.8149506</td>\n",
       "<td>2.8149506</td>\n",
       "<td>0.5155616</td>\n",
       "<td>0.6651941</td>\n",
       "<td>0.5155616</td>\n",
       "<td>0.6651941</td>\n",
       "<td>0.0281524</td>\n",
       "<td>0.0281524</td>\n",
       "<td>181.4950551</td>\n",
       "<td>181.4950551</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200020</td>\n",
       "<td>0.5304044</td>\n",
       "<td>2.6006892</td>\n",
       "<td>2.7078199</td>\n",
       "<td>0.4763194</td>\n",
       "<td>0.5618491</td>\n",
       "<td>0.4959405</td>\n",
       "<td>0.6135216</td>\n",
       "<td>0.0260095</td>\n",
       "<td>0.0541619</td>\n",
       "<td>160.0689223</td>\n",
       "<td>170.7819887</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300030</td>\n",
       "<td>0.4841323</td>\n",
       "<td>2.3938162</td>\n",
       "<td>2.6031520</td>\n",
       "<td>0.4384303</td>\n",
       "<td>0.5064587</td>\n",
       "<td>0.4767704</td>\n",
       "<td>0.5778339</td>\n",
       "<td>0.0239406</td>\n",
       "<td>0.0781025</td>\n",
       "<td>139.3816217</td>\n",
       "<td>160.3151997</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400041</td>\n",
       "<td>0.4512997</td>\n",
       "<td>2.2201907</td>\n",
       "<td>2.5074117</td>\n",
       "<td>0.4066306</td>\n",
       "<td>0.4675155</td>\n",
       "<td>0.4592355</td>\n",
       "<td>0.5502543</td>\n",
       "<td>0.0222042</td>\n",
       "<td>0.1003066</td>\n",
       "<td>122.0190658</td>\n",
       "<td>150.7411662</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500051</td>\n",
       "<td>0.4240390</td>\n",
       "<td>2.2682147</td>\n",
       "<td>2.4595723</td>\n",
       "<td>0.4154263</td>\n",
       "<td>0.4372611</td>\n",
       "<td>0.4504736</td>\n",
       "<td>0.5276557</td>\n",
       "<td>0.0226844</td>\n",
       "<td>0.1229911</td>\n",
       "<td>126.8214749</td>\n",
       "<td>145.9572280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000034</td>\n",
       "<td>0.3321782</td>\n",
       "<td>2.0098955</td>\n",
       "<td>2.2347491</td>\n",
       "<td>0.3681148</td>\n",
       "<td>0.3726242</td>\n",
       "<td>0.4092970</td>\n",
       "<td>0.4501452</td>\n",
       "<td>0.1004914</td>\n",
       "<td>0.2234825</td>\n",
       "<td>100.9895466</td>\n",
       "<td>123.4749086</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500017</td>\n",
       "<td>0.2786483</td>\n",
       "<td>1.7675257</td>\n",
       "<td>2.0790150</td>\n",
       "<td>0.3237245</td>\n",
       "<td>0.3035444</td>\n",
       "<td>0.3807741</td>\n",
       "<td>0.4012804</td>\n",
       "<td>0.0883733</td>\n",
       "<td>0.3118558</td>\n",
       "<td>76.7525718</td>\n",
       "<td>107.9014989</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2425797</td>\n",
       "<td>1.5857484</td>\n",
       "<td>1.9557025</td>\n",
       "<td>0.2904317</td>\n",
       "<td>0.2595146</td>\n",
       "<td>0.3581893</td>\n",
       "<td>0.3658402</td>\n",
       "<td>0.0792847</td>\n",
       "<td>0.3911405</td>\n",
       "<td>58.5748408</td>\n",
       "<td>95.5702516</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000034</td>\n",
       "<td>0.1959714</td>\n",
       "<td>1.3695181</td>\n",
       "<td>1.7603033</td>\n",
       "<td>0.2508289</td>\n",
       "<td>0.2172719</td>\n",
       "<td>0.3224017</td>\n",
       "<td>0.3163163</td>\n",
       "<td>0.1369564</td>\n",
       "<td>0.5280969</td>\n",
       "<td>36.9518079</td>\n",
       "<td>76.0303297</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1667087</td>\n",
       "<td>1.1305662</td>\n",
       "<td>1.6028743</td>\n",
       "<td>0.2070646</td>\n",
       "<td>0.1805241</td>\n",
       "<td>0.2935684</td>\n",
       "<td>0.2823694</td>\n",
       "<td>0.1130528</td>\n",
       "<td>0.6411497</td>\n",
       "<td>13.0566200</td>\n",
       "<td>60.2874349</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000034</td>\n",
       "<td>0.1457423</td>\n",
       "<td>0.9420748</td>\n",
       "<td>1.4707109</td>\n",
       "<td>0.1725421</td>\n",
       "<td>0.1557592</td>\n",
       "<td>0.2693625</td>\n",
       "<td>0.2570467</td>\n",
       "<td>0.0942107</td>\n",
       "<td>0.7353604</td>\n",
       "<td>-5.7925249</td>\n",
       "<td>47.0710852</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1292848</td>\n",
       "<td>0.7906574</td>\n",
       "<td>1.3573724</td>\n",
       "<td>0.1448099</td>\n",
       "<td>0.1372581</td>\n",
       "<td>0.2486044</td>\n",
       "<td>0.2370826</td>\n",
       "<td>0.0790631</td>\n",
       "<td>0.8144235</td>\n",
       "<td>-20.9342592</td>\n",
       "<td>35.7372446</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999966</td>\n",
       "<td>0.1152511</td>\n",
       "<td>0.6683641</td>\n",
       "<td>1.2589455</td>\n",
       "<td>0.1224117</td>\n",
       "<td>0.1221946</td>\n",
       "<td>0.2305774</td>\n",
       "<td>0.2206705</td>\n",
       "<td>0.0668342</td>\n",
       "<td>0.8812576</td>\n",
       "<td>-33.1635864</td>\n",
       "<td>25.8945542</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.1023359</td>\n",
       "<td>0.5445562</td>\n",
       "<td>1.1696438</td>\n",
       "<td>0.0997361</td>\n",
       "<td>0.1087495</td>\n",
       "<td>0.2142217</td>\n",
       "<td>0.2066799</td>\n",
       "<td>0.0544575</td>\n",
       "<td>0.9357151</td>\n",
       "<td>-45.5443850</td>\n",
       "<td>16.9643847</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999966</td>\n",
       "<td>0.0897076</td>\n",
       "<td>0.4208219</td>\n",
       "<td>1.0864439</td>\n",
       "<td>0.0770740</td>\n",
       "<td>0.0960452</td>\n",
       "<td>0.1989835</td>\n",
       "<td>0.1943875</td>\n",
       "<td>0.0420808</td>\n",
       "<td>0.9777958</td>\n",
       "<td>-57.9178137</td>\n",
       "<td>8.6443906</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0634272</td>\n",
       "<td>0.2220341</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0406658</td>\n",
       "<td>0.0820197</td>\n",
       "<td>0.1831512</td>\n",
       "<td>0.1831504</td>\n",
       "<td>0.0222042</td>\n",
       "<td>1.0</td>\n",
       "<td>-77.7965912</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010001                    0.599265           2.81495   2.81495            0.515562         0.665194   0.515562                    0.665194            0.0281524       0.0281524                  181.495   181.495\n",
       "    2        0.020002                    0.530404           2.60069   2.70782            0.476319         0.561849   0.49594                     0.613522            0.0260095       0.0541619                  160.069   170.782\n",
       "    3        0.030003                    0.484132           2.39382   2.60315            0.43843          0.506459   0.47677                     0.577834            0.0239406       0.0781025                  139.382   160.315\n",
       "    4        0.0400041                   0.4513             2.22019   2.50741            0.406631         0.467515   0.459235                    0.550254            0.0222042       0.100307                   122.019   150.741\n",
       "    5        0.0500051                   0.424039           2.26821   2.45957            0.415426         0.437261   0.450474                    0.527656            0.0226844       0.122991                   126.821   145.957\n",
       "    6        0.100003                    0.332178           2.0099    2.23475            0.368115         0.372624   0.409297                    0.450145            0.100491        0.223482                   100.99    123.475\n",
       "    7        0.150002                    0.278648           1.76753   2.07901            0.323724         0.303544   0.380774                    0.40128             0.0883733       0.311856                   76.7526   107.901\n",
       "    8        0.2                         0.24258            1.58575   1.9557             0.290432         0.259515   0.358189                    0.36584             0.0792847       0.391141                   58.5748   95.5703\n",
       "    9        0.300003                    0.195971           1.36952   1.7603             0.250829         0.217272   0.322402                    0.316316            0.136956        0.528097                   36.9518   76.0303\n",
       "    10       0.4                         0.166709           1.13057   1.60287            0.207065         0.180524   0.293568                    0.282369            0.113053        0.64115                    13.0566   60.2874\n",
       "    11       0.500003                    0.145742           0.942075  1.47071            0.172542         0.155759   0.269362                    0.257047            0.0942107       0.73536                    -5.79252  47.0711\n",
       "    12       0.6                         0.129285           0.790657  1.35737            0.14481          0.137258   0.248604                    0.237083            0.0790631       0.814423                   -20.9343  35.7372\n",
       "    13       0.699997                    0.115251           0.668364  1.25895            0.122412         0.122195   0.230577                    0.220671            0.0668342       0.881258                   -33.1636  25.8946\n",
       "    14       0.8                         0.102336           0.544556  1.16964            0.0997361        0.10875    0.214222                    0.20668             0.0544575       0.935715                   -45.5444  16.9644\n",
       "    15       0.899997                    0.0897076          0.420822  1.08644            0.077074         0.0960452  0.198984                    0.194388            0.0420808       0.977796                   -57.9178  8.64439\n",
       "    16       1                           0.0634272          0.222034  1                  0.0406658        0.0820197  0.183151                    0.18315             0.0222042       1                          -77.7966  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stackedensemble'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_and_save_parm',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'auc',\n",
       " 'base_models',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'confusion_matrix',\n",
       " 'convert_H2OXGBoostParams_2_XGBoostParams',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_levelone_frame',\n",
       " 'levelone_frame_id',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metalearner',\n",
       " 'metalearner_algorithm',\n",
       " 'metalearner_fold_assignment',\n",
       " 'metalearner_fold_column',\n",
       " 'metalearner_nfolds',\n",
       " 'metalearner_params',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'staged_predict_proba',\n",
       " 'start',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(aml.leader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Exploration\n",
    "\n",
    "To understand how the ensemble works, let's take a peek inside the Stacked Ensemble \"All Models\" model.  The \"All Models\" ensemble is an ensemble of all of the individual models in the AutoML run.  This is often the top performing model on the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>auc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>mean_per_class_error</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StackedEnsemble_AllModels_AutoML_20190108_000309</td>\n",
       "      <td>0.702053</td>\n",
       "      <td>0.440191</td>\n",
       "      <td>0.352014</td>\n",
       "      <td>0.372258</td>\n",
       "      <td>0.138576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StackedEnsemble_BestOfFamily_AutoML_20190108_0...</td>\n",
       "      <td>0.702053</td>\n",
       "      <td>0.440191</td>\n",
       "      <td>0.352014</td>\n",
       "      <td>0.372258</td>\n",
       "      <td>0.138576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GLM_grid_1_AutoML_20190108_000309_model_1</td>\n",
       "      <td>0.698362</td>\n",
       "      <td>0.439271</td>\n",
       "      <td>0.354907</td>\n",
       "      <td>0.371955</td>\n",
       "      <td>0.138351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XRT_1_AutoML_20190108_000309</td>\n",
       "      <td>0.688334</td>\n",
       "      <td>0.472261</td>\n",
       "      <td>0.364171</td>\n",
       "      <td>0.382791</td>\n",
       "      <td>0.146529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DRF_1_AutoML_20190108_000309</td>\n",
       "      <td>0.686498</td>\n",
       "      <td>0.478133</td>\n",
       "      <td>0.364403</td>\n",
       "      <td>0.383717</td>\n",
       "      <td>0.147239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_id       auc   logloss  \\\n",
       "0   StackedEnsemble_AllModels_AutoML_20190108_000309  0.702053  0.440191   \n",
       "1  StackedEnsemble_BestOfFamily_AutoML_20190108_0...  0.702053  0.440191   \n",
       "2          GLM_grid_1_AutoML_20190108_000309_model_1  0.698362  0.439271   \n",
       "3                       XRT_1_AutoML_20190108_000309  0.688334  0.472261   \n",
       "4                       DRF_1_AutoML_20190108_000309  0.686498  0.478133   \n",
       "\n",
       "   mean_per_class_error      rmse       mse  \n",
       "0              0.352014  0.372258  0.138576  \n",
       "1              0.352014  0.372258  0.138576  \n",
       "2              0.354907  0.371955  0.138351  \n",
       "3              0.364171  0.382791  0.146529  \n",
       "4              0.364403  0.383717  0.147239  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml_leaderboard_df=aml.leaderboard.as_data_frame()\n",
    "aml_leaderboard_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting models\n",
    "\n",
    "Individul models can ne found through a search of the leader board or directly by the name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get model ids for all models in the AutoML Leaderboard\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "# Get the \"All Models\" Stacked Ensemble model\n",
    "se = h2o.get_model([mid for mid in model_ids if \"StackedEnsemble_AllModels\" in mid][0])\n",
    "# Get the Stacked Ensemble metalearner model\n",
    "metalearner = h2o.get_model(aml.leader.metalearner()['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DRF_1_AutoML_20190108_000309': 0.12995000876961782,\n",
       " 'GLM_grid_1_AutoML_20190108_000309_model_1': 0.41137246925237375,\n",
       " 'Intercept': -1.6206460295544853,\n",
       " 'XRT_1_AutoML_20190108_000309': 0.14566705411351252}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metalearner.coef_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bear/anaconda/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:107: MatplotlibDeprecationWarning: Passing one of 'on', 'true', 'off', 'false' as a boolean is deprecated; use an actual boolean (True/False) instead.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBsAAAJTCAYAAABJmR7/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "metalearner.std_coef_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting a model directly by name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>auc</th>\n",
       "      <th>logloss</th>\n",
       "      <th>mean_per_class_error</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StackedEnsemble_AllModels_AutoML_20190108_000309</td>\n",
       "      <td>0.702053</td>\n",
       "      <td>0.440191</td>\n",
       "      <td>0.352014</td>\n",
       "      <td>0.372258</td>\n",
       "      <td>0.138576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>StackedEnsemble_BestOfFamily_AutoML_20190108_0...</td>\n",
       "      <td>0.702053</td>\n",
       "      <td>0.440191</td>\n",
       "      <td>0.352014</td>\n",
       "      <td>0.372258</td>\n",
       "      <td>0.138576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GLM_grid_1_AutoML_20190108_000309_model_1</td>\n",
       "      <td>0.698362</td>\n",
       "      <td>0.439271</td>\n",
       "      <td>0.354907</td>\n",
       "      <td>0.371955</td>\n",
       "      <td>0.138351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XRT_1_AutoML_20190108_000309</td>\n",
       "      <td>0.688334</td>\n",
       "      <td>0.472261</td>\n",
       "      <td>0.364171</td>\n",
       "      <td>0.382791</td>\n",
       "      <td>0.146529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DRF_1_AutoML_20190108_000309</td>\n",
       "      <td>0.686498</td>\n",
       "      <td>0.478133</td>\n",
       "      <td>0.364403</td>\n",
       "      <td>0.383717</td>\n",
       "      <td>0.147239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_id       auc   logloss  \\\n",
       "0   StackedEnsemble_AllModels_AutoML_20190108_000309  0.702053  0.440191   \n",
       "1  StackedEnsemble_BestOfFamily_AutoML_20190108_0...  0.702053  0.440191   \n",
       "2          GLM_grid_1_AutoML_20190108_000309_model_1  0.698362  0.439271   \n",
       "3                       XRT_1_AutoML_20190108_000309  0.688334  0.472261   \n",
       "4                       DRF_1_AutoML_20190108_000309  0.686498  0.478133   \n",
       "\n",
       "   mean_per_class_error      rmse       mse  \n",
       "0              0.352014  0.372258  0.138576  \n",
       "1              0.352014  0.372258  0.138576  \n",
       "2              0.354907  0.371955  0.138351  \n",
       "3              0.364171  0.382791  0.146529  \n",
       "4              0.364403  0.383717  0.147239  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml_leaderboard_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM_grid_1_AutoML_20190108_000309_model_1\n",
      "XRT_1_AutoML_20190108_000309\n",
      "DRF_1_AutoML_20190108_000309\n",
      "model_id  GLM_grid_1_AutoML_20190108_000309_model_1\n"
     ]
    }
   ],
   "source": [
    "m_id=''\n",
    "for model in aml_leaderboard_df['model_id']:\n",
    "    if 'StackedEnsemble' not in model:\n",
    "      print (model)\n",
    "      if m_id=='':\n",
    "            m_id=model\n",
    "print (\"model_id \", m_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm\n"
     ]
    }
   ],
   "source": [
    "non_stacked= h2o.get_model(m_id)\n",
    "print (non_stacked.algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " 'Lambda',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_and_save_parm',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'alpha',\n",
       " 'auc',\n",
       " 'balance_classes',\n",
       " 'beta_constraints',\n",
       " 'beta_epsilon',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'class_sampling_factors',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'compute_p_values',\n",
       " 'confusion_matrix',\n",
       " 'convert_H2OXGBoostParams_2_XGBoostParams',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'custom_metric_func',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'early_stopping',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'family',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fold_assignment',\n",
       " 'fold_column',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'getGLMRegularizationPath',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'gradient_epsilon',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'ignore_const_cols',\n",
       " 'ignored_columns',\n",
       " 'interaction_pairs',\n",
       " 'interactions',\n",
       " 'intercept',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_cross_validation_fold_assignment',\n",
       " 'keep_cross_validation_models',\n",
       " 'keep_cross_validation_predictions',\n",
       " 'lambda_',\n",
       " 'lambda_min_ratio',\n",
       " 'lambda_search',\n",
       " 'link',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'makeGLMModel',\n",
       " 'max_active_predictors',\n",
       " 'max_after_balance_size',\n",
       " 'max_confusion_matrix_size',\n",
       " 'max_hit_ratio_k',\n",
       " 'max_iterations',\n",
       " 'max_per_class_error',\n",
       " 'max_runtime_secs',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metric',\n",
       " 'missing_values_handling',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'nfolds',\n",
       " 'nlambdas',\n",
       " 'non_negative',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'obj_reg',\n",
       " 'objective_epsilon',\n",
       " 'offset_column',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'prior',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'remove_collinear_columns',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_each_iteration',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'solver',\n",
       " 'specificity',\n",
       " 'staged_predict_proba',\n",
       " 'standardize',\n",
       " 'start',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'tweedie_link_power',\n",
       " 'tweedie_variance_power',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'weights_column',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(non_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since this is a pandas dataframe the data can be saved.\n",
    "\n",
    "The type of exploration depends on the learner.  If the learner isn't an ensemble then ensemble exploration doesn't make sense.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the variable importance of the metalearner (combiner) algorithm in the ensemble.  This shows us how much each base learner is contributing to the ensemble. The AutoML Stacked Ensembles use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the metalearner is actually the standardized coefficient magnitudes of the GLM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Leader Model\n",
    "\n",
    "There are two ways to save the leader model -- binary format and MOJO format.  If you're taking your leader model to production, then we'd suggest the MOJO format since it's optimized for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bear/Documents/CSYE_7245/H2O/models/StackedEnsemble_AllModels_AutoML_20190108_000309'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.save_model(aml.leader, path = \"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/bear/Documents/CSYE_7245/H2O/models/StackedEnsemble_AllModels_AutoML_20190108_000309.zip'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.download_mojo(path = \"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "If one wants predictions the user will do this on new data.\n",
    "\n",
    "Here we are taking 10% of original file just to show the syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into training and test for showing how to predict\n",
    "train, test = df.split_frame([0.8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Using Leader Model\n",
    "\n",
    "If you need to generate predictions on a test set, you can make predictions on the `\"H2OAutoML\"` object directly, or on the leader model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackedensemble prediction progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  predict</th><th style=\"text-align: right;\">      p0</th><th style=\"text-align: right;\">       p1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.870901</td><td style=\"text-align: right;\">0.129099 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.829689</td><td style=\"text-align: right;\">0.170311 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.8714  </td><td style=\"text-align: right;\">0.1286   </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.792068</td><td style=\"text-align: right;\">0.207932 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.881985</td><td style=\"text-align: right;\">0.118015 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.902756</td><td style=\"text-align: right;\">0.0972436</td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.790041</td><td style=\"text-align: right;\">0.209959 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.849584</td><td style=\"text-align: right;\">0.150416 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        0</td><td style=\"text-align: right;\">0.87083 </td><td style=\"text-align: right;\">0.12917  </td></tr>\n",
       "<tr><td style=\"text-align: right;\">        1</td><td style=\"text-align: right;\">0.799034</td><td style=\"text-align: right;\">0.200966 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = aml.predict(test)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model_performance()\n",
    "\n",
    "The standard `model_performance()` method can be applied to the AutoML leader model and a test set to generate an H2O model performance object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on test data. **\n",
      "\n",
      "MSE: 0.07990584759654235\n",
      "RMSE: 0.28267622396753206\n",
      "LogLoss: 0.29029339065541043\n",
      "Null degrees of freedom: 32806\n",
      "Residual degrees of freedom: 32803\n",
      "Null deviance: 31036.082029430923\n",
      "Residual deviance: 19047.3105344641\n",
      "AIC: 19055.3105344641\n",
      "AUC: 0.9488501439752044\n",
      "pr_auc: 0.8533316712111314\n",
      "Gini: 0.8977002879504088\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2975039531508158: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>25252.0</td>\n",
       "<td>1615.0</td>\n",
       "<td>0.0601</td>\n",
       "<td> (1615.0/26867.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1270.0</td>\n",
       "<td>4670.0</td>\n",
       "<td>0.2138</td>\n",
       "<td> (1270.0/5940.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>26522.0</td>\n",
       "<td>6285.0</td>\n",
       "<td>0.0879</td>\n",
       "<td> (2885.0/32807.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      25252  1615  0.0601   (1615.0/26867.0)\n",
       "1      1270   4670  0.2138   (1270.0/5940.0)\n",
       "Total  26522  6285  0.0879   (2885.0/32807.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.2975040</td>\n",
       "<td>0.7640082</td>\n",
       "<td>210.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2215830</td>\n",
       "<td>0.8121077</td>\n",
       "<td>254.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4122572</td>\n",
       "<td>0.8071117</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.3616531</td>\n",
       "<td>0.9162069</td>\n",
       "<td>179.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9651071</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0783171</td>\n",
       "<td>1.0</td>\n",
       "<td>391.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9651071</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2975040</td>\n",
       "<td>0.7104510</td>\n",
       "<td>210.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2349870</td>\n",
       "<td>0.8794613</td>\n",
       "<td>246.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2315331</td>\n",
       "<td>0.8801173</td>\n",
       "<td>248.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.297504     0.764008  210\n",
       "max f2                       0.221583     0.812108  254\n",
       "max f0point5                 0.412257     0.807112  158\n",
       "max accuracy                 0.361653     0.916207  179\n",
       "max precision                0.965107     1         0\n",
       "max recall                   0.0783171    1         391\n",
       "max specificity              0.965107     1         0\n",
       "max absolute_mcc             0.297504     0.710451  210\n",
       "max min_per_class_accuracy   0.234987     0.879461  246\n",
       "max mean_per_class_accuracy  0.231533     0.880117  248"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.11 %, avg score: 21.19 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100283</td>\n",
       "<td>0.8100592</td>\n",
       "<td>5.5230640</td>\n",
       "<td>5.5230640</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8597813</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8597813</td>\n",
       "<td>0.0553872</td>\n",
       "<td>0.0553872</td>\n",
       "<td>452.3063973</td>\n",
       "<td>452.3063973</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200262</td>\n",
       "<td>0.7543489</td>\n",
       "<td>5.5230640</td>\n",
       "<td>5.5230640</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7802025</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8200524</td>\n",
       "<td>0.0552189</td>\n",
       "<td>0.1106061</td>\n",
       "<td>452.3063973</td>\n",
       "<td>452.3063973</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300241</td>\n",
       "<td>0.7061689</td>\n",
       "<td>5.4388709</td>\n",
       "<td>5.4950281</td>\n",
       "<td>0.9847561</td>\n",
       "<td>0.7297811</td>\n",
       "<td>0.9949239</td>\n",
       "<td>0.7899925</td>\n",
       "<td>0.0543771</td>\n",
       "<td>0.1649832</td>\n",
       "<td>443.8870925</td>\n",
       "<td>449.5028115</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400219</td>\n",
       "<td>0.6665688</td>\n",
       "<td>5.4220323</td>\n",
       "<td>5.4767931</td>\n",
       "<td>0.9817073</td>\n",
       "<td>0.6847444</td>\n",
       "<td>0.9916222</td>\n",
       "<td>0.7637005</td>\n",
       "<td>0.0542088</td>\n",
       "<td>0.2191919</td>\n",
       "<td>442.2032315</td>\n",
       "<td>447.6793064</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500198</td>\n",
       "<td>0.6223473</td>\n",
       "<td>5.2704848</td>\n",
       "<td>5.4355566</td>\n",
       "<td>0.9542683</td>\n",
       "<td>0.6444875</td>\n",
       "<td>0.9841560</td>\n",
       "<td>0.7398725</td>\n",
       "<td>0.0526936</td>\n",
       "<td>0.2718855</td>\n",
       "<td>427.0484828</td>\n",
       "<td>443.5556561</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000091</td>\n",
       "<td>0.4627741</td>\n",
       "<td>4.8259455</td>\n",
       "<td>5.1308439</td>\n",
       "<td>0.8737805</td>\n",
       "<td>0.5397761</td>\n",
       "<td>0.9289851</td>\n",
       "<td>0.6398548</td>\n",
       "<td>0.2412458</td>\n",
       "<td>0.5131313</td>\n",
       "<td>382.5945533</td>\n",
       "<td>413.0843947</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1499985</td>\n",
       "<td>0.3554582</td>\n",
       "<td>3.3879283</td>\n",
       "<td>4.5499901</td>\n",
       "<td>0.6134146</td>\n",
       "<td>0.4063965</td>\n",
       "<td>0.8238163</td>\n",
       "<td>0.5620512</td>\n",
       "<td>0.1693603</td>\n",
       "<td>0.6824916</td>\n",
       "<td>238.7928266</td>\n",
       "<td>354.9990113</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000183</td>\n",
       "<td>0.2868342</td>\n",
       "<td>2.3896255</td>\n",
       "<td>4.0097343</td>\n",
       "<td>0.4326630</td>\n",
       "<td>0.3193148</td>\n",
       "<td>0.7259982</td>\n",
       "<td>0.5013486</td>\n",
       "<td>0.1195286</td>\n",
       "<td>0.8020202</td>\n",
       "<td>138.9625485</td>\n",
       "<td>300.9734344</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.2999970</td>\n",
       "<td>0.2071195</td>\n",
       "<td>1.1399739</td>\n",
       "<td>3.0533419</td>\n",
       "<td>0.2064024</td>\n",
       "<td>0.2420204</td>\n",
       "<td>0.5528348</td>\n",
       "<td>0.4149234</td>\n",
       "<td>0.1139731</td>\n",
       "<td>0.9159933</td>\n",
       "<td>13.9973875</td>\n",
       "<td>205.3341910</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000061</td>\n",
       "<td>0.1677987</td>\n",
       "<td>0.4174702</td>\n",
       "<td>2.3943238</td>\n",
       "<td>0.0755867</td>\n",
       "<td>0.1854381</td>\n",
       "<td>0.4335137</td>\n",
       "<td>0.3575477</td>\n",
       "<td>0.0417508</td>\n",
       "<td>0.9577441</td>\n",
       "<td>-58.2529758</td>\n",
       "<td>139.4323778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000152</td>\n",
       "<td>0.1434788</td>\n",
       "<td>0.1666514</td>\n",
       "<td>1.9487621</td>\n",
       "<td>0.0301737</td>\n",
       "<td>0.1548304</td>\n",
       "<td>0.3528408</td>\n",
       "<td>0.3170018</td>\n",
       "<td>0.0166667</td>\n",
       "<td>0.9744108</td>\n",
       "<td>-83.3348573</td>\n",
       "<td>94.8762148</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999939</td>\n",
       "<td>0.1265203</td>\n",
       "<td>0.0993478</td>\n",
       "<td>1.6405891</td>\n",
       "<td>0.0179878</td>\n",
       "<td>0.1346094</td>\n",
       "<td>0.2970433</td>\n",
       "<td>0.2866092</td>\n",
       "<td>0.0099327</td>\n",
       "<td>0.9843434</td>\n",
       "<td>-90.0652203</td>\n",
       "<td>64.0589060</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7000030</td>\n",
       "<td>0.1122892</td>\n",
       "<td>0.0707006</td>\n",
       "<td>1.4162997</td>\n",
       "<td>0.0128010</td>\n",
       "<td>0.1192714</td>\n",
       "<td>0.2564337</td>\n",
       "<td>0.2627017</td>\n",
       "<td>0.0070707</td>\n",
       "<td>0.9914141</td>\n",
       "<td>-92.9299394</td>\n",
       "<td>41.6299749</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999817</td>\n",
       "<td>0.0996221</td>\n",
       "<td>0.0471481</td>\n",
       "<td>1.2451884</td>\n",
       "<td>0.0085366</td>\n",
       "<td>0.1059546</td>\n",
       "<td>0.2254525</td>\n",
       "<td>0.2431121</td>\n",
       "<td>0.0047138</td>\n",
       "<td>0.9961279</td>\n",
       "<td>-95.2851893</td>\n",
       "<td>24.5188399</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999909</td>\n",
       "<td>0.0877983</td>\n",
       "<td>0.0252502</td>\n",
       "<td>1.1096259</td>\n",
       "<td>0.0045718</td>\n",
       "<td>0.0937108</td>\n",
       "<td>0.2009077</td>\n",
       "<td>0.2265103</td>\n",
       "<td>0.0025253</td>\n",
       "<td>0.9986532</td>\n",
       "<td>-97.4749784</td>\n",
       "<td>10.9625939</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0638572</td>\n",
       "<td>0.0134668</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0024383</td>\n",
       "<td>0.0807270</td>\n",
       "<td>0.1810589</td>\n",
       "<td>0.2119306</td>\n",
       "<td>0.0013468</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.6533218</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100283                   0.810059           5.52306    5.52306            1                0.859781   1                           0.859781            0.0553872       0.0553872                  452.306   452.306\n",
       "    2        0.0200262                   0.754349           5.52306    5.52306            1                0.780202   1                           0.820052            0.0552189       0.110606                   452.306   452.306\n",
       "    3        0.0300241                   0.706169           5.43887    5.49503            0.984756         0.729781   0.994924                    0.789993            0.0543771       0.164983                   443.887   449.503\n",
       "    4        0.0400219                   0.666569           5.42203    5.47679            0.981707         0.684744   0.991622                    0.763701            0.0542088       0.219192                   442.203   447.679\n",
       "    5        0.0500198                   0.622347           5.27048    5.43556            0.954268         0.644488   0.984156                    0.739872            0.0526936       0.271886                   427.048   443.556\n",
       "    6        0.100009                    0.462774           4.82595    5.13084            0.87378          0.539776   0.928985                    0.639855            0.241246        0.513131                   382.595   413.084\n",
       "    7        0.149998                    0.355458           3.38793    4.54999            0.613415         0.406396   0.823816                    0.562051            0.16936         0.682492                   238.793   354.999\n",
       "    8        0.200018                    0.286834           2.38963    4.00973            0.432663         0.319315   0.725998                    0.501349            0.119529        0.80202                    138.963   300.973\n",
       "    9        0.299997                    0.20712            1.13997    3.05334            0.206402         0.24202    0.552835                    0.414923            0.113973        0.915993                   13.9974   205.334\n",
       "    10       0.400006                    0.167799           0.41747    2.39432            0.0755867        0.185438   0.433514                    0.357548            0.0417508       0.957744                   -58.253   139.432\n",
       "    11       0.500015                    0.143479           0.166651   1.94876            0.0301737        0.15483    0.352841                    0.317002            0.0166667       0.974411                   -83.3349  94.8762\n",
       "    12       0.599994                    0.12652            0.0993478  1.64059            0.0179878        0.134609   0.297043                    0.286609            0.00993266      0.984343                   -90.0652  64.0589\n",
       "    13       0.700003                    0.112289           0.0707006  1.4163             0.012801         0.119271   0.256434                    0.262702            0.00707071      0.991414                   -92.9299  41.63\n",
       "    14       0.799982                    0.0996221          0.0471481  1.24519            0.00853659       0.105955   0.225452                    0.243112            0.0047138       0.996128                   -95.2852  24.5188\n",
       "    15       0.899991                    0.0877983          0.0252502  1.10963            0.00457178       0.0937108  0.200908                    0.22651             0.00252525      0.998653                   -97.475   10.9626\n",
       "    16       1                           0.0638572          0.0134668  1                  0.00243828       0.080727   0.181059                    0.211931            0.0013468       1                          -98.6533  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf = aml.leader.model_performance(test)\n",
    "perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_algo',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_has',\n",
       " '_metric_json',\n",
       " '_on_train',\n",
       " '_on_valid',\n",
       " '_on_xval',\n",
       " 'accuracy',\n",
       " 'aic',\n",
       " 'auc',\n",
       " 'confusion_matrix',\n",
       " 'custom_metric_name',\n",
       " 'custom_metric_value',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'fprs',\n",
       " 'gains_lift',\n",
       " 'gini',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'make',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mse',\n",
       " 'nobs',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'plot',\n",
       " 'pr_auc',\n",
       " 'precision',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'sensitivity',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'tprs']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.2975039531508158: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>25252.0</td>\n",
       "<td>1615.0</td>\n",
       "<td>0.0601</td>\n",
       "<td> (1615.0/26867.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1270.0</td>\n",
       "<td>4670.0</td>\n",
       "<td>0.2138</td>\n",
       "<td> (1270.0/5940.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>26522.0</td>\n",
       "<td>6285.0</td>\n",
       "<td>0.0879</td>\n",
       "<td> (2885.0/32807.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      25252  1615  0.0601   (1615.0/26867.0)\n",
       "1      1270   4670  0.2138   (1270.0/5940.0)\n",
       "Total  26522  6285  0.0879   (2885.0/32807.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=perf.confusion_matrix()\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_algo',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_has',\n",
       " '_metric_json',\n",
       " '_on_train',\n",
       " '_on_valid',\n",
       " '_on_xval',\n",
       " 'accuracy',\n",
       " 'aic',\n",
       " 'auc',\n",
       " 'confusion_matrix',\n",
       " 'custom_metric_name',\n",
       " 'custom_metric_value',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'fprs',\n",
       " 'gains_lift',\n",
       " 'gini',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'make',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mse',\n",
       " 'nobs',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'plot',\n",
       " 'pr_auc',\n",
       " 'precision',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'sensitivity',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'tprs']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In R we get plots like:\n",
    "    \n",
    "    #compute performance\n",
    "perf <- h2o.performance(automl_leader,conv_data.hex)\n",
    "h2o.confusionMatrix(perf)\n",
    "h2o.accuracy(perf)\n",
    "h2o.tpr(perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stackedensemble'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F0point5',\n",
       " 'F1',\n",
       " 'F2',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_bc',\n",
       " '_bcin',\n",
       " '_check_and_save_parm',\n",
       " '_check_targets',\n",
       " '_compute_algo',\n",
       " '_estimator_type',\n",
       " '_future',\n",
       " '_get_metrics',\n",
       " '_have_mojo',\n",
       " '_have_pojo',\n",
       " '_id',\n",
       " '_is_xvalidated',\n",
       " '_job',\n",
       " '_keyify_if_h2oframe',\n",
       " '_metrics_class',\n",
       " '_model_json',\n",
       " '_parms',\n",
       " '_plot',\n",
       " '_requires_training_frame',\n",
       " '_resolve_model',\n",
       " '_verify_training_frame_params',\n",
       " '_xval_keys',\n",
       " 'accuracy',\n",
       " 'actual_params',\n",
       " 'aic',\n",
       " 'algo',\n",
       " 'auc',\n",
       " 'base_models',\n",
       " 'biases',\n",
       " 'catoffsets',\n",
       " 'coef',\n",
       " 'coef_norm',\n",
       " 'confusion_matrix',\n",
       " 'convert_H2OXGBoostParams_2_XGBoostParams',\n",
       " 'cross_validation_fold_assignment',\n",
       " 'cross_validation_holdout_predictions',\n",
       " 'cross_validation_metrics_summary',\n",
       " 'cross_validation_models',\n",
       " 'cross_validation_predictions',\n",
       " 'deepfeatures',\n",
       " 'default_params',\n",
       " 'download_mojo',\n",
       " 'download_pojo',\n",
       " 'error',\n",
       " 'fallout',\n",
       " 'find_idx_by_threshold',\n",
       " 'find_threshold_by_max_metric',\n",
       " 'fit',\n",
       " 'fnr',\n",
       " 'fpr',\n",
       " 'full_parameters',\n",
       " 'gains_lift',\n",
       " 'get_params',\n",
       " 'get_xval_models',\n",
       " 'gini',\n",
       " 'have_mojo',\n",
       " 'have_pojo',\n",
       " 'is_cross_validated',\n",
       " 'join',\n",
       " 'keep_levelone_frame',\n",
       " 'levelone_frame_id',\n",
       " 'logloss',\n",
       " 'mae',\n",
       " 'max_per_class_error',\n",
       " 'mcc',\n",
       " 'mean_per_class_error',\n",
       " 'mean_residual_deviance',\n",
       " 'metalearner',\n",
       " 'metalearner_algorithm',\n",
       " 'metalearner_fold_assignment',\n",
       " 'metalearner_fold_column',\n",
       " 'metalearner_nfolds',\n",
       " 'metalearner_params',\n",
       " 'metric',\n",
       " 'missrate',\n",
       " 'mixin',\n",
       " 'model_id',\n",
       " 'model_performance',\n",
       " 'mse',\n",
       " 'normmul',\n",
       " 'normsub',\n",
       " 'null_degrees_of_freedom',\n",
       " 'null_deviance',\n",
       " 'params',\n",
       " 'parms',\n",
       " 'partial_plot',\n",
       " 'plot',\n",
       " 'pprint_coef',\n",
       " 'precision',\n",
       " 'predict',\n",
       " 'predict_leaf_node_assignment',\n",
       " 'r2',\n",
       " 'recall',\n",
       " 'residual_degrees_of_freedom',\n",
       " 'residual_deviance',\n",
       " 'respmul',\n",
       " 'response_column',\n",
       " 'respsub',\n",
       " 'rmse',\n",
       " 'rmsle',\n",
       " 'roc',\n",
       " 'rotation',\n",
       " 'save_model_details',\n",
       " 'save_mojo',\n",
       " 'score_history',\n",
       " 'scoring_history',\n",
       " 'seed',\n",
       " 'sensitivity',\n",
       " 'set_params',\n",
       " 'show',\n",
       " 'specificity',\n",
       " 'staged_predict_proba',\n",
       " 'start',\n",
       " 'std_coef_plot',\n",
       " 'summary',\n",
       " 'tnr',\n",
       " 'tpr',\n",
       " 'train',\n",
       " 'training_frame',\n",
       " 'type',\n",
       " 'validation_frame',\n",
       " 'varimp',\n",
       " 'varimp_plot',\n",
       " 'weights',\n",
       " 'xval_keys',\n",
       " 'xvals']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(aml.leader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9488501439752044"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.model_performance(test).auc() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0748160248427708\n",
      "RMSE: 0.27352518136868276\n",
      "LogLoss: 0.2770108718654861\n",
      "Null degrees of freedom: 147784\n",
      "Residual degrees of freedom: 147781\n",
      "Null deviance: 140732.20131171282\n",
      "Residual deviance: 81876.10339728172\n",
      "AIC: 81884.10339728172\n",
      "AUC: 0.9705354292408278\n",
      "pr_auc: 0.8960099911776978\n",
      "Gini: 0.9410708584816556\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3054804393299643: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>114475.0</td>\n",
       "<td>6243.0</td>\n",
       "<td>0.0517</td>\n",
       "<td> (6243.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>4811.0</td>\n",
       "<td>22256.0</td>\n",
       "<td>0.1777</td>\n",
       "<td> (4811.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>119286.0</td>\n",
       "<td>28499.0</td>\n",
       "<td>0.0748</td>\n",
       "<td> (11054.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1      Error    Rate\n",
       "-----  ------  -----  -------  ------------------\n",
       "0      114475  6243   0.0517   (6243.0/120718.0)\n",
       "1      4811    22256  0.1777   (4811.0/27067.0)\n",
       "Total  119286  28499  0.0748   (11054.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.8010654</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2324010</td>\n",
       "<td>0.8527882</td>\n",
       "<td>256.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4116258</td>\n",
       "<td>0.8349729</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.3440950</td>\n",
       "<td>0.9274622</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1052357</td>\n",
       "<td>1.0</td>\n",
       "<td>361.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.7554259</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2517271</td>\n",
       "<td>0.9051343</td>\n",
       "<td>245.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2395344</td>\n",
       "<td>0.9063511</td>\n",
       "<td>252.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.30548      0.801065  216\n",
       "max f2                       0.232401     0.852788  256\n",
       "max f0point5                 0.411626     0.834973  167\n",
       "max accuracy                 0.344095     0.927462  198\n",
       "max precision                0.963831     1         0\n",
       "max recall                   0.105236     1         361\n",
       "max specificity              0.963831     1         0\n",
       "max absolute_mcc             0.30548      0.755426  216\n",
       "max min_per_class_accuracy   0.251727     0.905134  245\n",
       "max mean_per_class_accuracy  0.239534     0.906351  252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.32 %, avg score: 21.52 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100010</td>\n",
       "<td>0.8231913</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.0546052</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200020</td>\n",
       "<td>0.7645343</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7922026</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8293156</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1092105</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300030</td>\n",
       "<td>0.7172474</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7405818</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7997376</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1638157</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400041</td>\n",
       "<td>0.6758864</td>\n",
       "<td>5.4304164</td>\n",
       "<td>5.4525814</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.6959487</td>\n",
       "<td>0.9986468</td>\n",
       "<td>0.7737904</td>\n",
       "<td>0.0543097</td>\n",
       "<td>0.2181254</td>\n",
       "<td>443.0416418</td>\n",
       "<td>445.2581383</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500051</td>\n",
       "<td>0.6383709</td>\n",
       "<td>5.3417566</td>\n",
       "<td>5.4304164</td>\n",
       "<td>0.9783491</td>\n",
       "<td>0.6571369</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.7504597</td>\n",
       "<td>0.0534230</td>\n",
       "<td>0.2715484</td>\n",
       "<td>434.1756558</td>\n",
       "<td>443.0416418</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000034</td>\n",
       "<td>0.4826144</td>\n",
       "<td>4.9922256</td>\n",
       "<td>5.2113359</td>\n",
       "<td>0.9143321</td>\n",
       "<td>0.5558423</td>\n",
       "<td>0.9544624</td>\n",
       "<td>0.6531576</td>\n",
       "<td>0.2496028</td>\n",
       "<td>0.5211512</td>\n",
       "<td>399.2225650</td>\n",
       "<td>421.1335859</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500017</td>\n",
       "<td>0.3715061</td>\n",
       "<td>3.7611647</td>\n",
       "<td>4.7279673</td>\n",
       "<td>0.6888618</td>\n",
       "<td>0.4241561</td>\n",
       "<td>0.8659329</td>\n",
       "<td>0.5768272</td>\n",
       "<td>0.1880519</td>\n",
       "<td>0.7092031</td>\n",
       "<td>276.1164677</td>\n",
       "<td>372.7967271</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2957882</td>\n",
       "<td>2.5648335</td>\n",
       "<td>4.1872021</td>\n",
       "<td>0.4697523</td>\n",
       "<td>0.3308477</td>\n",
       "<td>0.7668911</td>\n",
       "<td>0.5153344</td>\n",
       "<td>0.1282373</td>\n",
       "<td>0.8374404</td>\n",
       "<td>156.4833515</td>\n",
       "<td>318.7202128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000034</td>\n",
       "<td>0.2099951</td>\n",
       "<td>1.1785170</td>\n",
       "<td>3.1842845</td>\n",
       "<td>0.2158468</td>\n",
       "<td>0.2476441</td>\n",
       "<td>0.5832055</td>\n",
       "<td>0.4261023</td>\n",
       "<td>0.1178557</td>\n",
       "<td>0.9552961</td>\n",
       "<td>17.8517042</td>\n",
       "<td>218.4284479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1682202</td>\n",
       "<td>0.3351057</td>\n",
       "<td>2.4720139</td>\n",
       "<td>0.0613750</td>\n",
       "<td>0.1869477</td>\n",
       "<td>0.4527523</td>\n",
       "<td>0.3663157</td>\n",
       "<td>0.0335094</td>\n",
       "<td>0.9888056</td>\n",
       "<td>-66.4894267</td>\n",
       "<td>147.2013891</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000034</td>\n",
       "<td>0.1432933</td>\n",
       "<td>0.0897742</td>\n",
       "<td>1.9955531</td>\n",
       "<td>0.0164422</td>\n",
       "<td>0.1548801</td>\n",
       "<td>0.3654879</td>\n",
       "<td>0.3240274</td>\n",
       "<td>0.0089777</td>\n",
       "<td>0.9977833</td>\n",
       "<td>-91.0225818</td>\n",
       "<td>99.5553054</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1259732</td>\n",
       "<td>0.0195817</td>\n",
       "<td>1.6662356</td>\n",
       "<td>0.0035864</td>\n",
       "<td>0.1341923</td>\n",
       "<td>0.3051731</td>\n",
       "<td>0.2923893</td>\n",
       "<td>0.0019581</td>\n",
       "<td>0.9997414</td>\n",
       "<td>-98.0418298</td>\n",
       "<td>66.6235637</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999966</td>\n",
       "<td>0.1118935</td>\n",
       "<td>0.0022168</td>\n",
       "<td>1.4285256</td>\n",
       "<td>0.0004060</td>\n",
       "<td>0.1187515</td>\n",
       "<td>0.2616362</td>\n",
       "<td>0.2675846</td>\n",
       "<td>0.0002217</td>\n",
       "<td>0.9999631</td>\n",
       "<td>-99.7783204</td>\n",
       "<td>42.8525554</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0991367</td>\n",
       "<td>0.0003694</td>\n",
       "<td>1.25</td>\n",
       "<td>0.0000677</td>\n",
       "<td>0.1054206</td>\n",
       "<td>0.2289390</td>\n",
       "<td>0.2473134</td>\n",
       "<td>0.0000369</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.9630559</td>\n",
       "<td>25.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999966</td>\n",
       "<td>0.0873202</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111153</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0932206</td>\n",
       "<td>0.2035021</td>\n",
       "<td>0.2301925</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1115288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0637468</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0802774</td>\n",
       "<td>0.1831512</td>\n",
       "<td>0.2152005</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift         cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -----------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010001                    0.823191           5.45997      5.45997            1                0.866429   1                           0.866429            0.0546052       0.0546052                  445.997   445.997\n",
       "    2        0.020002                    0.764534           5.45997      5.45997            1                0.792203   1                           0.829316            0.0546052       0.10921                    445.997   445.997\n",
       "    3        0.030003                    0.717247           5.45997      5.45997            1                0.740582   1                           0.799738            0.0546052       0.163816                   445.997   445.997\n",
       "    4        0.0400041                   0.675886           5.43042      5.45258            0.994587         0.695949   0.998647                    0.77379             0.0543097       0.218125                   443.042   445.258\n",
       "    5        0.0500051                   0.638371           5.34176      5.43042            0.978349         0.657137   0.994587                    0.75046             0.053423        0.271548                   434.176   443.042\n",
       "    6        0.100003                    0.482614           4.99223      5.21134            0.914332         0.555842   0.954462                    0.653158            0.249603        0.521151                   399.223   421.134\n",
       "    7        0.150002                    0.371506           3.76116      4.72797            0.688862         0.424156   0.865933                    0.576827            0.188052        0.709203                   276.116   372.797\n",
       "    8        0.2                         0.295788           2.56483      4.1872             0.469752         0.330848   0.766891                    0.515334            0.128237        0.83744                    156.483   318.72\n",
       "    9        0.300003                    0.209995           1.17852      3.18428            0.215847         0.247644   0.583206                    0.426102            0.117856        0.955296                   17.8517   218.428\n",
       "    10       0.4                         0.16822            0.335106     2.47201            0.061375         0.186948   0.452752                    0.366316            0.0335094       0.988806                   -66.4894  147.201\n",
       "    11       0.500003                    0.143293           0.0897742    1.99555            0.0164422        0.15488    0.365488                    0.324027            0.00897772      0.997783                   -91.0226  99.5553\n",
       "    12       0.6                         0.125973           0.0195817    1.66624            0.00358641       0.134192   0.305173                    0.292389            0.0019581       0.999741                   -98.0418  66.6236\n",
       "    13       0.699997                    0.111893           0.0022168    1.42853            0.000406009      0.118752   0.261636                    0.267585            0.000221672     0.999963                   -99.7783  42.8526\n",
       "    14       0.8                         0.0991367          0.000369441  1.25               6.76636e-05      0.105421   0.228939                    0.247313            3.69454e-05     1                          -99.9631  25\n",
       "    15       0.899997                    0.0873202          0            1.11112            0                0.0932206  0.203502                    0.230193            0               1                          -100      11.1115\n",
       "    16       1                           0.0637468          0            1                  0                0.0802774  0.183151                    0.215201            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_perf = aml.leader.model_performance()\n",
    "best_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhZmZ5eVEYWZmeTlRmJlZXv8fhI7gKkr+PFMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_perf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3054804393299643: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>114475.0</td>\n",
       "<td>6243.0</td>\n",
       "<td>0.0517</td>\n",
       "<td> (6243.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>4811.0</td>\n",
       "<td>22256.0</td>\n",
       "<td>0.1777</td>\n",
       "<td> (4811.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>119286.0</td>\n",
       "<td>28499.0</td>\n",
       "<td>0.0748</td>\n",
       "<td> (11054.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1      Error    Rate\n",
       "-----  ------  -----  -------  ------------------\n",
       "0      114475  6243   0.0517   (6243.0/120718.0)\n",
       "1      4811    22256  0.1777   (4811.0/27067.0)\n",
       "Total  119286  28499  0.0748   (11054.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.confusion_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  8.283768783445716e-06,\n",
       "  1.656753756689143e-05,\n",
       "  4.141884391722858e-05,\n",
       "  5.798638148412001e-05,\n",
       "  5.798638148412001e-05,\n",
       "  6.627015026756573e-05,\n",
       "  6.627015026756573e-05,\n",
       "  6.627015026756573e-05,\n",
       "  8.283768783445716e-05,\n",
       "  9.94052254013486e-05,\n",
       "  0.00011597276296824003,\n",
       "  0.00013254030053513145,\n",
       "  0.00018224291323580576,\n",
       "  0.00020709421958614292,\n",
       "  0.0002567968322868172,\n",
       "  0.0002650806010702629,\n",
       "  0.0002899319074206001,\n",
       "  0.00033135075133782865,\n",
       "  0.0003562020576881658,\n",
       "  0.00038105336403850297,\n",
       "  0.00042247220795573154,\n",
       "  0.00042247220795573154,\n",
       "  0.0004307559767391773,\n",
       "  0.00044732351430606867,\n",
       "  0.000497026127006743,\n",
       "  0.0005053098957901887,\n",
       "  0.0005715800460577544,\n",
       "  0.0005964313524080916,\n",
       "  0.0006295664275418744,\n",
       "  0.0006875528090259944,\n",
       "  0.0007372554217266688,\n",
       "  0.0007786742656438973,\n",
       "  0.0008118093407776802,\n",
       "  0.0008283768783445717,\n",
       "  0.0008697957222618002,\n",
       "  0.000936065872529366,\n",
       "  0.0009857684852300403,\n",
       "  0.001018903560363823,\n",
       "  0.0010934574794148346,\n",
       "  0.0011514438608989546,\n",
       "  0.0012508490863003031,\n",
       "  0.0013171192365678689,\n",
       "  0.0013999569244023261,\n",
       "  0.0014579433058864461,\n",
       "  0.001549064762504349,\n",
       "  0.0016567537566891434,\n",
       "  0.0017230239069567091,\n",
       "  0.0018555642074918404,\n",
       "  0.001963253201676635,\n",
       "  0.0020295233519442004,\n",
       "  0.002128928577345549,\n",
       "  0.0022531851090972347,\n",
       "  0.002344306565715138,\n",
       "  0.002427144253549595,\n",
       "  0.0025182657101674977,\n",
       "  0.0026425222419191835,\n",
       "  0.002775062542454315,\n",
       "  0.0029407379181232295,\n",
       "  0.002957305455690121,\n",
       "  0.0031146970625755893,\n",
       "  0.0033383588197286236,\n",
       "  0.0034460478139134183,\n",
       "  0.0035703043456651036,\n",
       "  0.0036697095710664525,\n",
       "  0.003827101177951921,\n",
       "  0.003909938865786378,\n",
       "  0.004067330472671847,\n",
       "  0.004175019466856641,\n",
       "  0.00433241107374211,\n",
       "  0.004556072830895144,\n",
       "  0.004746599512914396,\n",
       "  0.0049702612700674295,\n",
       "  0.005127652876952898,\n",
       "  0.00531817955897215,\n",
       "  0.005574976391258967,\n",
       "  0.0056992329230106525,\n",
       "  0.0059643135240809156,\n",
       "  0.006121705130966384,\n",
       "  0.00641992080717043,\n",
       "  0.0068258254775592705,\n",
       "  0.007066054772279196,\n",
       "  0.007389121754833579,\n",
       "  0.007795026425222419,\n",
       "  0.008142944714127139,\n",
       "  0.008441160390331185,\n",
       "  0.008714524760184894,\n",
       "  0.009062443049089614,\n",
       "  0.009468347719478454,\n",
       "  0.009899103696217632,\n",
       "  0.010213886909988569,\n",
       "  0.010586656505243626,\n",
       "  0.010992561175632467,\n",
       "  0.011381898308454414,\n",
       "  0.011796086747626701,\n",
       "  0.0120860186550473,\n",
       "  0.01247535578786925,\n",
       "  0.012964098146092546,\n",
       "  0.013411421660398616,\n",
       "  0.013949866631322586,\n",
       "  0.014339203764144536,\n",
       "  0.01473682466574993,\n",
       "  0.015175864411272553,\n",
       "  0.015590052850444838,\n",
       "  0.016169916665286038,\n",
       "  0.01676634801769413,\n",
       "  0.017279941682267763,\n",
       "  0.01786808926589241,\n",
       "  0.01840653423681638,\n",
       "  0.018754452525721103,\n",
       "  0.019094087045842378,\n",
       "  0.019599396941632565,\n",
       "  0.0201129906062062,\n",
       "  0.020510611507811593,\n",
       "  0.02099935386603489,\n",
       "  0.021579217680876093,\n",
       "  0.022208784108417964,\n",
       "  0.022714094004208155,\n",
       "  0.023451349425934822,\n",
       "  0.023757848870922315,\n",
       "  0.024470252986298645,\n",
       "  0.025340048708560447,\n",
       "  0.02587020991070097,\n",
       "  0.02659089779486075,\n",
       "  0.027377855829288093,\n",
       "  0.028073692407097534,\n",
       "  0.0284298944647857,\n",
       "  0.029192001192862704,\n",
       "  0.03012806706539207,\n",
       "  0.031006146556437318,\n",
       "  0.031528023989794395,\n",
       "  0.032339833330572075,\n",
       "  0.03314335890256631,\n",
       "  0.03349127719147103,\n",
       "  0.0342285326131977,\n",
       "  0.03531370632382909,\n",
       "  0.036050961745555755,\n",
       "  0.03687105485511689,\n",
       "  0.03803078248479928,\n",
       "  0.03917394257691479,\n",
       "  0.03998575191769247,\n",
       "  0.040913534021438394,\n",
       "  0.04179989728126709,\n",
       "  0.04271939561622956,\n",
       "  0.04382942063321128,\n",
       "  0.0442850279163008,\n",
       "  0.04536191785814874,\n",
       "  0.04653821302539803,\n",
       "  0.04786361603074935,\n",
       "  0.048294372007488526,\n",
       "  0.049379545718119915,\n",
       "  0.050746367567388456,\n",
       "  0.05171556851505161,\n",
       "  0.052535661624612734,\n",
       "  0.053687105485511685,\n",
       "  0.054971089646945775,\n",
       "  0.05611424973906128,\n",
       "  0.05728226113752713,\n",
       "  0.058334299773024734,\n",
       "  0.059535446246624366,\n",
       "  0.0607200251826571,\n",
       "  0.06231879255786212,\n",
       "  0.06376845209496512,\n",
       "  0.06491161218708064,\n",
       "  0.06635298795540019,\n",
       "  0.06798489040573899,\n",
       "  0.0698156033068805,\n",
       "  0.07163803243923855,\n",
       "  0.07322851604566014,\n",
       "  0.07491012110869961,\n",
       "  0.07583790321244553,\n",
       "  0.07703904968604516,\n",
       "  0.07816564224059377,\n",
       "  0.07973955830944847,\n",
       "  0.08166139266720787,\n",
       "  0.083169038585795,\n",
       "  0.08509915671233785,\n",
       "  0.08668135654997598,\n",
       "  0.0885700558326016,\n",
       "  0.09064099802846302,\n",
       "  0.09271194022432445,\n",
       "  0.09486572010802034,\n",
       "  0.09607515035040341,\n",
       "  0.09820407892774898,\n",
       "  0.10090458755115227,\n",
       "  0.10300866482214749,\n",
       "  0.10524528239367782,\n",
       "  0.10704286021968555,\n",
       "  0.10904753226527941,\n",
       "  0.11168177073841515,\n",
       "  0.11386868569724481,\n",
       "  0.1161301545751255,\n",
       "  0.11879752812339502,\n",
       "  0.12166371212246724,\n",
       "  0.12397488361304859,\n",
       "  0.12655113570470022,\n",
       "  0.12898656372703324,\n",
       "  0.13157938335625177,\n",
       "  0.134520121274375,\n",
       "  0.1370549545221094,\n",
       "  0.14000397620901606,\n",
       "  0.14288672774565517,\n",
       "  0.14624165410295067,\n",
       "  0.1491409731771567,\n",
       "  0.1520237247137958,\n",
       "  0.15558574529067745,\n",
       "  0.15862588843420203,\n",
       "  0.1630411371957786,\n",
       "  0.1659984426514687,\n",
       "  0.16890604549445815,\n",
       "  0.1718136483374476,\n",
       "  0.17458042711111849,\n",
       "  0.17733063834722246,\n",
       "  0.18083467254261998,\n",
       "  0.18365115392899153,\n",
       "  0.18805811892178464,\n",
       "  0.19179409864311867,\n",
       "  0.1952070113818983,\n",
       "  0.19837141105717457,\n",
       "  0.2017677562583873,\n",
       "  0.20512268261568284,\n",
       "  0.21056511870640668,\n",
       "  0.21485611093623155,\n",
       "  0.21871634718931726,\n",
       "  0.22185589555824317,\n",
       "  0.22618002286320185,\n",
       "  0.23007339419142134,\n",
       "  0.2342152785831442,\n",
       "  0.2384400006627015,\n",
       "  0.24301264103116355,\n",
       "  0.24681489090276512,\n",
       "  0.2511721532828576,\n",
       "  0.2561258470153581,\n",
       "  0.261253499892311,\n",
       "  0.2670190029655892,\n",
       "  0.27249457413144684,\n",
       "  0.27526963667390114,\n",
       "  0.280786626683676,\n",
       "  0.28630361669345084,\n",
       "  0.2906194602296261,\n",
       "  0.2959376397885982,\n",
       "  0.3012972381914876,\n",
       "  0.30521546082605744,\n",
       "  0.30980466873208634,\n",
       "  0.3161831706953396,\n",
       "  0.31900793585049453,\n",
       "  0.32437581802216736,\n",
       "  0.33144187279444653,\n",
       "  0.33745588893122813,\n",
       "  0.34332908099869114,\n",
       "  0.34950048874235823,\n",
       "  0.3564754220580195,\n",
       "  0.3621746549810302,\n",
       "  0.3680644145860601,\n",
       "  0.3737802150466376,\n",
       "  0.3766795341208436,\n",
       "  0.3841183584883779,\n",
       "  0.38962706472936925,\n",
       "  0.3954505541841316,\n",
       "  0.4041982140194503,\n",
       "  0.4105104458324359,\n",
       "  0.41723686608459387,\n",
       "  0.42050067098527144,\n",
       "  0.4273264964628307,\n",
       "  0.4347321857552312,\n",
       "  0.43986812240096756,\n",
       "  0.44483838367103495,\n",
       "  0.45226064050100234,\n",
       "  0.45855630477642106,\n",
       "  0.46540698156033067,\n",
       "  0.4717937672923673,\n",
       "  0.4788846733709969,\n",
       "  0.48691992909093923,\n",
       "  0.4959409532961116,\n",
       "  0.5048791398134496,\n",
       "  0.5161616328965026,\n",
       "  0.5241471860037442,\n",
       "  0.5301446346029589,\n",
       "  0.5378236882652131,\n",
       "  0.5460826057423085,\n",
       "  0.5538693483987475,\n",
       "  0.5613910104541162,\n",
       "  0.5700641163703839,\n",
       "  0.5785632631421992,\n",
       "  0.5870458423764476,\n",
       "  0.5932503851952484,\n",
       "  0.6035305422555045,\n",
       "  0.6122947696283901,\n",
       "  0.6211832535330274,\n",
       "  0.6280504978545038,\n",
       "  0.6385543166719131,\n",
       "  0.6450902102420517,\n",
       "  0.654517139117613,\n",
       "  0.6613595321327391,\n",
       "  0.6698669626733379,\n",
       "  0.6802548087277788,\n",
       "  0.6906260872446528,\n",
       "  0.7001855564207492,\n",
       "  0.706315545320499,\n",
       "  0.715775609271194,\n",
       "  0.7236700409218177,\n",
       "  0.7304792988618102,\n",
       "  0.7411570768236717,\n",
       "  0.7495153995261684,\n",
       "  0.7615517155685151,\n",
       "  0.7696118225948078,\n",
       "  0.7798505608111467,\n",
       "  0.7845143226362266,\n",
       "  0.7927235375006213,\n",
       "  0.8033350453122152,\n",
       "  0.8100034791828891,\n",
       "  0.8189002468563098,\n",
       "  0.8322536821352242,\n",
       "  0.8412415712652629,\n",
       "  0.8522009973657615,\n",
       "  0.8607084279063603,\n",
       "  0.869977965175036,\n",
       "  0.8816415116221276,\n",
       "  0.8899169966367899,\n",
       "  0.8993770605874849,\n",
       "  0.9084974900180586,\n",
       "  0.9188853360724996,\n",
       "  0.9305157474444573,\n",
       "  0.940638512897828,\n",
       "  0.9486654848489869,\n",
       "  0.9561208767540881,\n",
       "  0.9657134810053182,\n",
       "  0.9718600374426349,\n",
       "  0.9774847164465945,\n",
       "  0.9825626667108468,\n",
       "  0.9874583740618632,\n",
       "  0.9914594343842674,\n",
       "  0.9957007240013916,\n",
       "  0.9971669510760616,\n",
       "  0.9991302042777382,\n",
       "  1.0],\n",
       " [0.00022167214689474267,\n",
       "  0.0005172350094210663,\n",
       "  0.0009605793032105516,\n",
       "  0.0015886503860789891,\n",
       "  0.002955628625263236,\n",
       "  0.004470388295710644,\n",
       "  0.005504858314552777,\n",
       "  0.0061698747552370046,\n",
       "  0.008091033361658107,\n",
       "  0.009753574463368678,\n",
       "  0.01078804448221081,\n",
       "  0.012561421657368751,\n",
       "  0.014150072043447742,\n",
       "  0.016034285292053054,\n",
       "  0.017216536742158346,\n",
       "  0.019285476779842613,\n",
       "  0.02161303432223741,\n",
       "  0.02394059186463221,\n",
       "  0.02737651014150072,\n",
       "  0.02963017696826394,\n",
       "  0.031736062363763994,\n",
       "  0.03495030849373776,\n",
       "  0.03853400820186943,\n",
       "  0.041083237891158975,\n",
       "  0.044297484021132746,\n",
       "  0.04547973547123804,\n",
       "  0.048620090885580225,\n",
       "  0.05068903092326449,\n",
       "  0.05386633169542247,\n",
       "  0.05619388923781727,\n",
       "  0.06059038681789633,\n",
       "  0.06361990615879115,\n",
       "  0.06672331621531755,\n",
       "  0.06879225625300181,\n",
       "  0.07241290131894927,\n",
       "  0.07680939889902834,\n",
       "  0.08039309860716001,\n",
       "  0.08279454686518639,\n",
       "  0.08652602800458123,\n",
       "  0.09081168951121292,\n",
       "  0.09494956958658145,\n",
       "  0.098200761074371,\n",
       "  0.09990024753389737,\n",
       "  0.10348394724202904,\n",
       "  0.10913658698784498,\n",
       "  0.11242472383345033,\n",
       "  0.11696900284479256,\n",
       "  0.1214393911405032,\n",
       "  0.12469058262829276,\n",
       "  0.12860679055676655,\n",
       "  0.1334466324306351,\n",
       "  0.1376214578638194,\n",
       "  0.14098348542505634,\n",
       "  0.14475191192226697,\n",
       "  0.14948091772268815,\n",
       "  0.15258432777921455,\n",
       "  0.15731333357963573,\n",
       "  0.16119259615029372,\n",
       "  0.16573687516163593,\n",
       "  0.1705028263198729,\n",
       "  0.17353234566076772,\n",
       "  0.1786677503971626,\n",
       "  0.18302730261942587,\n",
       "  0.18657405696974175,\n",
       "  0.1917464070639524,\n",
       "  0.1961059592862157,\n",
       "  0.19965271363653156,\n",
       "  0.20390142978534748,\n",
       "  0.20851959951232127,\n",
       "  0.21254664351424243,\n",
       "  0.21709092252558465,\n",
       "  0.22244799940887428,\n",
       "  0.2282114752281376,\n",
       "  0.23138877600029556,\n",
       "  0.23615472715853253,\n",
       "  0.24040344330734842,\n",
       "  0.24465215945616434,\n",
       "  0.24956589204566446,\n",
       "  0.25344515461632244,\n",
       "  0.25765692540732255,\n",
       "  0.2606864447482174,\n",
       "  0.26593268555805966,\n",
       "  0.2707725274319282,\n",
       "  0.2752429157276388,\n",
       "  0.2816714079875864,\n",
       "  0.285439834484797,\n",
       "  0.29027967635866553,\n",
       "  0.2947131192965604,\n",
       "  0.30007019617985,\n",
       "  0.3039125133926922,\n",
       "  0.3090479181290871,\n",
       "  0.3145527764436399,\n",
       "  0.32031625226290317,\n",
       "  0.3251930394945875,\n",
       "  0.32885062991835073,\n",
       "  0.33468799645324565,\n",
       "  0.3387889311707984,\n",
       "  0.34314848339306164,\n",
       "  0.34632578416521964,\n",
       "  0.35271733106735137,\n",
       "  0.3577049543724831,\n",
       "  0.36217534266819373,\n",
       "  0.3641703919902464,\n",
       "  0.3688993977906676,\n",
       "  0.3754017807662467,\n",
       "  0.38035245871356266,\n",
       "  0.3855248088077733,\n",
       "  0.3901060331769313,\n",
       "  0.3943178039679314,\n",
       "  0.3983079026120368,\n",
       "  0.4021871651826948,\n",
       "  0.4056969741751949,\n",
       "  0.4114604499944582,\n",
       "  0.41500720434477406,\n",
       "  0.42014260908116896,\n",
       "  0.4232829644955111,\n",
       "  0.42786418886466915,\n",
       "  0.43299959360106405,\n",
       "  0.4351793697121957,\n",
       "  0.4398344847969853,\n",
       "  0.4440462555879854,\n",
       "  0.4489230428196697,\n",
       "  0.4543540104185909,\n",
       "  0.45645989581409097,\n",
       "  0.46155835519267,\n",
       "  0.4648464920382754,\n",
       "  0.46817157424169653,\n",
       "  0.4728636346843019,\n",
       "  0.4759300993830125,\n",
       "  0.47947685373332843,\n",
       "  0.4815457937710127,\n",
       "  0.4860161820667233,\n",
       "  0.4903387889311708,\n",
       "  0.4944397236487235,\n",
       "  0.4992426201647763,\n",
       "  0.5031957734510659,\n",
       "  0.5075183803155133,\n",
       "  0.5115823696752503,\n",
       "  0.5154985776037241,\n",
       "  0.5210403812760926,\n",
       "  0.5248088077733033,\n",
       "  0.5295747589315403,\n",
       "  0.5331215132818561,\n",
       "  0.537887464440093,\n",
       "  0.5429859238186722,\n",
       "  0.5477149296190934,\n",
       "  0.552554771492962,\n",
       "  0.5569882144308568,\n",
       "  0.561532493442199,\n",
       "  0.5654856467284886,\n",
       "  0.5701407618132781,\n",
       "  0.5747589315402519,\n",
       "  0.577751505523331,\n",
       "  0.5824435659659364,\n",
       "  0.5870986810507259,\n",
       "  0.5920493589980419,\n",
       "  0.5973694905235157,\n",
       "  0.6028743488380685,\n",
       "  0.6062363763993054,\n",
       "  0.609302841098016,\n",
       "  0.6132559943843056,\n",
       "  0.6171352569549636,\n",
       "  0.6225662245538848,\n",
       "  0.626925776776148,\n",
       "  0.6301769682639377,\n",
       "  0.6345365204862009,\n",
       "  0.6390069087819116,\n",
       "  0.6428492259947538,\n",
       "  0.6476890678686223,\n",
       "  0.6524550190268593,\n",
       "  0.656371226955333,\n",
       "  0.6603613255994384,\n",
       "  0.6642775335279122,\n",
       "  0.667307052868807,\n",
       "  0.6698562825580966,\n",
       "  0.672959692614623,\n",
       "  0.6760631026711493,\n",
       "  0.6792404034433074,\n",
       "  0.6835999556655706,\n",
       "  0.6887353604019655,\n",
       "  0.6939077104961762,\n",
       "  0.6982303173606237,\n",
       "  0.7018879077843869,\n",
       "  0.7040307385377027,\n",
       "  0.7084641814755976,\n",
       "  0.7128606790556766,\n",
       "  0.7164074334059926,\n",
       "  0.7206192041969927,\n",
       "  0.7237965049691506,\n",
       "  0.7269368603834928,\n",
       "  0.7287471829164666,\n",
       "  0.7325156094136772,\n",
       "  0.7366165441312299,\n",
       "  0.7402371891971774,\n",
       "  0.742416965308309,\n",
       "  0.7465179000258617,\n",
       "  0.7508774522481251,\n",
       "  0.7522813758451251,\n",
       "  0.7566039827095725,\n",
       "  0.7609635349318358,\n",
       "  0.764288617135257,\n",
       "  0.7678723168433886,\n",
       "  0.7733032844423098,\n",
       "  0.7785125798943363,\n",
       "  0.7818376620977574,\n",
       "  0.7846824546495733,\n",
       "  0.787379465770126,\n",
       "  0.7905198211844682,\n",
       "  0.7941404662504157,\n",
       "  0.7959877341412052,\n",
       "  0.8000147781431263,\n",
       "  0.8042265489341264,\n",
       "  0.808881664018916,\n",
       "  0.8110983854878635,\n",
       "  0.8144604130491003,\n",
       "  0.8185244024088373,\n",
       "  0.8222558835482322,\n",
       "  0.8239553700077585,\n",
       "  0.8274651790002586,\n",
       "  0.8305316436989693,\n",
       "  0.8344478516274431,\n",
       "  0.8380315513355747,\n",
       "  0.8412457974655485,\n",
       "  0.8442753168064433,\n",
       "  0.846972327926996,\n",
       "  0.8499279565522592,\n",
       "  0.8531791480400488,\n",
       "  0.8565042302434699,\n",
       "  0.8597923670890752,\n",
       "  0.8631543946503122,\n",
       "  0.8665164222115491,\n",
       "  0.8695089961946282,\n",
       "  0.8720951712417335,\n",
       "  0.8747921823622862,\n",
       "  0.8759744338123915,\n",
       "  0.8795211881627073,\n",
       "  0.88081427568626,\n",
       "  0.8843979753943917,\n",
       "  0.8882033472494181,\n",
       "  0.890161451213655,\n",
       "  0.8928215169763919,\n",
       "  0.8953707466656815,\n",
       "  0.8984002660065763,\n",
       "  0.9012081132005764,\n",
       "  0.9037942882476817,\n",
       "  0.9064174086526028,\n",
       "  0.9078213322496028,\n",
       "  0.9103705619388924,\n",
       "  0.9129567369859977,\n",
       "  0.9153581852440241,\n",
       "  0.9175749067129715,\n",
       "  0.9197177374662874,\n",
       "  0.9217497321461559,\n",
       "  0.924188125761998,\n",
       "  0.9260723390106033,\n",
       "  0.9284368419108139,\n",
       "  0.9314663612517087,\n",
       "  0.9340894816566299,\n",
       "  0.9361953670521299,\n",
       "  0.9380056895851037,\n",
       "  0.9398529574758931,\n",
       "  0.9415154985776037,\n",
       "  0.9435474932574722,\n",
       "  0.9452839250748144,\n",
       "  0.9474267558281302,\n",
       "  0.9494956958658145,\n",
       "  0.9512690730409724,\n",
       "  0.9531532862895777,\n",
       "  0.9546680459600251,\n",
       "  0.956626149924262,\n",
       "  0.9586950899619463,\n",
       "  0.9609487567887095,\n",
       "  0.9617615546606569,\n",
       "  0.9635349318358148,\n",
       "  0.9650127461484465,\n",
       "  0.9661211068829202,\n",
       "  0.9673033583330255,\n",
       "  0.9688920087191044,\n",
       "  0.9700742601692097,\n",
       "  0.9714781837662098,\n",
       "  0.9724018177116045,\n",
       "  0.9733623970148151,\n",
       "  0.9743229763180257,\n",
       "  0.975135774189973,\n",
       "  0.9761702442088152,\n",
       "  0.9775741678058152,\n",
       "  0.9784608563933942,\n",
       "  0.9796431078434995,\n",
       "  0.9804928510732627,\n",
       "  0.9814534303764731,\n",
       "  0.9822292828906048,\n",
       "  0.982894299331289,\n",
       "  0.9838179332766838,\n",
       "  0.9844460043595522,\n",
       "  0.9854065836627628,\n",
       "  0.986256326892526,\n",
       "  0.9869952340488418,\n",
       "  0.9878819226364207,\n",
       "  0.9888055565818155,\n",
       "  0.9891750101599734,\n",
       "  0.9895444637381313,\n",
       "  0.9901355894631839,\n",
       "  0.9908744966194998,\n",
       "  0.9912808955554735,\n",
       "  0.9921675841430524,\n",
       "  0.9927217645102893,\n",
       "  0.9930542727306314,\n",
       "  0.9934976170244209,\n",
       "  0.994014852033842,\n",
       "  0.9941995788229209,\n",
       "  0.9944212509698156,\n",
       "  0.9949754313370525,\n",
       "  0.9951971034839472,\n",
       "  0.9956404477777367,\n",
       "  0.995751283851184,\n",
       "  0.995936010640263,\n",
       "  0.9961576827871578,\n",
       "  0.9967488085122105,\n",
       "  0.9970443713747368,\n",
       "  0.9971182620903684,\n",
       "  0.9975616063841578,\n",
       "  0.9978202238888684,\n",
       "  0.9980049506779474,\n",
       "  0.9983744042561052,\n",
       "  0.998596076403,\n",
       "  0.9987069124764474,\n",
       "  0.9987069124764474,\n",
       "  0.9990394206967894,\n",
       "  0.9991872021280526,\n",
       "  0.9991872021280526,\n",
       "  0.9992241474858684,\n",
       "  0.9993719289171316,\n",
       "  0.9994458196327631,\n",
       "  0.999482764990579,\n",
       "  0.9995197103483947,\n",
       "  0.9995566557062106,\n",
       "  0.9995936010640263,\n",
       "  0.9996305464218421,\n",
       "  0.9997044371374737,\n",
       "  0.9997783278531053,\n",
       "  0.999815273210921,\n",
       "  0.9998522185687369,\n",
       "  0.9998522185687369,\n",
       "  0.9998891639265526,\n",
       "  0.9999261092843684,\n",
       "  0.9999261092843684,\n",
       "  0.9999261092843684,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  0.9999630546421842,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc=aml.leader.roc()\n",
    "roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_AutoML_20190108_000309\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0748160248427708\n",
      "RMSE: 0.27352518136868276\n",
      "LogLoss: 0.2770108718654861\n",
      "Null degrees of freedom: 147784\n",
      "Residual degrees of freedom: 147781\n",
      "Null deviance: 140732.20131171282\n",
      "Residual deviance: 81876.10339728172\n",
      "AIC: 81884.10339728172\n",
      "AUC: 0.9705354292408278\n",
      "pr_auc: 0.8960099911776978\n",
      "Gini: 0.9410708584816556\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3054804393299643: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>114475.0</td>\n",
       "<td>6243.0</td>\n",
       "<td>0.0517</td>\n",
       "<td> (6243.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>4811.0</td>\n",
       "<td>22256.0</td>\n",
       "<td>0.1777</td>\n",
       "<td> (4811.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>119286.0</td>\n",
       "<td>28499.0</td>\n",
       "<td>0.0748</td>\n",
       "<td> (11054.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1      Error    Rate\n",
       "-----  ------  -----  -------  ------------------\n",
       "0      114475  6243   0.0517   (6243.0/120718.0)\n",
       "1      4811    22256  0.1777   (4811.0/27067.0)\n",
       "Total  119286  28499  0.0748   (11054.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.8010654</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2324010</td>\n",
       "<td>0.8527882</td>\n",
       "<td>256.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4116258</td>\n",
       "<td>0.8349729</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.3440950</td>\n",
       "<td>0.9274622</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1052357</td>\n",
       "<td>1.0</td>\n",
       "<td>361.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.7554259</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2517271</td>\n",
       "<td>0.9051343</td>\n",
       "<td>245.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2395344</td>\n",
       "<td>0.9063511</td>\n",
       "<td>252.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.30548      0.801065  216\n",
       "max f2                       0.232401     0.852788  256\n",
       "max f0point5                 0.411626     0.834973  167\n",
       "max accuracy                 0.344095     0.927462  198\n",
       "max precision                0.963831     1         0\n",
       "max recall                   0.105236     1         361\n",
       "max specificity              0.963831     1         0\n",
       "max absolute_mcc             0.30548      0.755426  216\n",
       "max min_per_class_accuracy   0.251727     0.905134  245\n",
       "max mean_per_class_accuracy  0.239534     0.906351  252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.32 %, avg score: 21.52 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100010</td>\n",
       "<td>0.8231913</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.0546052</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200020</td>\n",
       "<td>0.7645343</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7922026</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8293156</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1092105</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300030</td>\n",
       "<td>0.7172474</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7405818</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7997376</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1638157</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400041</td>\n",
       "<td>0.6758864</td>\n",
       "<td>5.4304164</td>\n",
       "<td>5.4525814</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.6959487</td>\n",
       "<td>0.9986468</td>\n",
       "<td>0.7737904</td>\n",
       "<td>0.0543097</td>\n",
       "<td>0.2181254</td>\n",
       "<td>443.0416418</td>\n",
       "<td>445.2581383</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500051</td>\n",
       "<td>0.6383709</td>\n",
       "<td>5.3417566</td>\n",
       "<td>5.4304164</td>\n",
       "<td>0.9783491</td>\n",
       "<td>0.6571369</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.7504597</td>\n",
       "<td>0.0534230</td>\n",
       "<td>0.2715484</td>\n",
       "<td>434.1756558</td>\n",
       "<td>443.0416418</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000034</td>\n",
       "<td>0.4826144</td>\n",
       "<td>4.9922256</td>\n",
       "<td>5.2113359</td>\n",
       "<td>0.9143321</td>\n",
       "<td>0.5558423</td>\n",
       "<td>0.9544624</td>\n",
       "<td>0.6531576</td>\n",
       "<td>0.2496028</td>\n",
       "<td>0.5211512</td>\n",
       "<td>399.2225650</td>\n",
       "<td>421.1335859</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500017</td>\n",
       "<td>0.3715061</td>\n",
       "<td>3.7611647</td>\n",
       "<td>4.7279673</td>\n",
       "<td>0.6888618</td>\n",
       "<td>0.4241561</td>\n",
       "<td>0.8659329</td>\n",
       "<td>0.5768272</td>\n",
       "<td>0.1880519</td>\n",
       "<td>0.7092031</td>\n",
       "<td>276.1164677</td>\n",
       "<td>372.7967271</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2957882</td>\n",
       "<td>2.5648335</td>\n",
       "<td>4.1872021</td>\n",
       "<td>0.4697523</td>\n",
       "<td>0.3308477</td>\n",
       "<td>0.7668911</td>\n",
       "<td>0.5153344</td>\n",
       "<td>0.1282373</td>\n",
       "<td>0.8374404</td>\n",
       "<td>156.4833515</td>\n",
       "<td>318.7202128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000034</td>\n",
       "<td>0.2099951</td>\n",
       "<td>1.1785170</td>\n",
       "<td>3.1842845</td>\n",
       "<td>0.2158468</td>\n",
       "<td>0.2476441</td>\n",
       "<td>0.5832055</td>\n",
       "<td>0.4261023</td>\n",
       "<td>0.1178557</td>\n",
       "<td>0.9552961</td>\n",
       "<td>17.8517042</td>\n",
       "<td>218.4284479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1682202</td>\n",
       "<td>0.3351057</td>\n",
       "<td>2.4720139</td>\n",
       "<td>0.0613750</td>\n",
       "<td>0.1869477</td>\n",
       "<td>0.4527523</td>\n",
       "<td>0.3663157</td>\n",
       "<td>0.0335094</td>\n",
       "<td>0.9888056</td>\n",
       "<td>-66.4894267</td>\n",
       "<td>147.2013891</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000034</td>\n",
       "<td>0.1432933</td>\n",
       "<td>0.0897742</td>\n",
       "<td>1.9955531</td>\n",
       "<td>0.0164422</td>\n",
       "<td>0.1548801</td>\n",
       "<td>0.3654879</td>\n",
       "<td>0.3240274</td>\n",
       "<td>0.0089777</td>\n",
       "<td>0.9977833</td>\n",
       "<td>-91.0225818</td>\n",
       "<td>99.5553054</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1259732</td>\n",
       "<td>0.0195817</td>\n",
       "<td>1.6662356</td>\n",
       "<td>0.0035864</td>\n",
       "<td>0.1341923</td>\n",
       "<td>0.3051731</td>\n",
       "<td>0.2923893</td>\n",
       "<td>0.0019581</td>\n",
       "<td>0.9997414</td>\n",
       "<td>-98.0418298</td>\n",
       "<td>66.6235637</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999966</td>\n",
       "<td>0.1118935</td>\n",
       "<td>0.0022168</td>\n",
       "<td>1.4285256</td>\n",
       "<td>0.0004060</td>\n",
       "<td>0.1187515</td>\n",
       "<td>0.2616362</td>\n",
       "<td>0.2675846</td>\n",
       "<td>0.0002217</td>\n",
       "<td>0.9999631</td>\n",
       "<td>-99.7783204</td>\n",
       "<td>42.8525554</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0991367</td>\n",
       "<td>0.0003694</td>\n",
       "<td>1.25</td>\n",
       "<td>0.0000677</td>\n",
       "<td>0.1054206</td>\n",
       "<td>0.2289390</td>\n",
       "<td>0.2473134</td>\n",
       "<td>0.0000369</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.9630559</td>\n",
       "<td>25.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999966</td>\n",
       "<td>0.0873202</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111153</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0932206</td>\n",
       "<td>0.2035021</td>\n",
       "<td>0.2301925</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1115288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0637468</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0802774</td>\n",
       "<td>0.1831512</td>\n",
       "<td>0.2152005</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift         cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -----------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010001                    0.823191           5.45997      5.45997            1                0.866429   1                           0.866429            0.0546052       0.0546052                  445.997   445.997\n",
       "    2        0.020002                    0.764534           5.45997      5.45997            1                0.792203   1                           0.829316            0.0546052       0.10921                    445.997   445.997\n",
       "    3        0.030003                    0.717247           5.45997      5.45997            1                0.740582   1                           0.799738            0.0546052       0.163816                   445.997   445.997\n",
       "    4        0.0400041                   0.675886           5.43042      5.45258            0.994587         0.695949   0.998647                    0.77379             0.0543097       0.218125                   443.042   445.258\n",
       "    5        0.0500051                   0.638371           5.34176      5.43042            0.978349         0.657137   0.994587                    0.75046             0.053423        0.271548                   434.176   443.042\n",
       "    6        0.100003                    0.482614           4.99223      5.21134            0.914332         0.555842   0.954462                    0.653158            0.249603        0.521151                   399.223   421.134\n",
       "    7        0.150002                    0.371506           3.76116      4.72797            0.688862         0.424156   0.865933                    0.576827            0.188052        0.709203                   276.116   372.797\n",
       "    8        0.2                         0.295788           2.56483      4.1872             0.469752         0.330848   0.766891                    0.515334            0.128237        0.83744                    156.483   318.72\n",
       "    9        0.300003                    0.209995           1.17852      3.18428            0.215847         0.247644   0.583206                    0.426102            0.117856        0.955296                   17.8517   218.428\n",
       "    10       0.4                         0.16822            0.335106     2.47201            0.061375         0.186948   0.452752                    0.366316            0.0335094       0.988806                   -66.4894  147.201\n",
       "    11       0.500003                    0.143293           0.0897742    1.99555            0.0164422        0.15488    0.365488                    0.324027            0.00897772      0.997783                   -91.0226  99.5553\n",
       "    12       0.6                         0.125973           0.0195817    1.66624            0.00358641       0.134192   0.305173                    0.292389            0.0019581       0.999741                   -98.0418  66.6236\n",
       "    13       0.699997                    0.111893           0.0022168    1.42853            0.000406009      0.118752   0.261636                    0.267585            0.000221672     0.999963                   -99.7783  42.8526\n",
       "    14       0.8                         0.0991367          0.000369441  1.25               6.76636e-05      0.105421   0.228939                    0.247313            3.69454e-05     1                          -99.9631  25\n",
       "    15       0.899997                    0.0873202          0            1.11112            0                0.0932206  0.203502                    0.230193            0               1                          -100      11.1115\n",
       "    16       1                           0.0637468          0            1                  0                0.0802774  0.183151                    0.215201            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1374305415577651\n",
      "RMSE: 0.370716254779535\n",
      "LogLoss: 0.4374035749971233\n",
      "Null degrees of freedom: 16201\n",
      "Residual degrees of freedom: 16198\n",
      "Null deviance: 15373.717640908228\n",
      "Residual deviance: 14173.625444206784\n",
      "AIC: 14181.625444206784\n",
      "AUC: 0.7038412027583177\n",
      "pr_auc: 0.34120380509032733\n",
      "Gini: 0.4076824055166355\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.19535344211739553: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>9850.0</td>\n",
       "<td>3403.0</td>\n",
       "<td>0.2568</td>\n",
       "<td> (3403.0/13253.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1367.0</td>\n",
       "<td>1582.0</td>\n",
       "<td>0.4635</td>\n",
       "<td> (1367.0/2949.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11217.0</td>\n",
       "<td>4985.0</td>\n",
       "<td>0.2944</td>\n",
       "<td> (4770.0/16202.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      9850   3403  0.2568   (3403.0/13253.0)\n",
       "1      1367   1582  0.4635   (1367.0/2949.0)\n",
       "Total  11217  4985  0.2944   (4770.0/16202.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1953534</td>\n",
       "<td>0.3987900</td>\n",
       "<td>248.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1166556</td>\n",
       "<td>0.5648862</td>\n",
       "<td>337.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2709433</td>\n",
       "<td>0.3762722</td>\n",
       "<td>185.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6365348</td>\n",
       "<td>0.8189730</td>\n",
       "<td>28.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7519927</td>\n",
       "<td>0.6470588</td>\n",
       "<td>7.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0720133</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8355440</td>\n",
       "<td>0.9999245</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2288610</td>\n",
       "<td>0.2366986</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1667771</td>\n",
       "<td>0.6414397</td>\n",
       "<td>277.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1542798</td>\n",
       "<td>0.6472159</td>\n",
       "<td>291.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.195353     0.39879   248\n",
       "max f2                       0.116656     0.564886  337\n",
       "max f0point5                 0.270943     0.376272  185\n",
       "max accuracy                 0.636535     0.818973  28\n",
       "max precision                0.751993     0.647059  7\n",
       "max recall                   0.0720133    1         397\n",
       "max specificity              0.835544     0.999925  0\n",
       "max absolute_mcc             0.228861     0.236699  218\n",
       "max min_per_class_accuracy   0.166777     0.64144   277\n",
       "max mean_per_class_accuracy  0.15428      0.647216  291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.20 %, avg score: 18.47 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100605</td>\n",
       "<td>0.6070463</td>\n",
       "<td>2.9324155</td>\n",
       "<td>2.9324155</td>\n",
       "<td>0.5337423</td>\n",
       "<td>0.6747785</td>\n",
       "<td>0.5337423</td>\n",
       "<td>0.6747785</td>\n",
       "<td>0.0295015</td>\n",
       "<td>0.0295015</td>\n",
       "<td>193.2415480</td>\n",
       "<td>193.2415480</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200593</td>\n",
       "<td>0.5374457</td>\n",
       "<td>2.6792049</td>\n",
       "<td>2.8061998</td>\n",
       "<td>0.4876543</td>\n",
       "<td>0.5705091</td>\n",
       "<td>0.5107692</td>\n",
       "<td>0.6228042</td>\n",
       "<td>0.0267887</td>\n",
       "<td>0.0562903</td>\n",
       "<td>167.9204920</td>\n",
       "<td>180.6199755</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300580</td>\n",
       "<td>0.4880498</td>\n",
       "<td>2.4418070</td>\n",
       "<td>2.6849849</td>\n",
       "<td>0.4444444</td>\n",
       "<td>0.5106372</td>\n",
       "<td>0.4887064</td>\n",
       "<td>0.5854920</td>\n",
       "<td>0.0244151</td>\n",
       "<td>0.0807053</td>\n",
       "<td>144.1807016</td>\n",
       "<td>168.4984922</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400568</td>\n",
       "<td>0.4585920</td>\n",
       "<td>2.5096350</td>\n",
       "<td>2.6412150</td>\n",
       "<td>0.4567901</td>\n",
       "<td>0.4730993</td>\n",
       "<td>0.4807396</td>\n",
       "<td>0.5574371</td>\n",
       "<td>0.0250933</td>\n",
       "<td>0.1057986</td>\n",
       "<td>150.9634988</td>\n",
       "<td>164.1214984</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500555</td>\n",
       "<td>0.4315051</td>\n",
       "<td>2.1026672</td>\n",
       "<td>2.5336382</td>\n",
       "<td>0.3827160</td>\n",
       "<td>0.4444311</td>\n",
       "<td>0.4611591</td>\n",
       "<td>0.5348638</td>\n",
       "<td>0.0210241</td>\n",
       "<td>0.1268227</td>\n",
       "<td>110.2667152</td>\n",
       "<td>153.3638229</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000494</td>\n",
       "<td>0.3337569</td>\n",
       "<td>2.0280564</td>\n",
       "<td>2.2810033</td>\n",
       "<td>0.3691358</td>\n",
       "<td>0.3752998</td>\n",
       "<td>0.4151758</td>\n",
       "<td>0.4551310</td>\n",
       "<td>0.1013903</td>\n",
       "<td>0.2282130</td>\n",
       "<td>102.8056382</td>\n",
       "<td>128.1003253</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500432</td>\n",
       "<td>0.2806429</td>\n",
       "<td>1.8313553</td>\n",
       "<td>2.1311822</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.3060461</td>\n",
       "<td>0.3879062</td>\n",
       "<td>0.4054565</td>\n",
       "<td>0.0915565</td>\n",
       "<td>0.3197694</td>\n",
       "<td>83.1355262</td>\n",
       "<td>113.1182244</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000370</td>\n",
       "<td>0.2443961</td>\n",
       "<td>1.5939574</td>\n",
       "<td>1.9969175</td>\n",
       "<td>0.2901235</td>\n",
       "<td>0.2612294</td>\n",
       "<td>0.3634681</td>\n",
       "<td>0.3694109</td>\n",
       "<td>0.0796880</td>\n",
       "<td>0.3994574</td>\n",
       "<td>59.3957357</td>\n",
       "<td>99.6917462</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000247</td>\n",
       "<td>0.1976308</td>\n",
       "<td>1.2582089</td>\n",
       "<td>1.7507319</td>\n",
       "<td>0.2290123</td>\n",
       "<td>0.2187809</td>\n",
       "<td>0.3186587</td>\n",
       "<td>0.3192112</td>\n",
       "<td>0.1258054</td>\n",
       "<td>0.5252628</td>\n",
       "<td>25.8208893</td>\n",
       "<td>75.0731928</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000123</td>\n",
       "<td>0.1687228</td>\n",
       "<td>1.0988132</td>\n",
       "<td>1.5877774</td>\n",
       "<td>0.2</td>\n",
       "<td>0.1820912</td>\n",
       "<td>0.2889986</td>\n",
       "<td>0.2849365</td>\n",
       "<td>0.1098678</td>\n",
       "<td>0.6351306</td>\n",
       "<td>9.8813157</td>\n",
       "<td>58.7777382</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1468073</td>\n",
       "<td>1.0174196</td>\n",
       "<td>1.4737199</td>\n",
       "<td>0.1851852</td>\n",
       "<td>0.1575376</td>\n",
       "<td>0.2682385</td>\n",
       "<td>0.2594599</td>\n",
       "<td>0.1017294</td>\n",
       "<td>0.7368600</td>\n",
       "<td>1.7419590</td>\n",
       "<td>47.3719905</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999877</td>\n",
       "<td>0.1302072</td>\n",
       "<td>0.7935873</td>\n",
       "<td>1.3603761</td>\n",
       "<td>0.1444444</td>\n",
       "<td>0.1384109</td>\n",
       "<td>0.2476083</td>\n",
       "<td>0.2392871</td>\n",
       "<td>0.0793489</td>\n",
       "<td>0.8162089</td>\n",
       "<td>-20.6412720</td>\n",
       "<td>36.0376128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999753</td>\n",
       "<td>0.1160390</td>\n",
       "<td>0.6952367</td>\n",
       "<td>1.2653646</td>\n",
       "<td>0.1265432</td>\n",
       "<td>0.1229040</td>\n",
       "<td>0.2303148</td>\n",
       "<td>0.2226624</td>\n",
       "<td>0.0695151</td>\n",
       "<td>0.8857240</td>\n",
       "<td>-30.4763280</td>\n",
       "<td>26.5364591</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999630</td>\n",
       "<td>0.1030191</td>\n",
       "<td>0.5460152</td>\n",
       "<td>1.1754529</td>\n",
       "<td>0.0993827</td>\n",
       "<td>0.1093927</td>\n",
       "<td>0.2139495</td>\n",
       "<td>0.2085048</td>\n",
       "<td>0.0545948</td>\n",
       "<td>0.9403188</td>\n",
       "<td>-45.3984820</td>\n",
       "<td>17.5452853</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999506</td>\n",
       "<td>0.0899922</td>\n",
       "<td>0.3866194</td>\n",
       "<td>1.0878107</td>\n",
       "<td>0.0703704</td>\n",
       "<td>0.0965274</td>\n",
       "<td>0.1979974</td>\n",
       "<td>0.1960637</td>\n",
       "<td>0.0386572</td>\n",
       "<td>0.9789759</td>\n",
       "<td>-61.3380556</td>\n",
       "<td>8.7810707</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0661632</td>\n",
       "<td>0.2101370</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0382480</td>\n",
       "<td>0.0822573</td>\n",
       "<td>0.1820146</td>\n",
       "<td>0.1846775</td>\n",
       "<td>0.0210241</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.9862999</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100605                   0.607046           2.93242   2.93242            0.533742         0.674778   0.533742                    0.674778            0.0295015       0.0295015                  193.242   193.242\n",
       "    2        0.0200593                   0.537446           2.6792    2.8062             0.487654         0.570509   0.510769                    0.622804            0.0267887       0.0562903                  167.92    180.62\n",
       "    3        0.030058                    0.48805            2.44181   2.68498            0.444444         0.510637   0.488706                    0.585492            0.0244151       0.0807053                  144.181   168.498\n",
       "    4        0.0400568                   0.458592           2.50963   2.64121            0.45679          0.473099   0.48074                     0.557437            0.0250933       0.105799                   150.963   164.121\n",
       "    5        0.0500555                   0.431505           2.10267   2.53364            0.382716         0.444431   0.461159                    0.534864            0.0210241       0.126823                   110.267   153.364\n",
       "    6        0.100049                    0.333757           2.02806   2.281              0.369136         0.3753     0.415176                    0.455131            0.10139         0.228213                   102.806   128.1\n",
       "    7        0.150043                    0.280643           1.83136   2.13118            0.333333         0.306046   0.387906                    0.405457            0.0915565       0.319769                   83.1355   113.118\n",
       "    8        0.200037                    0.244396           1.59396   1.99692            0.290123         0.261229   0.363468                    0.369411            0.079688        0.399457                   59.3957   99.6917\n",
       "    9        0.300025                    0.197631           1.25821   1.75073            0.229012         0.218781   0.318659                    0.319211            0.125805        0.525263                   25.8209   75.0732\n",
       "    10       0.400012                    0.168723           1.09881   1.58778            0.2              0.182091   0.288999                    0.284937            0.109868        0.635131                   9.88132   58.7777\n",
       "    11       0.5                         0.146807           1.01742   1.47372            0.185185         0.157538   0.268238                    0.25946             0.101729        0.73686                    1.74196   47.372\n",
       "    12       0.599988                    0.130207           0.793587  1.36038            0.144444         0.138411   0.247608                    0.239287            0.0793489       0.816209                   -20.6413  36.0376\n",
       "    13       0.699975                    0.116039           0.695237  1.26536            0.126543         0.122904   0.230315                    0.222662            0.0695151       0.885724                   -30.4763  26.5365\n",
       "    14       0.799963                    0.103019           0.546015  1.17545            0.0993827        0.109393   0.21395                     0.208505            0.0545948       0.940319                   -45.3985  17.5453\n",
       "    15       0.899951                    0.0899922          0.386619  1.08781            0.0703704        0.0965274  0.197997                    0.196064            0.0386572       0.978976                   -61.3381  8.78107\n",
       "    16       1                           0.0661632          0.210137  1                  0.038248         0.0822573  0.182015                    0.184677            0.0210241       1                          -78.9863  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.13857609376371857\n",
      "RMSE: 0.3722581010048251\n",
      "LogLoss: 0.4401913910510862\n",
      "Null degrees of freedom: 147784\n",
      "Residual degrees of freedom: 147781\n",
      "Null deviance: 140733.9728124682\n",
      "Residual deviance: 130107.36945296955\n",
      "AIC: 130115.36945296955\n",
      "AUC: 0.7020527753495225\n",
      "pr_auc: 0.3375985314167658\n",
      "Gini: 0.40410555069904497\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.17298272394626243: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>81649.0</td>\n",
       "<td>39069.0</td>\n",
       "<td>0.3236</td>\n",
       "<td> (39069.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>10296.0</td>\n",
       "<td>16771.0</td>\n",
       "<td>0.3804</td>\n",
       "<td> (10296.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>91945.0</td>\n",
       "<td>55840.0</td>\n",
       "<td>0.334</td>\n",
       "<td> (49365.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      81649  39069  0.3236   (39069.0/120718.0)\n",
       "1      10296  16771  0.3804   (10296.0/27067.0)\n",
       "Total  91945  55840  0.334    (49365.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1729827</td>\n",
       "<td>0.4045738</td>\n",
       "<td>275.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1166841</td>\n",
       "<td>0.5633569</td>\n",
       "<td>341.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2578390</td>\n",
       "<td>0.3662781</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6424048</td>\n",
       "<td>0.8173698</td>\n",
       "<td>31.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8611637</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0700469</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8611637</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1825932</td>\n",
       "<td>0.2367313</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1657609</td>\n",
       "<td>0.6461004</td>\n",
       "<td>282.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1604917</td>\n",
       "<td>0.6483125</td>\n",
       "<td>288.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.172983     0.404574  275\n",
       "max f2                       0.116684     0.563357  341\n",
       "max f0point5                 0.257839     0.366278  202\n",
       "max accuracy                 0.642405     0.81737   31\n",
       "max precision                0.861164     1         0\n",
       "max recall                   0.0700469    1         397\n",
       "max specificity              0.861164     1         0\n",
       "max absolute_mcc             0.182593     0.236731  265\n",
       "max min_per_class_accuracy   0.165761     0.6461    282\n",
       "max mean_per_class_accuracy  0.160492     0.648312  288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.32 %, avg score: 18.32 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100010</td>\n",
       "<td>0.5992652</td>\n",
       "<td>2.8149506</td>\n",
       "<td>2.8149506</td>\n",
       "<td>0.5155616</td>\n",
       "<td>0.6651941</td>\n",
       "<td>0.5155616</td>\n",
       "<td>0.6651941</td>\n",
       "<td>0.0281524</td>\n",
       "<td>0.0281524</td>\n",
       "<td>181.4950551</td>\n",
       "<td>181.4950551</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200020</td>\n",
       "<td>0.5304044</td>\n",
       "<td>2.6006892</td>\n",
       "<td>2.7078199</td>\n",
       "<td>0.4763194</td>\n",
       "<td>0.5618491</td>\n",
       "<td>0.4959405</td>\n",
       "<td>0.6135216</td>\n",
       "<td>0.0260095</td>\n",
       "<td>0.0541619</td>\n",
       "<td>160.0689223</td>\n",
       "<td>170.7819887</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300030</td>\n",
       "<td>0.4841323</td>\n",
       "<td>2.3938162</td>\n",
       "<td>2.6031520</td>\n",
       "<td>0.4384303</td>\n",
       "<td>0.5064587</td>\n",
       "<td>0.4767704</td>\n",
       "<td>0.5778339</td>\n",
       "<td>0.0239406</td>\n",
       "<td>0.0781025</td>\n",
       "<td>139.3816217</td>\n",
       "<td>160.3151997</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400041</td>\n",
       "<td>0.4512997</td>\n",
       "<td>2.2201907</td>\n",
       "<td>2.5074117</td>\n",
       "<td>0.4066306</td>\n",
       "<td>0.4675155</td>\n",
       "<td>0.4592355</td>\n",
       "<td>0.5502543</td>\n",
       "<td>0.0222042</td>\n",
       "<td>0.1003066</td>\n",
       "<td>122.0190658</td>\n",
       "<td>150.7411662</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500051</td>\n",
       "<td>0.4240390</td>\n",
       "<td>2.2682147</td>\n",
       "<td>2.4595723</td>\n",
       "<td>0.4154263</td>\n",
       "<td>0.4372611</td>\n",
       "<td>0.4504736</td>\n",
       "<td>0.5276557</td>\n",
       "<td>0.0226844</td>\n",
       "<td>0.1229911</td>\n",
       "<td>126.8214749</td>\n",
       "<td>145.9572280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000034</td>\n",
       "<td>0.3321782</td>\n",
       "<td>2.0098955</td>\n",
       "<td>2.2347491</td>\n",
       "<td>0.3681148</td>\n",
       "<td>0.3726242</td>\n",
       "<td>0.4092970</td>\n",
       "<td>0.4501452</td>\n",
       "<td>0.1004914</td>\n",
       "<td>0.2234825</td>\n",
       "<td>100.9895466</td>\n",
       "<td>123.4749086</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500017</td>\n",
       "<td>0.2786483</td>\n",
       "<td>1.7675257</td>\n",
       "<td>2.0790150</td>\n",
       "<td>0.3237245</td>\n",
       "<td>0.3035444</td>\n",
       "<td>0.3807741</td>\n",
       "<td>0.4012804</td>\n",
       "<td>0.0883733</td>\n",
       "<td>0.3118558</td>\n",
       "<td>76.7525718</td>\n",
       "<td>107.9014989</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2425797</td>\n",
       "<td>1.5857484</td>\n",
       "<td>1.9557025</td>\n",
       "<td>0.2904317</td>\n",
       "<td>0.2595146</td>\n",
       "<td>0.3581893</td>\n",
       "<td>0.3658402</td>\n",
       "<td>0.0792847</td>\n",
       "<td>0.3911405</td>\n",
       "<td>58.5748408</td>\n",
       "<td>95.5702516</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000034</td>\n",
       "<td>0.1959714</td>\n",
       "<td>1.3695181</td>\n",
       "<td>1.7603033</td>\n",
       "<td>0.2508289</td>\n",
       "<td>0.2172719</td>\n",
       "<td>0.3224017</td>\n",
       "<td>0.3163163</td>\n",
       "<td>0.1369564</td>\n",
       "<td>0.5280969</td>\n",
       "<td>36.9518079</td>\n",
       "<td>76.0303297</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1667087</td>\n",
       "<td>1.1305662</td>\n",
       "<td>1.6028743</td>\n",
       "<td>0.2070646</td>\n",
       "<td>0.1805241</td>\n",
       "<td>0.2935684</td>\n",
       "<td>0.2823694</td>\n",
       "<td>0.1130528</td>\n",
       "<td>0.6411497</td>\n",
       "<td>13.0566200</td>\n",
       "<td>60.2874349</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000034</td>\n",
       "<td>0.1457423</td>\n",
       "<td>0.9420748</td>\n",
       "<td>1.4707109</td>\n",
       "<td>0.1725421</td>\n",
       "<td>0.1557592</td>\n",
       "<td>0.2693625</td>\n",
       "<td>0.2570467</td>\n",
       "<td>0.0942107</td>\n",
       "<td>0.7353604</td>\n",
       "<td>-5.7925249</td>\n",
       "<td>47.0710852</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1292848</td>\n",
       "<td>0.7906574</td>\n",
       "<td>1.3573724</td>\n",
       "<td>0.1448099</td>\n",
       "<td>0.1372581</td>\n",
       "<td>0.2486044</td>\n",
       "<td>0.2370826</td>\n",
       "<td>0.0790631</td>\n",
       "<td>0.8144235</td>\n",
       "<td>-20.9342592</td>\n",
       "<td>35.7372446</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999966</td>\n",
       "<td>0.1152511</td>\n",
       "<td>0.6683641</td>\n",
       "<td>1.2589455</td>\n",
       "<td>0.1224117</td>\n",
       "<td>0.1221946</td>\n",
       "<td>0.2305774</td>\n",
       "<td>0.2206705</td>\n",
       "<td>0.0668342</td>\n",
       "<td>0.8812576</td>\n",
       "<td>-33.1635864</td>\n",
       "<td>25.8945542</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.1023359</td>\n",
       "<td>0.5445562</td>\n",
       "<td>1.1696438</td>\n",
       "<td>0.0997361</td>\n",
       "<td>0.1087495</td>\n",
       "<td>0.2142217</td>\n",
       "<td>0.2066799</td>\n",
       "<td>0.0544575</td>\n",
       "<td>0.9357151</td>\n",
       "<td>-45.5443850</td>\n",
       "<td>16.9643847</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999966</td>\n",
       "<td>0.0897076</td>\n",
       "<td>0.4208219</td>\n",
       "<td>1.0864439</td>\n",
       "<td>0.0770740</td>\n",
       "<td>0.0960452</td>\n",
       "<td>0.1989835</td>\n",
       "<td>0.1943875</td>\n",
       "<td>0.0420808</td>\n",
       "<td>0.9777958</td>\n",
       "<td>-57.9178137</td>\n",
       "<td>8.6443906</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0634272</td>\n",
       "<td>0.2220341</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0406658</td>\n",
       "<td>0.0820197</td>\n",
       "<td>0.1831512</td>\n",
       "<td>0.1831504</td>\n",
       "<td>0.0222042</td>\n",
       "<td>1.0</td>\n",
       "<td>-77.7965912</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010001                    0.599265           2.81495   2.81495            0.515562         0.665194   0.515562                    0.665194            0.0281524       0.0281524                  181.495   181.495\n",
       "    2        0.020002                    0.530404           2.60069   2.70782            0.476319         0.561849   0.49594                     0.613522            0.0260095       0.0541619                  160.069   170.782\n",
       "    3        0.030003                    0.484132           2.39382   2.60315            0.43843          0.506459   0.47677                     0.577834            0.0239406       0.0781025                  139.382   160.315\n",
       "    4        0.0400041                   0.4513             2.22019   2.50741            0.406631         0.467515   0.459235                    0.550254            0.0222042       0.100307                   122.019   150.741\n",
       "    5        0.0500051                   0.424039           2.26821   2.45957            0.415426         0.437261   0.450474                    0.527656            0.0226844       0.122991                   126.821   145.957\n",
       "    6        0.100003                    0.332178           2.0099    2.23475            0.368115         0.372624   0.409297                    0.450145            0.100491        0.223482                   100.99    123.475\n",
       "    7        0.150002                    0.278648           1.76753   2.07901            0.323724         0.303544   0.380774                    0.40128             0.0883733       0.311856                   76.7526   107.901\n",
       "    8        0.2                         0.24258            1.58575   1.9557             0.290432         0.259515   0.358189                    0.36584             0.0792847       0.391141                   58.5748   95.5703\n",
       "    9        0.300003                    0.195971           1.36952   1.7603             0.250829         0.217272   0.322402                    0.316316            0.136956        0.528097                   36.9518   76.0303\n",
       "    10       0.4                         0.166709           1.13057   1.60287            0.207065         0.180524   0.293568                    0.282369            0.113053        0.64115                    13.0566   60.2874\n",
       "    11       0.500003                    0.145742           0.942075  1.47071            0.172542         0.155759   0.269362                    0.257047            0.0942107       0.73536                    -5.79252  47.0711\n",
       "    12       0.6                         0.129285           0.790657  1.35737            0.14481          0.137258   0.248604                    0.237083            0.0790631       0.814423                   -20.9343  35.7372\n",
       "    13       0.699997                    0.115251           0.668364  1.25895            0.122412         0.122195   0.230577                    0.220671            0.0668342       0.881258                   -33.1636  25.8946\n",
       "    14       0.8                         0.102336           0.544556  1.16964            0.0997361        0.10875    0.214222                    0.20668             0.0544575       0.935715                   -45.5444  16.9644\n",
       "    15       0.899997                    0.0897076          0.420822  1.08644            0.077074         0.0960452  0.198984                    0.194388            0.0420808       0.977796                   -57.9178  8.64439\n",
       "    16       1                           0.0634272          0.222034  1                  0.0406658        0.0820197  0.183151                    0.18315             0.0222042       1                          -77.7966  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method H2OBinomialModel.tnr of >"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.tnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_AutoML_20190108_000309\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0748160248427708\n",
      "RMSE: 0.27352518136868276\n",
      "LogLoss: 0.2770108718654861\n",
      "Null degrees of freedom: 147784\n",
      "Residual degrees of freedom: 147781\n",
      "Null deviance: 140732.20131171282\n",
      "Residual deviance: 81876.10339728172\n",
      "AIC: 81884.10339728172\n",
      "AUC: 0.9705354292408278\n",
      "pr_auc: 0.8960099911776978\n",
      "Gini: 0.9410708584816556\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3054804393299643: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>114475.0</td>\n",
       "<td>6243.0</td>\n",
       "<td>0.0517</td>\n",
       "<td> (6243.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>4811.0</td>\n",
       "<td>22256.0</td>\n",
       "<td>0.1777</td>\n",
       "<td> (4811.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>119286.0</td>\n",
       "<td>28499.0</td>\n",
       "<td>0.0748</td>\n",
       "<td> (11054.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1      Error    Rate\n",
       "-----  ------  -----  -------  ------------------\n",
       "0      114475  6243   0.0517   (6243.0/120718.0)\n",
       "1      4811    22256  0.1777   (4811.0/27067.0)\n",
       "Total  119286  28499  0.0748   (11054.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.8010654</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2324010</td>\n",
       "<td>0.8527882</td>\n",
       "<td>256.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4116258</td>\n",
       "<td>0.8349729</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.3440950</td>\n",
       "<td>0.9274622</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1052357</td>\n",
       "<td>1.0</td>\n",
       "<td>361.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.7554259</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2517271</td>\n",
       "<td>0.9051343</td>\n",
       "<td>245.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2395344</td>\n",
       "<td>0.9063511</td>\n",
       "<td>252.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.30548      0.801065  216\n",
       "max f2                       0.232401     0.852788  256\n",
       "max f0point5                 0.411626     0.834973  167\n",
       "max accuracy                 0.344095     0.927462  198\n",
       "max precision                0.963831     1         0\n",
       "max recall                   0.105236     1         361\n",
       "max specificity              0.963831     1         0\n",
       "max absolute_mcc             0.30548      0.755426  216\n",
       "max min_per_class_accuracy   0.251727     0.905134  245\n",
       "max mean_per_class_accuracy  0.239534     0.906351  252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.32 %, avg score: 21.52 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100010</td>\n",
       "<td>0.8231913</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.0546052</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200020</td>\n",
       "<td>0.7645343</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7922026</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8293156</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1092105</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300030</td>\n",
       "<td>0.7172474</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7405818</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7997376</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1638157</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400041</td>\n",
       "<td>0.6758864</td>\n",
       "<td>5.4304164</td>\n",
       "<td>5.4525814</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.6959487</td>\n",
       "<td>0.9986468</td>\n",
       "<td>0.7737904</td>\n",
       "<td>0.0543097</td>\n",
       "<td>0.2181254</td>\n",
       "<td>443.0416418</td>\n",
       "<td>445.2581383</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500051</td>\n",
       "<td>0.6383709</td>\n",
       "<td>5.3417566</td>\n",
       "<td>5.4304164</td>\n",
       "<td>0.9783491</td>\n",
       "<td>0.6571369</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.7504597</td>\n",
       "<td>0.0534230</td>\n",
       "<td>0.2715484</td>\n",
       "<td>434.1756558</td>\n",
       "<td>443.0416418</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000034</td>\n",
       "<td>0.4826144</td>\n",
       "<td>4.9922256</td>\n",
       "<td>5.2113359</td>\n",
       "<td>0.9143321</td>\n",
       "<td>0.5558423</td>\n",
       "<td>0.9544624</td>\n",
       "<td>0.6531576</td>\n",
       "<td>0.2496028</td>\n",
       "<td>0.5211512</td>\n",
       "<td>399.2225650</td>\n",
       "<td>421.1335859</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500017</td>\n",
       "<td>0.3715061</td>\n",
       "<td>3.7611647</td>\n",
       "<td>4.7279673</td>\n",
       "<td>0.6888618</td>\n",
       "<td>0.4241561</td>\n",
       "<td>0.8659329</td>\n",
       "<td>0.5768272</td>\n",
       "<td>0.1880519</td>\n",
       "<td>0.7092031</td>\n",
       "<td>276.1164677</td>\n",
       "<td>372.7967271</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2957882</td>\n",
       "<td>2.5648335</td>\n",
       "<td>4.1872021</td>\n",
       "<td>0.4697523</td>\n",
       "<td>0.3308477</td>\n",
       "<td>0.7668911</td>\n",
       "<td>0.5153344</td>\n",
       "<td>0.1282373</td>\n",
       "<td>0.8374404</td>\n",
       "<td>156.4833515</td>\n",
       "<td>318.7202128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000034</td>\n",
       "<td>0.2099951</td>\n",
       "<td>1.1785170</td>\n",
       "<td>3.1842845</td>\n",
       "<td>0.2158468</td>\n",
       "<td>0.2476441</td>\n",
       "<td>0.5832055</td>\n",
       "<td>0.4261023</td>\n",
       "<td>0.1178557</td>\n",
       "<td>0.9552961</td>\n",
       "<td>17.8517042</td>\n",
       "<td>218.4284479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1682202</td>\n",
       "<td>0.3351057</td>\n",
       "<td>2.4720139</td>\n",
       "<td>0.0613750</td>\n",
       "<td>0.1869477</td>\n",
       "<td>0.4527523</td>\n",
       "<td>0.3663157</td>\n",
       "<td>0.0335094</td>\n",
       "<td>0.9888056</td>\n",
       "<td>-66.4894267</td>\n",
       "<td>147.2013891</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000034</td>\n",
       "<td>0.1432933</td>\n",
       "<td>0.0897742</td>\n",
       "<td>1.9955531</td>\n",
       "<td>0.0164422</td>\n",
       "<td>0.1548801</td>\n",
       "<td>0.3654879</td>\n",
       "<td>0.3240274</td>\n",
       "<td>0.0089777</td>\n",
       "<td>0.9977833</td>\n",
       "<td>-91.0225818</td>\n",
       "<td>99.5553054</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1259732</td>\n",
       "<td>0.0195817</td>\n",
       "<td>1.6662356</td>\n",
       "<td>0.0035864</td>\n",
       "<td>0.1341923</td>\n",
       "<td>0.3051731</td>\n",
       "<td>0.2923893</td>\n",
       "<td>0.0019581</td>\n",
       "<td>0.9997414</td>\n",
       "<td>-98.0418298</td>\n",
       "<td>66.6235637</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999966</td>\n",
       "<td>0.1118935</td>\n",
       "<td>0.0022168</td>\n",
       "<td>1.4285256</td>\n",
       "<td>0.0004060</td>\n",
       "<td>0.1187515</td>\n",
       "<td>0.2616362</td>\n",
       "<td>0.2675846</td>\n",
       "<td>0.0002217</td>\n",
       "<td>0.9999631</td>\n",
       "<td>-99.7783204</td>\n",
       "<td>42.8525554</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0991367</td>\n",
       "<td>0.0003694</td>\n",
       "<td>1.25</td>\n",
       "<td>0.0000677</td>\n",
       "<td>0.1054206</td>\n",
       "<td>0.2289390</td>\n",
       "<td>0.2473134</td>\n",
       "<td>0.0000369</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.9630559</td>\n",
       "<td>25.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999966</td>\n",
       "<td>0.0873202</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111153</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0932206</td>\n",
       "<td>0.2035021</td>\n",
       "<td>0.2301925</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1115288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0637468</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0802774</td>\n",
       "<td>0.1831512</td>\n",
       "<td>0.2152005</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift         cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -----------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010001                    0.823191           5.45997      5.45997            1                0.866429   1                           0.866429            0.0546052       0.0546052                  445.997   445.997\n",
       "    2        0.020002                    0.764534           5.45997      5.45997            1                0.792203   1                           0.829316            0.0546052       0.10921                    445.997   445.997\n",
       "    3        0.030003                    0.717247           5.45997      5.45997            1                0.740582   1                           0.799738            0.0546052       0.163816                   445.997   445.997\n",
       "    4        0.0400041                   0.675886           5.43042      5.45258            0.994587         0.695949   0.998647                    0.77379             0.0543097       0.218125                   443.042   445.258\n",
       "    5        0.0500051                   0.638371           5.34176      5.43042            0.978349         0.657137   0.994587                    0.75046             0.053423        0.271548                   434.176   443.042\n",
       "    6        0.100003                    0.482614           4.99223      5.21134            0.914332         0.555842   0.954462                    0.653158            0.249603        0.521151                   399.223   421.134\n",
       "    7        0.150002                    0.371506           3.76116      4.72797            0.688862         0.424156   0.865933                    0.576827            0.188052        0.709203                   276.116   372.797\n",
       "    8        0.2                         0.295788           2.56483      4.1872             0.469752         0.330848   0.766891                    0.515334            0.128237        0.83744                    156.483   318.72\n",
       "    9        0.300003                    0.209995           1.17852      3.18428            0.215847         0.247644   0.583206                    0.426102            0.117856        0.955296                   17.8517   218.428\n",
       "    10       0.4                         0.16822            0.335106     2.47201            0.061375         0.186948   0.452752                    0.366316            0.0335094       0.988806                   -66.4894  147.201\n",
       "    11       0.500003                    0.143293           0.0897742    1.99555            0.0164422        0.15488    0.365488                    0.324027            0.00897772      0.997783                   -91.0226  99.5553\n",
       "    12       0.6                         0.125973           0.0195817    1.66624            0.00358641       0.134192   0.305173                    0.292389            0.0019581       0.999741                   -98.0418  66.6236\n",
       "    13       0.699997                    0.111893           0.0022168    1.42853            0.000406009      0.118752   0.261636                    0.267585            0.000221672     0.999963                   -99.7783  42.8526\n",
       "    14       0.8                         0.0991367          0.000369441  1.25               6.76636e-05      0.105421   0.228939                    0.247313            3.69454e-05     1                          -99.9631  25\n",
       "    15       0.899997                    0.0873202          0            1.11112            0                0.0932206  0.203502                    0.230193            0               1                          -100      11.1115\n",
       "    16       1                           0.0637468          0            1                  0                0.0802774  0.183151                    0.215201            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1374305415577651\n",
      "RMSE: 0.370716254779535\n",
      "LogLoss: 0.4374035749971233\n",
      "Null degrees of freedom: 16201\n",
      "Residual degrees of freedom: 16198\n",
      "Null deviance: 15373.717640908228\n",
      "Residual deviance: 14173.625444206784\n",
      "AIC: 14181.625444206784\n",
      "AUC: 0.7038412027583177\n",
      "pr_auc: 0.34120380509032733\n",
      "Gini: 0.4076824055166355\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.19535344211739553: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>9850.0</td>\n",
       "<td>3403.0</td>\n",
       "<td>0.2568</td>\n",
       "<td> (3403.0/13253.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1367.0</td>\n",
       "<td>1582.0</td>\n",
       "<td>0.4635</td>\n",
       "<td> (1367.0/2949.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11217.0</td>\n",
       "<td>4985.0</td>\n",
       "<td>0.2944</td>\n",
       "<td> (4770.0/16202.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      9850   3403  0.2568   (3403.0/13253.0)\n",
       "1      1367   1582  0.4635   (1367.0/2949.0)\n",
       "Total  11217  4985  0.2944   (4770.0/16202.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1953534</td>\n",
       "<td>0.3987900</td>\n",
       "<td>248.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1166556</td>\n",
       "<td>0.5648862</td>\n",
       "<td>337.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2709433</td>\n",
       "<td>0.3762722</td>\n",
       "<td>185.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6365348</td>\n",
       "<td>0.8189730</td>\n",
       "<td>28.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7519927</td>\n",
       "<td>0.6470588</td>\n",
       "<td>7.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0720133</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8355440</td>\n",
       "<td>0.9999245</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2288610</td>\n",
       "<td>0.2366986</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1667771</td>\n",
       "<td>0.6414397</td>\n",
       "<td>277.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1542798</td>\n",
       "<td>0.6472159</td>\n",
       "<td>291.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.195353     0.39879   248\n",
       "max f2                       0.116656     0.564886  337\n",
       "max f0point5                 0.270943     0.376272  185\n",
       "max accuracy                 0.636535     0.818973  28\n",
       "max precision                0.751993     0.647059  7\n",
       "max recall                   0.0720133    1         397\n",
       "max specificity              0.835544     0.999925  0\n",
       "max absolute_mcc             0.228861     0.236699  218\n",
       "max min_per_class_accuracy   0.166777     0.64144   277\n",
       "max mean_per_class_accuracy  0.15428      0.647216  291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.20 %, avg score: 18.47 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100605</td>\n",
       "<td>0.6070463</td>\n",
       "<td>2.9324155</td>\n",
       "<td>2.9324155</td>\n",
       "<td>0.5337423</td>\n",
       "<td>0.6747785</td>\n",
       "<td>0.5337423</td>\n",
       "<td>0.6747785</td>\n",
       "<td>0.0295015</td>\n",
       "<td>0.0295015</td>\n",
       "<td>193.2415480</td>\n",
       "<td>193.2415480</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200593</td>\n",
       "<td>0.5374457</td>\n",
       "<td>2.6792049</td>\n",
       "<td>2.8061998</td>\n",
       "<td>0.4876543</td>\n",
       "<td>0.5705091</td>\n",
       "<td>0.5107692</td>\n",
       "<td>0.6228042</td>\n",
       "<td>0.0267887</td>\n",
       "<td>0.0562903</td>\n",
       "<td>167.9204920</td>\n",
       "<td>180.6199755</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300580</td>\n",
       "<td>0.4880498</td>\n",
       "<td>2.4418070</td>\n",
       "<td>2.6849849</td>\n",
       "<td>0.4444444</td>\n",
       "<td>0.5106372</td>\n",
       "<td>0.4887064</td>\n",
       "<td>0.5854920</td>\n",
       "<td>0.0244151</td>\n",
       "<td>0.0807053</td>\n",
       "<td>144.1807016</td>\n",
       "<td>168.4984922</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400568</td>\n",
       "<td>0.4585920</td>\n",
       "<td>2.5096350</td>\n",
       "<td>2.6412150</td>\n",
       "<td>0.4567901</td>\n",
       "<td>0.4730993</td>\n",
       "<td>0.4807396</td>\n",
       "<td>0.5574371</td>\n",
       "<td>0.0250933</td>\n",
       "<td>0.1057986</td>\n",
       "<td>150.9634988</td>\n",
       "<td>164.1214984</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500555</td>\n",
       "<td>0.4315051</td>\n",
       "<td>2.1026672</td>\n",
       "<td>2.5336382</td>\n",
       "<td>0.3827160</td>\n",
       "<td>0.4444311</td>\n",
       "<td>0.4611591</td>\n",
       "<td>0.5348638</td>\n",
       "<td>0.0210241</td>\n",
       "<td>0.1268227</td>\n",
       "<td>110.2667152</td>\n",
       "<td>153.3638229</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000494</td>\n",
       "<td>0.3337569</td>\n",
       "<td>2.0280564</td>\n",
       "<td>2.2810033</td>\n",
       "<td>0.3691358</td>\n",
       "<td>0.3752998</td>\n",
       "<td>0.4151758</td>\n",
       "<td>0.4551310</td>\n",
       "<td>0.1013903</td>\n",
       "<td>0.2282130</td>\n",
       "<td>102.8056382</td>\n",
       "<td>128.1003253</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500432</td>\n",
       "<td>0.2806429</td>\n",
       "<td>1.8313553</td>\n",
       "<td>2.1311822</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.3060461</td>\n",
       "<td>0.3879062</td>\n",
       "<td>0.4054565</td>\n",
       "<td>0.0915565</td>\n",
       "<td>0.3197694</td>\n",
       "<td>83.1355262</td>\n",
       "<td>113.1182244</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000370</td>\n",
       "<td>0.2443961</td>\n",
       "<td>1.5939574</td>\n",
       "<td>1.9969175</td>\n",
       "<td>0.2901235</td>\n",
       "<td>0.2612294</td>\n",
       "<td>0.3634681</td>\n",
       "<td>0.3694109</td>\n",
       "<td>0.0796880</td>\n",
       "<td>0.3994574</td>\n",
       "<td>59.3957357</td>\n",
       "<td>99.6917462</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000247</td>\n",
       "<td>0.1976308</td>\n",
       "<td>1.2582089</td>\n",
       "<td>1.7507319</td>\n",
       "<td>0.2290123</td>\n",
       "<td>0.2187809</td>\n",
       "<td>0.3186587</td>\n",
       "<td>0.3192112</td>\n",
       "<td>0.1258054</td>\n",
       "<td>0.5252628</td>\n",
       "<td>25.8208893</td>\n",
       "<td>75.0731928</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000123</td>\n",
       "<td>0.1687228</td>\n",
       "<td>1.0988132</td>\n",
       "<td>1.5877774</td>\n",
       "<td>0.2</td>\n",
       "<td>0.1820912</td>\n",
       "<td>0.2889986</td>\n",
       "<td>0.2849365</td>\n",
       "<td>0.1098678</td>\n",
       "<td>0.6351306</td>\n",
       "<td>9.8813157</td>\n",
       "<td>58.7777382</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1468073</td>\n",
       "<td>1.0174196</td>\n",
       "<td>1.4737199</td>\n",
       "<td>0.1851852</td>\n",
       "<td>0.1575376</td>\n",
       "<td>0.2682385</td>\n",
       "<td>0.2594599</td>\n",
       "<td>0.1017294</td>\n",
       "<td>0.7368600</td>\n",
       "<td>1.7419590</td>\n",
       "<td>47.3719905</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999877</td>\n",
       "<td>0.1302072</td>\n",
       "<td>0.7935873</td>\n",
       "<td>1.3603761</td>\n",
       "<td>0.1444444</td>\n",
       "<td>0.1384109</td>\n",
       "<td>0.2476083</td>\n",
       "<td>0.2392871</td>\n",
       "<td>0.0793489</td>\n",
       "<td>0.8162089</td>\n",
       "<td>-20.6412720</td>\n",
       "<td>36.0376128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999753</td>\n",
       "<td>0.1160390</td>\n",
       "<td>0.6952367</td>\n",
       "<td>1.2653646</td>\n",
       "<td>0.1265432</td>\n",
       "<td>0.1229040</td>\n",
       "<td>0.2303148</td>\n",
       "<td>0.2226624</td>\n",
       "<td>0.0695151</td>\n",
       "<td>0.8857240</td>\n",
       "<td>-30.4763280</td>\n",
       "<td>26.5364591</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999630</td>\n",
       "<td>0.1030191</td>\n",
       "<td>0.5460152</td>\n",
       "<td>1.1754529</td>\n",
       "<td>0.0993827</td>\n",
       "<td>0.1093927</td>\n",
       "<td>0.2139495</td>\n",
       "<td>0.2085048</td>\n",
       "<td>0.0545948</td>\n",
       "<td>0.9403188</td>\n",
       "<td>-45.3984820</td>\n",
       "<td>17.5452853</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999506</td>\n",
       "<td>0.0899922</td>\n",
       "<td>0.3866194</td>\n",
       "<td>1.0878107</td>\n",
       "<td>0.0703704</td>\n",
       "<td>0.0965274</td>\n",
       "<td>0.1979974</td>\n",
       "<td>0.1960637</td>\n",
       "<td>0.0386572</td>\n",
       "<td>0.9789759</td>\n",
       "<td>-61.3380556</td>\n",
       "<td>8.7810707</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0661632</td>\n",
       "<td>0.2101370</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0382480</td>\n",
       "<td>0.0822573</td>\n",
       "<td>0.1820146</td>\n",
       "<td>0.1846775</td>\n",
       "<td>0.0210241</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.9862999</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100605                   0.607046           2.93242   2.93242            0.533742         0.674778   0.533742                    0.674778            0.0295015       0.0295015                  193.242   193.242\n",
       "    2        0.0200593                   0.537446           2.6792    2.8062             0.487654         0.570509   0.510769                    0.622804            0.0267887       0.0562903                  167.92    180.62\n",
       "    3        0.030058                    0.48805            2.44181   2.68498            0.444444         0.510637   0.488706                    0.585492            0.0244151       0.0807053                  144.181   168.498\n",
       "    4        0.0400568                   0.458592           2.50963   2.64121            0.45679          0.473099   0.48074                     0.557437            0.0250933       0.105799                   150.963   164.121\n",
       "    5        0.0500555                   0.431505           2.10267   2.53364            0.382716         0.444431   0.461159                    0.534864            0.0210241       0.126823                   110.267   153.364\n",
       "    6        0.100049                    0.333757           2.02806   2.281              0.369136         0.3753     0.415176                    0.455131            0.10139         0.228213                   102.806   128.1\n",
       "    7        0.150043                    0.280643           1.83136   2.13118            0.333333         0.306046   0.387906                    0.405457            0.0915565       0.319769                   83.1355   113.118\n",
       "    8        0.200037                    0.244396           1.59396   1.99692            0.290123         0.261229   0.363468                    0.369411            0.079688        0.399457                   59.3957   99.6917\n",
       "    9        0.300025                    0.197631           1.25821   1.75073            0.229012         0.218781   0.318659                    0.319211            0.125805        0.525263                   25.8209   75.0732\n",
       "    10       0.400012                    0.168723           1.09881   1.58778            0.2              0.182091   0.288999                    0.284937            0.109868        0.635131                   9.88132   58.7777\n",
       "    11       0.5                         0.146807           1.01742   1.47372            0.185185         0.157538   0.268238                    0.25946             0.101729        0.73686                    1.74196   47.372\n",
       "    12       0.599988                    0.130207           0.793587  1.36038            0.144444         0.138411   0.247608                    0.239287            0.0793489       0.816209                   -20.6413  36.0376\n",
       "    13       0.699975                    0.116039           0.695237  1.26536            0.126543         0.122904   0.230315                    0.222662            0.0695151       0.885724                   -30.4763  26.5365\n",
       "    14       0.799963                    0.103019           0.546015  1.17545            0.0993827        0.109393   0.21395                     0.208505            0.0545948       0.940319                   -45.3985  17.5453\n",
       "    15       0.899951                    0.0899922          0.386619  1.08781            0.0703704        0.0965274  0.197997                    0.196064            0.0386572       0.978976                   -61.3381  8.78107\n",
       "    16       1                           0.0661632          0.210137  1                  0.038248         0.0822573  0.182015                    0.184677            0.0210241       1                          -78.9863  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.13857609376371857\n",
      "RMSE: 0.3722581010048251\n",
      "LogLoss: 0.4401913910510862\n",
      "Null degrees of freedom: 147784\n",
      "Residual degrees of freedom: 147781\n",
      "Null deviance: 140733.9728124682\n",
      "Residual deviance: 130107.36945296955\n",
      "AIC: 130115.36945296955\n",
      "AUC: 0.7020527753495225\n",
      "pr_auc: 0.3375985314167658\n",
      "Gini: 0.40410555069904497\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.17298272394626243: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>81649.0</td>\n",
       "<td>39069.0</td>\n",
       "<td>0.3236</td>\n",
       "<td> (39069.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>10296.0</td>\n",
       "<td>16771.0</td>\n",
       "<td>0.3804</td>\n",
       "<td> (10296.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>91945.0</td>\n",
       "<td>55840.0</td>\n",
       "<td>0.334</td>\n",
       "<td> (49365.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      81649  39069  0.3236   (39069.0/120718.0)\n",
       "1      10296  16771  0.3804   (10296.0/27067.0)\n",
       "Total  91945  55840  0.334    (49365.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1729827</td>\n",
       "<td>0.4045738</td>\n",
       "<td>275.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1166841</td>\n",
       "<td>0.5633569</td>\n",
       "<td>341.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2578390</td>\n",
       "<td>0.3662781</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6424048</td>\n",
       "<td>0.8173698</td>\n",
       "<td>31.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8611637</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0700469</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8611637</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1825932</td>\n",
       "<td>0.2367313</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1657609</td>\n",
       "<td>0.6461004</td>\n",
       "<td>282.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1604917</td>\n",
       "<td>0.6483125</td>\n",
       "<td>288.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.172983     0.404574  275\n",
       "max f2                       0.116684     0.563357  341\n",
       "max f0point5                 0.257839     0.366278  202\n",
       "max accuracy                 0.642405     0.81737   31\n",
       "max precision                0.861164     1         0\n",
       "max recall                   0.0700469    1         397\n",
       "max specificity              0.861164     1         0\n",
       "max absolute_mcc             0.182593     0.236731  265\n",
       "max min_per_class_accuracy   0.165761     0.6461    282\n",
       "max mean_per_class_accuracy  0.160492     0.648312  288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.32 %, avg score: 18.32 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100010</td>\n",
       "<td>0.5992652</td>\n",
       "<td>2.8149506</td>\n",
       "<td>2.8149506</td>\n",
       "<td>0.5155616</td>\n",
       "<td>0.6651941</td>\n",
       "<td>0.5155616</td>\n",
       "<td>0.6651941</td>\n",
       "<td>0.0281524</td>\n",
       "<td>0.0281524</td>\n",
       "<td>181.4950551</td>\n",
       "<td>181.4950551</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200020</td>\n",
       "<td>0.5304044</td>\n",
       "<td>2.6006892</td>\n",
       "<td>2.7078199</td>\n",
       "<td>0.4763194</td>\n",
       "<td>0.5618491</td>\n",
       "<td>0.4959405</td>\n",
       "<td>0.6135216</td>\n",
       "<td>0.0260095</td>\n",
       "<td>0.0541619</td>\n",
       "<td>160.0689223</td>\n",
       "<td>170.7819887</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300030</td>\n",
       "<td>0.4841323</td>\n",
       "<td>2.3938162</td>\n",
       "<td>2.6031520</td>\n",
       "<td>0.4384303</td>\n",
       "<td>0.5064587</td>\n",
       "<td>0.4767704</td>\n",
       "<td>0.5778339</td>\n",
       "<td>0.0239406</td>\n",
       "<td>0.0781025</td>\n",
       "<td>139.3816217</td>\n",
       "<td>160.3151997</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400041</td>\n",
       "<td>0.4512997</td>\n",
       "<td>2.2201907</td>\n",
       "<td>2.5074117</td>\n",
       "<td>0.4066306</td>\n",
       "<td>0.4675155</td>\n",
       "<td>0.4592355</td>\n",
       "<td>0.5502543</td>\n",
       "<td>0.0222042</td>\n",
       "<td>0.1003066</td>\n",
       "<td>122.0190658</td>\n",
       "<td>150.7411662</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500051</td>\n",
       "<td>0.4240390</td>\n",
       "<td>2.2682147</td>\n",
       "<td>2.4595723</td>\n",
       "<td>0.4154263</td>\n",
       "<td>0.4372611</td>\n",
       "<td>0.4504736</td>\n",
       "<td>0.5276557</td>\n",
       "<td>0.0226844</td>\n",
       "<td>0.1229911</td>\n",
       "<td>126.8214749</td>\n",
       "<td>145.9572280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000034</td>\n",
       "<td>0.3321782</td>\n",
       "<td>2.0098955</td>\n",
       "<td>2.2347491</td>\n",
       "<td>0.3681148</td>\n",
       "<td>0.3726242</td>\n",
       "<td>0.4092970</td>\n",
       "<td>0.4501452</td>\n",
       "<td>0.1004914</td>\n",
       "<td>0.2234825</td>\n",
       "<td>100.9895466</td>\n",
       "<td>123.4749086</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500017</td>\n",
       "<td>0.2786483</td>\n",
       "<td>1.7675257</td>\n",
       "<td>2.0790150</td>\n",
       "<td>0.3237245</td>\n",
       "<td>0.3035444</td>\n",
       "<td>0.3807741</td>\n",
       "<td>0.4012804</td>\n",
       "<td>0.0883733</td>\n",
       "<td>0.3118558</td>\n",
       "<td>76.7525718</td>\n",
       "<td>107.9014989</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2425797</td>\n",
       "<td>1.5857484</td>\n",
       "<td>1.9557025</td>\n",
       "<td>0.2904317</td>\n",
       "<td>0.2595146</td>\n",
       "<td>0.3581893</td>\n",
       "<td>0.3658402</td>\n",
       "<td>0.0792847</td>\n",
       "<td>0.3911405</td>\n",
       "<td>58.5748408</td>\n",
       "<td>95.5702516</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000034</td>\n",
       "<td>0.1959714</td>\n",
       "<td>1.3695181</td>\n",
       "<td>1.7603033</td>\n",
       "<td>0.2508289</td>\n",
       "<td>0.2172719</td>\n",
       "<td>0.3224017</td>\n",
       "<td>0.3163163</td>\n",
       "<td>0.1369564</td>\n",
       "<td>0.5280969</td>\n",
       "<td>36.9518079</td>\n",
       "<td>76.0303297</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1667087</td>\n",
       "<td>1.1305662</td>\n",
       "<td>1.6028743</td>\n",
       "<td>0.2070646</td>\n",
       "<td>0.1805241</td>\n",
       "<td>0.2935684</td>\n",
       "<td>0.2823694</td>\n",
       "<td>0.1130528</td>\n",
       "<td>0.6411497</td>\n",
       "<td>13.0566200</td>\n",
       "<td>60.2874349</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000034</td>\n",
       "<td>0.1457423</td>\n",
       "<td>0.9420748</td>\n",
       "<td>1.4707109</td>\n",
       "<td>0.1725421</td>\n",
       "<td>0.1557592</td>\n",
       "<td>0.2693625</td>\n",
       "<td>0.2570467</td>\n",
       "<td>0.0942107</td>\n",
       "<td>0.7353604</td>\n",
       "<td>-5.7925249</td>\n",
       "<td>47.0710852</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1292848</td>\n",
       "<td>0.7906574</td>\n",
       "<td>1.3573724</td>\n",
       "<td>0.1448099</td>\n",
       "<td>0.1372581</td>\n",
       "<td>0.2486044</td>\n",
       "<td>0.2370826</td>\n",
       "<td>0.0790631</td>\n",
       "<td>0.8144235</td>\n",
       "<td>-20.9342592</td>\n",
       "<td>35.7372446</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999966</td>\n",
       "<td>0.1152511</td>\n",
       "<td>0.6683641</td>\n",
       "<td>1.2589455</td>\n",
       "<td>0.1224117</td>\n",
       "<td>0.1221946</td>\n",
       "<td>0.2305774</td>\n",
       "<td>0.2206705</td>\n",
       "<td>0.0668342</td>\n",
       "<td>0.8812576</td>\n",
       "<td>-33.1635864</td>\n",
       "<td>25.8945542</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.1023359</td>\n",
       "<td>0.5445562</td>\n",
       "<td>1.1696438</td>\n",
       "<td>0.0997361</td>\n",
       "<td>0.1087495</td>\n",
       "<td>0.2142217</td>\n",
       "<td>0.2066799</td>\n",
       "<td>0.0544575</td>\n",
       "<td>0.9357151</td>\n",
       "<td>-45.5443850</td>\n",
       "<td>16.9643847</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999966</td>\n",
       "<td>0.0897076</td>\n",
       "<td>0.4208219</td>\n",
       "<td>1.0864439</td>\n",
       "<td>0.0770740</td>\n",
       "<td>0.0960452</td>\n",
       "<td>0.1989835</td>\n",
       "<td>0.1943875</td>\n",
       "<td>0.0420808</td>\n",
       "<td>0.9777958</td>\n",
       "<td>-57.9178137</td>\n",
       "<td>8.6443906</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0634272</td>\n",
       "<td>0.2220341</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0406658</td>\n",
       "<td>0.0820197</td>\n",
       "<td>0.1831512</td>\n",
       "<td>0.1831504</td>\n",
       "<td>0.0222042</td>\n",
       "<td>1.0</td>\n",
       "<td>-77.7965912</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010001                    0.599265           2.81495   2.81495            0.515562         0.665194   0.515562                    0.665194            0.0281524       0.0281524                  181.495   181.495\n",
       "    2        0.020002                    0.530404           2.60069   2.70782            0.476319         0.561849   0.49594                     0.613522            0.0260095       0.0541619                  160.069   170.782\n",
       "    3        0.030003                    0.484132           2.39382   2.60315            0.43843          0.506459   0.47677                     0.577834            0.0239406       0.0781025                  139.382   160.315\n",
       "    4        0.0400041                   0.4513             2.22019   2.50741            0.406631         0.467515   0.459235                    0.550254            0.0222042       0.100307                   122.019   150.741\n",
       "    5        0.0500051                   0.424039           2.26821   2.45957            0.415426         0.437261   0.450474                    0.527656            0.0226844       0.122991                   126.821   145.957\n",
       "    6        0.100003                    0.332178           2.0099    2.23475            0.368115         0.372624   0.409297                    0.450145            0.100491        0.223482                   100.99    123.475\n",
       "    7        0.150002                    0.278648           1.76753   2.07901            0.323724         0.303544   0.380774                    0.40128             0.0883733       0.311856                   76.7526   107.901\n",
       "    8        0.2                         0.24258            1.58575   1.9557             0.290432         0.259515   0.358189                    0.36584             0.0792847       0.391141                   58.5748   95.5703\n",
       "    9        0.300003                    0.195971           1.36952   1.7603             0.250829         0.217272   0.322402                    0.316316            0.136956        0.528097                   36.9518   76.0303\n",
       "    10       0.4                         0.166709           1.13057   1.60287            0.207065         0.180524   0.293568                    0.282369            0.113053        0.64115                    13.0566   60.2874\n",
       "    11       0.500003                    0.145742           0.942075  1.47071            0.172542         0.155759   0.269362                    0.257047            0.0942107       0.73536                    -5.79252  47.0711\n",
       "    12       0.6                         0.129285           0.790657  1.35737            0.14481          0.137258   0.248604                    0.237083            0.0790631       0.814423                   -20.9343  35.7372\n",
       "    13       0.699997                    0.115251           0.668364  1.25895            0.122412         0.122195   0.230577                    0.220671            0.0668342       0.881258                   -33.1636  25.8946\n",
       "    14       0.8                         0.102336           0.544556  1.16964            0.0997361        0.10875    0.214222                    0.20668             0.0544575       0.935715                   -45.5444  16.9644\n",
       "    15       0.899997                    0.0897076          0.420822  1.08644            0.077074         0.0960452  0.198984                    0.194388            0.0420808       0.977796                   -57.9178  8.64439\n",
       "    16       1                           0.0634272          0.222034  1                  0.0406658        0.0820197  0.183151                    0.18315             0.0222042       1                          -77.7966  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method H2OBinomialModel.tpr of >"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OStackedEnsembleEstimator :  Stacked Ensemble\n",
      "Model Key:  StackedEnsemble_AllModels_AutoML_20190108_000309\n",
      "No model summary for this model\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.0748160248427708\n",
      "RMSE: 0.27352518136868276\n",
      "LogLoss: 0.2770108718654861\n",
      "Null degrees of freedom: 147784\n",
      "Residual degrees of freedom: 147781\n",
      "Null deviance: 140732.20131171282\n",
      "Residual deviance: 81876.10339728172\n",
      "AIC: 81884.10339728172\n",
      "AUC: 0.9705354292408278\n",
      "pr_auc: 0.8960099911776978\n",
      "Gini: 0.9410708584816556\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3054804393299643: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>114475.0</td>\n",
       "<td>6243.0</td>\n",
       "<td>0.0517</td>\n",
       "<td> (6243.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>4811.0</td>\n",
       "<td>22256.0</td>\n",
       "<td>0.1777</td>\n",
       "<td> (4811.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>119286.0</td>\n",
       "<td>28499.0</td>\n",
       "<td>0.0748</td>\n",
       "<td> (11054.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0       1      Error    Rate\n",
       "-----  ------  -----  -------  ------------------\n",
       "0      114475  6243   0.0517   (6243.0/120718.0)\n",
       "1      4811    22256  0.1777   (4811.0/27067.0)\n",
       "Total  119286  28499  0.0748   (11054.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.8010654</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2324010</td>\n",
       "<td>0.8527882</td>\n",
       "<td>256.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.4116258</td>\n",
       "<td>0.8349729</td>\n",
       "<td>167.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.3440950</td>\n",
       "<td>0.9274622</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1052357</td>\n",
       "<td>1.0</td>\n",
       "<td>361.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9638314</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.3054804</td>\n",
       "<td>0.7554259</td>\n",
       "<td>216.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.2517271</td>\n",
       "<td>0.9051343</td>\n",
       "<td>245.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.2395344</td>\n",
       "<td>0.9063511</td>\n",
       "<td>252.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.30548      0.801065  216\n",
       "max f2                       0.232401     0.852788  256\n",
       "max f0point5                 0.411626     0.834973  167\n",
       "max accuracy                 0.344095     0.927462  198\n",
       "max precision                0.963831     1         0\n",
       "max recall                   0.105236     1         361\n",
       "max specificity              0.963831     1         0\n",
       "max absolute_mcc             0.30548      0.755426  216\n",
       "max min_per_class_accuracy   0.251727     0.905134  245\n",
       "max mean_per_class_accuracy  0.239534     0.906351  252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.32 %, avg score: 21.52 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100010</td>\n",
       "<td>0.8231913</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8664285</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.0546052</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200020</td>\n",
       "<td>0.7645343</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7922026</td>\n",
       "<td>1.0</td>\n",
       "<td>0.8293156</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1092105</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300030</td>\n",
       "<td>0.7172474</td>\n",
       "<td>5.4599697</td>\n",
       "<td>5.4599697</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7405818</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7997376</td>\n",
       "<td>0.0546052</td>\n",
       "<td>0.1638157</td>\n",
       "<td>445.9969705</td>\n",
       "<td>445.9969705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400041</td>\n",
       "<td>0.6758864</td>\n",
       "<td>5.4304164</td>\n",
       "<td>5.4525814</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.6959487</td>\n",
       "<td>0.9986468</td>\n",
       "<td>0.7737904</td>\n",
       "<td>0.0543097</td>\n",
       "<td>0.2181254</td>\n",
       "<td>443.0416418</td>\n",
       "<td>445.2581383</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500051</td>\n",
       "<td>0.6383709</td>\n",
       "<td>5.3417566</td>\n",
       "<td>5.4304164</td>\n",
       "<td>0.9783491</td>\n",
       "<td>0.6571369</td>\n",
       "<td>0.9945873</td>\n",
       "<td>0.7504597</td>\n",
       "<td>0.0534230</td>\n",
       "<td>0.2715484</td>\n",
       "<td>434.1756558</td>\n",
       "<td>443.0416418</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000034</td>\n",
       "<td>0.4826144</td>\n",
       "<td>4.9922256</td>\n",
       "<td>5.2113359</td>\n",
       "<td>0.9143321</td>\n",
       "<td>0.5558423</td>\n",
       "<td>0.9544624</td>\n",
       "<td>0.6531576</td>\n",
       "<td>0.2496028</td>\n",
       "<td>0.5211512</td>\n",
       "<td>399.2225650</td>\n",
       "<td>421.1335859</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500017</td>\n",
       "<td>0.3715061</td>\n",
       "<td>3.7611647</td>\n",
       "<td>4.7279673</td>\n",
       "<td>0.6888618</td>\n",
       "<td>0.4241561</td>\n",
       "<td>0.8659329</td>\n",
       "<td>0.5768272</td>\n",
       "<td>0.1880519</td>\n",
       "<td>0.7092031</td>\n",
       "<td>276.1164677</td>\n",
       "<td>372.7967271</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2957882</td>\n",
       "<td>2.5648335</td>\n",
       "<td>4.1872021</td>\n",
       "<td>0.4697523</td>\n",
       "<td>0.3308477</td>\n",
       "<td>0.7668911</td>\n",
       "<td>0.5153344</td>\n",
       "<td>0.1282373</td>\n",
       "<td>0.8374404</td>\n",
       "<td>156.4833515</td>\n",
       "<td>318.7202128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000034</td>\n",
       "<td>0.2099951</td>\n",
       "<td>1.1785170</td>\n",
       "<td>3.1842845</td>\n",
       "<td>0.2158468</td>\n",
       "<td>0.2476441</td>\n",
       "<td>0.5832055</td>\n",
       "<td>0.4261023</td>\n",
       "<td>0.1178557</td>\n",
       "<td>0.9552961</td>\n",
       "<td>17.8517042</td>\n",
       "<td>218.4284479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1682202</td>\n",
       "<td>0.3351057</td>\n",
       "<td>2.4720139</td>\n",
       "<td>0.0613750</td>\n",
       "<td>0.1869477</td>\n",
       "<td>0.4527523</td>\n",
       "<td>0.3663157</td>\n",
       "<td>0.0335094</td>\n",
       "<td>0.9888056</td>\n",
       "<td>-66.4894267</td>\n",
       "<td>147.2013891</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000034</td>\n",
       "<td>0.1432933</td>\n",
       "<td>0.0897742</td>\n",
       "<td>1.9955531</td>\n",
       "<td>0.0164422</td>\n",
       "<td>0.1548801</td>\n",
       "<td>0.3654879</td>\n",
       "<td>0.3240274</td>\n",
       "<td>0.0089777</td>\n",
       "<td>0.9977833</td>\n",
       "<td>-91.0225818</td>\n",
       "<td>99.5553054</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1259732</td>\n",
       "<td>0.0195817</td>\n",
       "<td>1.6662356</td>\n",
       "<td>0.0035864</td>\n",
       "<td>0.1341923</td>\n",
       "<td>0.3051731</td>\n",
       "<td>0.2923893</td>\n",
       "<td>0.0019581</td>\n",
       "<td>0.9997414</td>\n",
       "<td>-98.0418298</td>\n",
       "<td>66.6235637</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999966</td>\n",
       "<td>0.1118935</td>\n",
       "<td>0.0022168</td>\n",
       "<td>1.4285256</td>\n",
       "<td>0.0004060</td>\n",
       "<td>0.1187515</td>\n",
       "<td>0.2616362</td>\n",
       "<td>0.2675846</td>\n",
       "<td>0.0002217</td>\n",
       "<td>0.9999631</td>\n",
       "<td>-99.7783204</td>\n",
       "<td>42.8525554</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0991367</td>\n",
       "<td>0.0003694</td>\n",
       "<td>1.25</td>\n",
       "<td>0.0000677</td>\n",
       "<td>0.1054206</td>\n",
       "<td>0.2289390</td>\n",
       "<td>0.2473134</td>\n",
       "<td>0.0000369</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.9630559</td>\n",
       "<td>25.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999966</td>\n",
       "<td>0.0873202</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111153</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0932206</td>\n",
       "<td>0.2035021</td>\n",
       "<td>0.2301925</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1115288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0637468</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0802774</td>\n",
       "<td>0.1831512</td>\n",
       "<td>0.2152005</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift         cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -----------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010001                    0.823191           5.45997      5.45997            1                0.866429   1                           0.866429            0.0546052       0.0546052                  445.997   445.997\n",
       "    2        0.020002                    0.764534           5.45997      5.45997            1                0.792203   1                           0.829316            0.0546052       0.10921                    445.997   445.997\n",
       "    3        0.030003                    0.717247           5.45997      5.45997            1                0.740582   1                           0.799738            0.0546052       0.163816                   445.997   445.997\n",
       "    4        0.0400041                   0.675886           5.43042      5.45258            0.994587         0.695949   0.998647                    0.77379             0.0543097       0.218125                   443.042   445.258\n",
       "    5        0.0500051                   0.638371           5.34176      5.43042            0.978349         0.657137   0.994587                    0.75046             0.053423        0.271548                   434.176   443.042\n",
       "    6        0.100003                    0.482614           4.99223      5.21134            0.914332         0.555842   0.954462                    0.653158            0.249603        0.521151                   399.223   421.134\n",
       "    7        0.150002                    0.371506           3.76116      4.72797            0.688862         0.424156   0.865933                    0.576827            0.188052        0.709203                   276.116   372.797\n",
       "    8        0.2                         0.295788           2.56483      4.1872             0.469752         0.330848   0.766891                    0.515334            0.128237        0.83744                    156.483   318.72\n",
       "    9        0.300003                    0.209995           1.17852      3.18428            0.215847         0.247644   0.583206                    0.426102            0.117856        0.955296                   17.8517   218.428\n",
       "    10       0.4                         0.16822            0.335106     2.47201            0.061375         0.186948   0.452752                    0.366316            0.0335094       0.988806                   -66.4894  147.201\n",
       "    11       0.500003                    0.143293           0.0897742    1.99555            0.0164422        0.15488    0.365488                    0.324027            0.00897772      0.997783                   -91.0226  99.5553\n",
       "    12       0.6                         0.125973           0.0195817    1.66624            0.00358641       0.134192   0.305173                    0.292389            0.0019581       0.999741                   -98.0418  66.6236\n",
       "    13       0.699997                    0.111893           0.0022168    1.42853            0.000406009      0.118752   0.261636                    0.267585            0.000221672     0.999963                   -99.7783  42.8526\n",
       "    14       0.8                         0.0991367          0.000369441  1.25               6.76636e-05      0.105421   0.228939                    0.247313            3.69454e-05     1                          -99.9631  25\n",
       "    15       0.899997                    0.0873202          0            1.11112            0                0.0932206  0.203502                    0.230193            0               1                          -100      11.1115\n",
       "    16       1                           0.0637468          0            1                  0                0.0802774  0.183151                    0.215201            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1374305415577651\n",
      "RMSE: 0.370716254779535\n",
      "LogLoss: 0.4374035749971233\n",
      "Null degrees of freedom: 16201\n",
      "Residual degrees of freedom: 16198\n",
      "Null deviance: 15373.717640908228\n",
      "Residual deviance: 14173.625444206784\n",
      "AIC: 14181.625444206784\n",
      "AUC: 0.7038412027583177\n",
      "pr_auc: 0.34120380509032733\n",
      "Gini: 0.4076824055166355\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.19535344211739553: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>9850.0</td>\n",
       "<td>3403.0</td>\n",
       "<td>0.2568</td>\n",
       "<td> (3403.0/13253.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1367.0</td>\n",
       "<td>1582.0</td>\n",
       "<td>0.4635</td>\n",
       "<td> (1367.0/2949.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11217.0</td>\n",
       "<td>4985.0</td>\n",
       "<td>0.2944</td>\n",
       "<td> (4770.0/16202.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1     Error    Rate\n",
       "-----  -----  ----  -------  ----------------\n",
       "0      9850   3403  0.2568   (3403.0/13253.0)\n",
       "1      1367   1582  0.4635   (1367.0/2949.0)\n",
       "Total  11217  4985  0.2944   (4770.0/16202.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1953534</td>\n",
       "<td>0.3987900</td>\n",
       "<td>248.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1166556</td>\n",
       "<td>0.5648862</td>\n",
       "<td>337.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2709433</td>\n",
       "<td>0.3762722</td>\n",
       "<td>185.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6365348</td>\n",
       "<td>0.8189730</td>\n",
       "<td>28.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.7519927</td>\n",
       "<td>0.6470588</td>\n",
       "<td>7.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0720133</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8355440</td>\n",
       "<td>0.9999245</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.2288610</td>\n",
       "<td>0.2366986</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1667771</td>\n",
       "<td>0.6414397</td>\n",
       "<td>277.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1542798</td>\n",
       "<td>0.6472159</td>\n",
       "<td>291.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.195353     0.39879   248\n",
       "max f2                       0.116656     0.564886  337\n",
       "max f0point5                 0.270943     0.376272  185\n",
       "max accuracy                 0.636535     0.818973  28\n",
       "max precision                0.751993     0.647059  7\n",
       "max recall                   0.0720133    1         397\n",
       "max specificity              0.835544     0.999925  0\n",
       "max absolute_mcc             0.228861     0.236699  218\n",
       "max min_per_class_accuracy   0.166777     0.64144   277\n",
       "max mean_per_class_accuracy  0.15428      0.647216  291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.20 %, avg score: 18.47 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100605</td>\n",
       "<td>0.6070463</td>\n",
       "<td>2.9324155</td>\n",
       "<td>2.9324155</td>\n",
       "<td>0.5337423</td>\n",
       "<td>0.6747785</td>\n",
       "<td>0.5337423</td>\n",
       "<td>0.6747785</td>\n",
       "<td>0.0295015</td>\n",
       "<td>0.0295015</td>\n",
       "<td>193.2415480</td>\n",
       "<td>193.2415480</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200593</td>\n",
       "<td>0.5374457</td>\n",
       "<td>2.6792049</td>\n",
       "<td>2.8061998</td>\n",
       "<td>0.4876543</td>\n",
       "<td>0.5705091</td>\n",
       "<td>0.5107692</td>\n",
       "<td>0.6228042</td>\n",
       "<td>0.0267887</td>\n",
       "<td>0.0562903</td>\n",
       "<td>167.9204920</td>\n",
       "<td>180.6199755</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300580</td>\n",
       "<td>0.4880498</td>\n",
       "<td>2.4418070</td>\n",
       "<td>2.6849849</td>\n",
       "<td>0.4444444</td>\n",
       "<td>0.5106372</td>\n",
       "<td>0.4887064</td>\n",
       "<td>0.5854920</td>\n",
       "<td>0.0244151</td>\n",
       "<td>0.0807053</td>\n",
       "<td>144.1807016</td>\n",
       "<td>168.4984922</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400568</td>\n",
       "<td>0.4585920</td>\n",
       "<td>2.5096350</td>\n",
       "<td>2.6412150</td>\n",
       "<td>0.4567901</td>\n",
       "<td>0.4730993</td>\n",
       "<td>0.4807396</td>\n",
       "<td>0.5574371</td>\n",
       "<td>0.0250933</td>\n",
       "<td>0.1057986</td>\n",
       "<td>150.9634988</td>\n",
       "<td>164.1214984</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500555</td>\n",
       "<td>0.4315051</td>\n",
       "<td>2.1026672</td>\n",
       "<td>2.5336382</td>\n",
       "<td>0.3827160</td>\n",
       "<td>0.4444311</td>\n",
       "<td>0.4611591</td>\n",
       "<td>0.5348638</td>\n",
       "<td>0.0210241</td>\n",
       "<td>0.1268227</td>\n",
       "<td>110.2667152</td>\n",
       "<td>153.3638229</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000494</td>\n",
       "<td>0.3337569</td>\n",
       "<td>2.0280564</td>\n",
       "<td>2.2810033</td>\n",
       "<td>0.3691358</td>\n",
       "<td>0.3752998</td>\n",
       "<td>0.4151758</td>\n",
       "<td>0.4551310</td>\n",
       "<td>0.1013903</td>\n",
       "<td>0.2282130</td>\n",
       "<td>102.8056382</td>\n",
       "<td>128.1003253</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500432</td>\n",
       "<td>0.2806429</td>\n",
       "<td>1.8313553</td>\n",
       "<td>2.1311822</td>\n",
       "<td>0.3333333</td>\n",
       "<td>0.3060461</td>\n",
       "<td>0.3879062</td>\n",
       "<td>0.4054565</td>\n",
       "<td>0.0915565</td>\n",
       "<td>0.3197694</td>\n",
       "<td>83.1355262</td>\n",
       "<td>113.1182244</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000370</td>\n",
       "<td>0.2443961</td>\n",
       "<td>1.5939574</td>\n",
       "<td>1.9969175</td>\n",
       "<td>0.2901235</td>\n",
       "<td>0.2612294</td>\n",
       "<td>0.3634681</td>\n",
       "<td>0.3694109</td>\n",
       "<td>0.0796880</td>\n",
       "<td>0.3994574</td>\n",
       "<td>59.3957357</td>\n",
       "<td>99.6917462</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000247</td>\n",
       "<td>0.1976308</td>\n",
       "<td>1.2582089</td>\n",
       "<td>1.7507319</td>\n",
       "<td>0.2290123</td>\n",
       "<td>0.2187809</td>\n",
       "<td>0.3186587</td>\n",
       "<td>0.3192112</td>\n",
       "<td>0.1258054</td>\n",
       "<td>0.5252628</td>\n",
       "<td>25.8208893</td>\n",
       "<td>75.0731928</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000123</td>\n",
       "<td>0.1687228</td>\n",
       "<td>1.0988132</td>\n",
       "<td>1.5877774</td>\n",
       "<td>0.2</td>\n",
       "<td>0.1820912</td>\n",
       "<td>0.2889986</td>\n",
       "<td>0.2849365</td>\n",
       "<td>0.1098678</td>\n",
       "<td>0.6351306</td>\n",
       "<td>9.8813157</td>\n",
       "<td>58.7777382</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.1468073</td>\n",
       "<td>1.0174196</td>\n",
       "<td>1.4737199</td>\n",
       "<td>0.1851852</td>\n",
       "<td>0.1575376</td>\n",
       "<td>0.2682385</td>\n",
       "<td>0.2594599</td>\n",
       "<td>0.1017294</td>\n",
       "<td>0.7368600</td>\n",
       "<td>1.7419590</td>\n",
       "<td>47.3719905</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999877</td>\n",
       "<td>0.1302072</td>\n",
       "<td>0.7935873</td>\n",
       "<td>1.3603761</td>\n",
       "<td>0.1444444</td>\n",
       "<td>0.1384109</td>\n",
       "<td>0.2476083</td>\n",
       "<td>0.2392871</td>\n",
       "<td>0.0793489</td>\n",
       "<td>0.8162089</td>\n",
       "<td>-20.6412720</td>\n",
       "<td>36.0376128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999753</td>\n",
       "<td>0.1160390</td>\n",
       "<td>0.6952367</td>\n",
       "<td>1.2653646</td>\n",
       "<td>0.1265432</td>\n",
       "<td>0.1229040</td>\n",
       "<td>0.2303148</td>\n",
       "<td>0.2226624</td>\n",
       "<td>0.0695151</td>\n",
       "<td>0.8857240</td>\n",
       "<td>-30.4763280</td>\n",
       "<td>26.5364591</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999630</td>\n",
       "<td>0.1030191</td>\n",
       "<td>0.5460152</td>\n",
       "<td>1.1754529</td>\n",
       "<td>0.0993827</td>\n",
       "<td>0.1093927</td>\n",
       "<td>0.2139495</td>\n",
       "<td>0.2085048</td>\n",
       "<td>0.0545948</td>\n",
       "<td>0.9403188</td>\n",
       "<td>-45.3984820</td>\n",
       "<td>17.5452853</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999506</td>\n",
       "<td>0.0899922</td>\n",
       "<td>0.3866194</td>\n",
       "<td>1.0878107</td>\n",
       "<td>0.0703704</td>\n",
       "<td>0.0965274</td>\n",
       "<td>0.1979974</td>\n",
       "<td>0.1960637</td>\n",
       "<td>0.0386572</td>\n",
       "<td>0.9789759</td>\n",
       "<td>-61.3380556</td>\n",
       "<td>8.7810707</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0661632</td>\n",
       "<td>0.2101370</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0382480</td>\n",
       "<td>0.0822573</td>\n",
       "<td>0.1820146</td>\n",
       "<td>0.1846775</td>\n",
       "<td>0.0210241</td>\n",
       "<td>1.0</td>\n",
       "<td>-78.9862999</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100605                   0.607046           2.93242   2.93242            0.533742         0.674778   0.533742                    0.674778            0.0295015       0.0295015                  193.242   193.242\n",
       "    2        0.0200593                   0.537446           2.6792    2.8062             0.487654         0.570509   0.510769                    0.622804            0.0267887       0.0562903                  167.92    180.62\n",
       "    3        0.030058                    0.48805            2.44181   2.68498            0.444444         0.510637   0.488706                    0.585492            0.0244151       0.0807053                  144.181   168.498\n",
       "    4        0.0400568                   0.458592           2.50963   2.64121            0.45679          0.473099   0.48074                     0.557437            0.0250933       0.105799                   150.963   164.121\n",
       "    5        0.0500555                   0.431505           2.10267   2.53364            0.382716         0.444431   0.461159                    0.534864            0.0210241       0.126823                   110.267   153.364\n",
       "    6        0.100049                    0.333757           2.02806   2.281              0.369136         0.3753     0.415176                    0.455131            0.10139         0.228213                   102.806   128.1\n",
       "    7        0.150043                    0.280643           1.83136   2.13118            0.333333         0.306046   0.387906                    0.405457            0.0915565       0.319769                   83.1355   113.118\n",
       "    8        0.200037                    0.244396           1.59396   1.99692            0.290123         0.261229   0.363468                    0.369411            0.079688        0.399457                   59.3957   99.6917\n",
       "    9        0.300025                    0.197631           1.25821   1.75073            0.229012         0.218781   0.318659                    0.319211            0.125805        0.525263                   25.8209   75.0732\n",
       "    10       0.400012                    0.168723           1.09881   1.58778            0.2              0.182091   0.288999                    0.284937            0.109868        0.635131                   9.88132   58.7777\n",
       "    11       0.5                         0.146807           1.01742   1.47372            0.185185         0.157538   0.268238                    0.25946             0.101729        0.73686                    1.74196   47.372\n",
       "    12       0.599988                    0.130207           0.793587  1.36038            0.144444         0.138411   0.247608                    0.239287            0.0793489       0.816209                   -20.6413  36.0376\n",
       "    13       0.699975                    0.116039           0.695237  1.26536            0.126543         0.122904   0.230315                    0.222662            0.0695151       0.885724                   -30.4763  26.5365\n",
       "    14       0.799963                    0.103019           0.546015  1.17545            0.0993827        0.109393   0.21395                     0.208505            0.0545948       0.940319                   -45.3985  17.5453\n",
       "    15       0.899951                    0.0899922          0.386619  1.08781            0.0703704        0.0965274  0.197997                    0.196064            0.0386572       0.978976                   -61.3381  8.78107\n",
       "    16       1                           0.0661632          0.210137  1                  0.038248         0.0822573  0.182015                    0.184677            0.0210241       1                          -78.9863  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: stackedensemble\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.13857609376371857\n",
      "RMSE: 0.3722581010048251\n",
      "LogLoss: 0.4401913910510862\n",
      "Null degrees of freedom: 147784\n",
      "Residual degrees of freedom: 147781\n",
      "Null deviance: 140733.9728124682\n",
      "Residual deviance: 130107.36945296955\n",
      "AIC: 130115.36945296955\n",
      "AUC: 0.7020527753495225\n",
      "pr_auc: 0.3375985314167658\n",
      "Gini: 0.40410555069904497\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.17298272394626243: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>81649.0</td>\n",
       "<td>39069.0</td>\n",
       "<td>0.3236</td>\n",
       "<td> (39069.0/120718.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>10296.0</td>\n",
       "<td>16771.0</td>\n",
       "<td>0.3804</td>\n",
       "<td> (10296.0/27067.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>91945.0</td>\n",
       "<td>55840.0</td>\n",
       "<td>0.334</td>\n",
       "<td> (49365.0/147785.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      81649  39069  0.3236   (39069.0/120718.0)\n",
       "1      10296  16771  0.3804   (10296.0/27067.0)\n",
       "Total  91945  55840  0.334    (49365.0/147785.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.1729827</td>\n",
       "<td>0.4045738</td>\n",
       "<td>275.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1166841</td>\n",
       "<td>0.5633569</td>\n",
       "<td>341.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.2578390</td>\n",
       "<td>0.3662781</td>\n",
       "<td>202.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.6424048</td>\n",
       "<td>0.8173698</td>\n",
       "<td>31.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.8611637</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0700469</td>\n",
       "<td>1.0</td>\n",
       "<td>397.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.8611637</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.1825932</td>\n",
       "<td>0.2367313</td>\n",
       "<td>265.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.1657609</td>\n",
       "<td>0.6461004</td>\n",
       "<td>282.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.1604917</td>\n",
       "<td>0.6483125</td>\n",
       "<td>288.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.172983     0.404574  275\n",
       "max f2                       0.116684     0.563357  341\n",
       "max f0point5                 0.257839     0.366278  202\n",
       "max accuracy                 0.642405     0.81737   31\n",
       "max precision                0.861164     1         0\n",
       "max recall                   0.0700469    1         397\n",
       "max specificity              0.861164     1         0\n",
       "max absolute_mcc             0.182593     0.236731  265\n",
       "max min_per_class_accuracy   0.165761     0.6461    282\n",
       "max mean_per_class_accuracy  0.160492     0.648312  288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 18.32 %, avg score: 18.32 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100010</td>\n",
       "<td>0.5992652</td>\n",
       "<td>2.8149506</td>\n",
       "<td>2.8149506</td>\n",
       "<td>0.5155616</td>\n",
       "<td>0.6651941</td>\n",
       "<td>0.5155616</td>\n",
       "<td>0.6651941</td>\n",
       "<td>0.0281524</td>\n",
       "<td>0.0281524</td>\n",
       "<td>181.4950551</td>\n",
       "<td>181.4950551</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200020</td>\n",
       "<td>0.5304044</td>\n",
       "<td>2.6006892</td>\n",
       "<td>2.7078199</td>\n",
       "<td>0.4763194</td>\n",
       "<td>0.5618491</td>\n",
       "<td>0.4959405</td>\n",
       "<td>0.6135216</td>\n",
       "<td>0.0260095</td>\n",
       "<td>0.0541619</td>\n",
       "<td>160.0689223</td>\n",
       "<td>170.7819887</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300030</td>\n",
       "<td>0.4841323</td>\n",
       "<td>2.3938162</td>\n",
       "<td>2.6031520</td>\n",
       "<td>0.4384303</td>\n",
       "<td>0.5064587</td>\n",
       "<td>0.4767704</td>\n",
       "<td>0.5778339</td>\n",
       "<td>0.0239406</td>\n",
       "<td>0.0781025</td>\n",
       "<td>139.3816217</td>\n",
       "<td>160.3151997</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400041</td>\n",
       "<td>0.4512997</td>\n",
       "<td>2.2201907</td>\n",
       "<td>2.5074117</td>\n",
       "<td>0.4066306</td>\n",
       "<td>0.4675155</td>\n",
       "<td>0.4592355</td>\n",
       "<td>0.5502543</td>\n",
       "<td>0.0222042</td>\n",
       "<td>0.1003066</td>\n",
       "<td>122.0190658</td>\n",
       "<td>150.7411662</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500051</td>\n",
       "<td>0.4240390</td>\n",
       "<td>2.2682147</td>\n",
       "<td>2.4595723</td>\n",
       "<td>0.4154263</td>\n",
       "<td>0.4372611</td>\n",
       "<td>0.4504736</td>\n",
       "<td>0.5276557</td>\n",
       "<td>0.0226844</td>\n",
       "<td>0.1229911</td>\n",
       "<td>126.8214749</td>\n",
       "<td>145.9572280</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000034</td>\n",
       "<td>0.3321782</td>\n",
       "<td>2.0098955</td>\n",
       "<td>2.2347491</td>\n",
       "<td>0.3681148</td>\n",
       "<td>0.3726242</td>\n",
       "<td>0.4092970</td>\n",
       "<td>0.4501452</td>\n",
       "<td>0.1004914</td>\n",
       "<td>0.2234825</td>\n",
       "<td>100.9895466</td>\n",
       "<td>123.4749086</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500017</td>\n",
       "<td>0.2786483</td>\n",
       "<td>1.7675257</td>\n",
       "<td>2.0790150</td>\n",
       "<td>0.3237245</td>\n",
       "<td>0.3035444</td>\n",
       "<td>0.3807741</td>\n",
       "<td>0.4012804</td>\n",
       "<td>0.0883733</td>\n",
       "<td>0.3118558</td>\n",
       "<td>76.7525718</td>\n",
       "<td>107.9014989</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2425797</td>\n",
       "<td>1.5857484</td>\n",
       "<td>1.9557025</td>\n",
       "<td>0.2904317</td>\n",
       "<td>0.2595146</td>\n",
       "<td>0.3581893</td>\n",
       "<td>0.3658402</td>\n",
       "<td>0.0792847</td>\n",
       "<td>0.3911405</td>\n",
       "<td>58.5748408</td>\n",
       "<td>95.5702516</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000034</td>\n",
       "<td>0.1959714</td>\n",
       "<td>1.3695181</td>\n",
       "<td>1.7603033</td>\n",
       "<td>0.2508289</td>\n",
       "<td>0.2172719</td>\n",
       "<td>0.3224017</td>\n",
       "<td>0.3163163</td>\n",
       "<td>0.1369564</td>\n",
       "<td>0.5280969</td>\n",
       "<td>36.9518079</td>\n",
       "<td>76.0303297</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.1667087</td>\n",
       "<td>1.1305662</td>\n",
       "<td>1.6028743</td>\n",
       "<td>0.2070646</td>\n",
       "<td>0.1805241</td>\n",
       "<td>0.2935684</td>\n",
       "<td>0.2823694</td>\n",
       "<td>0.1130528</td>\n",
       "<td>0.6411497</td>\n",
       "<td>13.0566200</td>\n",
       "<td>60.2874349</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000034</td>\n",
       "<td>0.1457423</td>\n",
       "<td>0.9420748</td>\n",
       "<td>1.4707109</td>\n",
       "<td>0.1725421</td>\n",
       "<td>0.1557592</td>\n",
       "<td>0.2693625</td>\n",
       "<td>0.2570467</td>\n",
       "<td>0.0942107</td>\n",
       "<td>0.7353604</td>\n",
       "<td>-5.7925249</td>\n",
       "<td>47.0710852</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1292848</td>\n",
       "<td>0.7906574</td>\n",
       "<td>1.3573724</td>\n",
       "<td>0.1448099</td>\n",
       "<td>0.1372581</td>\n",
       "<td>0.2486044</td>\n",
       "<td>0.2370826</td>\n",
       "<td>0.0790631</td>\n",
       "<td>0.8144235</td>\n",
       "<td>-20.9342592</td>\n",
       "<td>35.7372446</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999966</td>\n",
       "<td>0.1152511</td>\n",
       "<td>0.6683641</td>\n",
       "<td>1.2589455</td>\n",
       "<td>0.1224117</td>\n",
       "<td>0.1221946</td>\n",
       "<td>0.2305774</td>\n",
       "<td>0.2206705</td>\n",
       "<td>0.0668342</td>\n",
       "<td>0.8812576</td>\n",
       "<td>-33.1635864</td>\n",
       "<td>25.8945542</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.1023359</td>\n",
       "<td>0.5445562</td>\n",
       "<td>1.1696438</td>\n",
       "<td>0.0997361</td>\n",
       "<td>0.1087495</td>\n",
       "<td>0.2142217</td>\n",
       "<td>0.2066799</td>\n",
       "<td>0.0544575</td>\n",
       "<td>0.9357151</td>\n",
       "<td>-45.5443850</td>\n",
       "<td>16.9643847</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999966</td>\n",
       "<td>0.0897076</td>\n",
       "<td>0.4208219</td>\n",
       "<td>1.0864439</td>\n",
       "<td>0.0770740</td>\n",
       "<td>0.0960452</td>\n",
       "<td>0.1989835</td>\n",
       "<td>0.1943875</td>\n",
       "<td>0.0420808</td>\n",
       "<td>0.9777958</td>\n",
       "<td>-57.9178137</td>\n",
       "<td>8.6443906</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0634272</td>\n",
       "<td>0.2220341</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0406658</td>\n",
       "<td>0.0820197</td>\n",
       "<td>0.1831512</td>\n",
       "<td>0.1831504</td>\n",
       "<td>0.0222042</td>\n",
       "<td>1.0</td>\n",
       "<td>-77.7965912</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010001                    0.599265           2.81495   2.81495            0.515562         0.665194   0.515562                    0.665194            0.0281524       0.0281524                  181.495   181.495\n",
       "    2        0.020002                    0.530404           2.60069   2.70782            0.476319         0.561849   0.49594                     0.613522            0.0260095       0.0541619                  160.069   170.782\n",
       "    3        0.030003                    0.484132           2.39382   2.60315            0.43843          0.506459   0.47677                     0.577834            0.0239406       0.0781025                  139.382   160.315\n",
       "    4        0.0400041                   0.4513             2.22019   2.50741            0.406631         0.467515   0.459235                    0.550254            0.0222042       0.100307                   122.019   150.741\n",
       "    5        0.0500051                   0.424039           2.26821   2.45957            0.415426         0.437261   0.450474                    0.527656            0.0226844       0.122991                   126.821   145.957\n",
       "    6        0.100003                    0.332178           2.0099    2.23475            0.368115         0.372624   0.409297                    0.450145            0.100491        0.223482                   100.99    123.475\n",
       "    7        0.150002                    0.278648           1.76753   2.07901            0.323724         0.303544   0.380774                    0.40128             0.0883733       0.311856                   76.7526   107.901\n",
       "    8        0.2                         0.24258            1.58575   1.9557             0.290432         0.259515   0.358189                    0.36584             0.0792847       0.391141                   58.5748   95.5703\n",
       "    9        0.300003                    0.195971           1.36952   1.7603             0.250829         0.217272   0.322402                    0.316316            0.136956        0.528097                   36.9518   76.0303\n",
       "    10       0.4                         0.166709           1.13057   1.60287            0.207065         0.180524   0.293568                    0.282369            0.113053        0.64115                    13.0566   60.2874\n",
       "    11       0.500003                    0.145742           0.942075  1.47071            0.172542         0.155759   0.269362                    0.257047            0.0942107       0.73536                    -5.79252  47.0711\n",
       "    12       0.6                         0.129285           0.790657  1.35737            0.14481          0.137258   0.248604                    0.237083            0.0790631       0.814423                   -20.9343  35.7372\n",
       "    13       0.699997                    0.115251           0.668364  1.25895            0.122412         0.122195   0.230577                    0.220671            0.0668342       0.881258                   -33.1636  25.8946\n",
       "    14       0.8                         0.102336           0.544556  1.16964            0.0997361        0.10875    0.214222                    0.20668             0.0544575       0.935715                   -45.5444  16.9644\n",
       "    15       0.899997                    0.0897076          0.420822  1.08644            0.077074         0.0960452  0.198984                    0.194388            0.0420808       0.977796                   -57.9178  8.64439\n",
       "    16       1                           0.0634272          0.222034  1                  0.0406658        0.0820197  0.183151                    0.18315             0.0222042       1                          -77.7966  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method ModelBase.weights of >"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml.leader.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Sets for Binary Classifier \n",
    "\n",
    "#### Some Kaggle Binary classification competitions  \n",
    "\n",
    "The idea here is to get a range of datasets to test our H2O binary classification models as well as to understand which approaches work best for binary classification.   The hope is to get a single model or set of models that perform well in these competitions as well as logic and tests to dynamically choose the best models and their parameters.  \n",
    "\n",
    "[Santander Customer Satisfaction](https://www.kaggle.com/c/santander-customer-satisfaction)    \n",
    "\n",
    "[Facebook Recruiting IV: Human or Robot?](https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot)    \n",
    "\n",
    "[DonorsChoose.org Application Screening Predict whether teachers' project proposals are accepted](https://www.kaggle.com/c/donorschoose-application-screening)    \n",
    "\n",
    "[Statoil/C-CORE Iceberg Classifier Challenge Ship or iceberg, can you decide from space?](https://www.kaggle.com/c/statoil-iceberg-classifier-challenge)    \n",
    "\n",
    "[WSDM - KKBox's Churn Prediction Challenge Can you predict when subscribers will churn?](https://www.kaggle.com/c/kkbox-churn-prediction-challenge)    \n",
    "\n",
    "[Porto Seguroâ€™s Safe Driver Prediction Predict if a driver will file an insurance claim next year.](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)    \n",
    "\n",
    "[Porto Seguroâ€™s Safe Driver Prediction Predict if a driver will file an insurance claim next year.](https://www.kaggle.com/c/dato-native)    \n",
    "\n",
    "[Data Science Bowl 2017 Can you improve lung cancer detection?](https://www.kaggle.com/c/data-science-bowl-2017)    \n",
    "\n",
    "[Random Acts of Pizza Predicting altruism through free pizza](https://www.kaggle.com/c/random-acts-of-pizza)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last update:  October 3, 2018"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
